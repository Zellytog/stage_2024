\documentclass{article}
\usepackage{prelude}

\title{Vers une étude du contenu calculatoire\\ des principes de choix}

\author{Titouan Leclercq\\ Sous l'encadrement de\\ \'Etienne Miquey}

\date{April 15$^{\mathrm{th}}$ -- July 15$^{\mathrm{th}}$}

\begin{document}

\maketitle

\tableofcontents

\section{Introduction}

Les mathématiques usuelles se déroulent dans $\ZFC$. Dans cette pratique des mathématiques, l'objectif est de démontrer un maximum de résultats, sans considération pour les moyens logiques employés (sous réserve que ceux-ci soient cohérents). Le cadre de ce rapport, celui des mathématiques à rebours, cherche au contraire à étudier la force logique de résultats déjà démontrés~: quels principes doit-on admettre pour les démontrer ?

Cette question est renouvelée dans le cadre de la logique intuitionniste et des mathématiques constructivistes (qui n'acceptent que les procédés démonstratifs explicites), puisque l'on peut étudier le caractère constructif des principes en plus de leur force logique. Dans cette étude, une famille de formules a un rôle privilégié : les principes de choix. Leur forme générale est
\[\forall x^A, \exists y^B, R(x,y) \implies \exists f^{A\to B}, \forall x^A, R(x,f(x))\]
Où $R$ est une relation binaire entre $A$ et $B$. Suivant le choix de $A,B$ et des formes que peut prendre $R$, on obtient de nombreux principes de choix. On peut, de plus, étudier une variante de ces principes en en prenant la contraposée~: en logique intuitionniste, ces ``principes de co-choix'' sont plus faibles, souvent strictement.

Dans ce rapport, nous étudierons l'un des principes les plus faibles~: le lemme de König et sa contraposée, le Fan theorem. Le premier énonce qu'un arbre infini qui reste finiment branchant (comprendre par-là qu'à un n\oe ud donné il n'y a qu'un nombre fini de fils directs) possède une branche infini ; le deuxième énonce que pour un arbre $T$, si toute branche infinie $\alpha$ sort de l'arbre à partir d'une longueur $n_\alpha$, alors on peut trouver un $n$ uniforme tel que toute branche infini $\alpha$ sort de $T$ à partir de la longueur $n$~: autrement dit, l'arbre a une hauteur finie. Sans rentrer dans les détails, en considérant un arbre comme une partie de $\bN^*$ close par préfixe (son complémentaire est donc une partie de $\bN^*$ close par extension), on peut énoncer le lemme de König et le Fan theorem de la façon suivante (en omettant plusieurs hypothèses sur $T$, respectivement $C$)~:
\[\KL \defeq (\forall n^{\bN}, \exists p^{\bN^*}, |p| = n \land p \in T) \implies \exists \alpha^{\bN\to\bN}, \forall n^{\bN}, \alpha_0\ldots\alpha_{n-1}\in T\]
\[\FT \defeq (\forall \alpha^{\bN\to\bN}, \exists n^\bN, \alpha_0\ldots \alpha_{n-1}\in C)\implies \exists n^\bN, \forall \alpha^{\bN\to\bN}, \alpha_0\ldots\alpha_{n-1}\in T\]
L'écriture $\exists p^{\bN^*},|p| = n \land p \in T$ est équivalente à $\exists \alpha^{\bN\to\bN}, \alpha_0\ldots\alpha_{n-1}\in T$, mais plus naturelle à considérer. On suppose ici que $T$ est clos par préfixe et $C$ par extension, nous donnant donc que $\FT$ est la contraposée de $\KL$.

Mentionnons que $\KL$ comme $\FT$ sont toujours vrais dans $\ZFC$, et même dans $\ZF$~: on travaille donc dans une théorie plus faible dans laquelle on peut étudier l'ajout de $\FT$ ou $\KL$ comme non trivial. La théorie habituellement utilisée en mathématiques à rebours est un fragment faible de l'arithmétique du second ordre nommée $\RCAO$. Cette théorie est très expressive car elle permet de décrire les réels ainsi que les fonctions continues voire mesurables. Cependant, cette expressivité se fait au coût d'une grande quantité de codages, qui nous intéressent peu. Nous faisons donc le choix de travailler dans une version de l'arithmétique dans laquelle on peut parler de façon interne de fonctions, de paires et d'autres outils primitifs en plus des entiers, le tout avec une logique du second ordre.

Maintenant que le contexte est plus clair, nous pouvons décrire l'objectif du stage présenté dans ce rapport~: il s'agit de mesurer le contenu logique et calculatoire de $\FT$, en le séparant notamment de $\KL$. Pour ce faire, nous utilisons la réalisabilité, qui est un outil reliant les langages de programmation et la logique.

Depuis la découverte de la correspondance de Curry-Howard, en effet, le lien entre les langages de programmation et la logique ont été longuement explorés. Cette correspondance établit un parallèle entre, d'un côté, les programmes d'un langage typé (par exemple un programme doublant sont entrée, qu'on pourrait écrire $\double : \intt \to \intt$) et les preuves d'une proposition. A une proposition $A$ il est possible de faire correspondre un type $A'$, et les programmes de types $A'$ correspondent alors à des preuves de $A$. La réalisabilité étend cette correspondance en considérant, plutôt que la relation de typage, syntaxique, une approche sémantique. On entend par-là que la relation de typage est définie par un ensemble simple de règles d'inférences, que l'on peut vérifier méthodiquement, et que vérifier si un certain programme est d'un type donné est en général un problème décidable. La réalisabilité, elle, remplace la relation de typage $t : A'$ par une relation, directement entre $t$ et $A$ (la proposition), que l'on note $t \reali A$, et qui signifie que $t$ est une preuve de $A$. Dans cette définition, on autorise les termes à être par exemple non typés, et le seul critère considéré pour dire que $t \reali A$ est le comportement (la sémantique) de $t$. Avec cette relation, il devient alors possible de définir une nouvelle notion de modèle~: plutôt que d'attribuer à une proposition une valeur de vérité dans $\{0,1\}$, la valeur de vérité de $A$ devient l'ensemble de ses preuves, qu'on appellera ses réaliseurs.

Partant d'un langage simple (le $\lambda$-calcul), nous allons construire un modèle de réalisabilité vérifiant Fan theorem mais invalidant le lemme de König (sous une forme faible). La preuve que ce modèle vérifie $\FT$ pourra ensuite servir à chercher une présentation plus générale de l'argument qu'une forme de continuité des fonctions calculables implique $\FT$, qui semble être une notion de continuité des fonctions. En effet, il existe des cadres de travail plus généraux pour parler de réalisabilité, et extraire l'argument derrière la vérification de $\FT$ nous permettrait alors, en employant un de ces cadres de travail (en particulier les evidenced frames décrites dans \cite{DBLP:conf/lics/0001MT21}), d'extraire la notion calculatoire qui permet de vérifier ou non $\FT$.

La principale contribution de ce stage, qui explique d'ailleurs le point de vue développé dans ce rapport, est la définition d'un modèle de réalisabilité pour aborder la réalisabilité permettant, plus tard, d'étudier les principes de choix. La construction de modèles de réalisabilité est un exercice classique~: on peut par exemple retrouver plusieurs modèles dans \cite{Dinis_2023} ou dans \cite{COHEN201987}, mais ceux-ci sont utilisés dans un objectif différent du nôtre. Le modèle que nous construisons a l'avantage d'être facilement expressif (peu de codage doit être employé) et modulaire (on peut modifier une partie du modèle, comme le langage de programmation ou la syntaxe, avec un impact minime sur les autres parties).

\section{Construire un modèle de réalisabilité}

\subsection{L'interprétation BHK}

L'interprétation de Brouwer-Heying-Kolmogorov (BHK) désigne une interprétation de la sémantique d'une propositions, non en considérant sa de valeurs de vérité, mais en considérant l'ensemble de ses preuves.

La volonté derrière l'interprétation BHK est de définir une logique constructiviste, en ce qu'une preuve d'une proposition doit explicitement construire les outils auxquels elle fait appel (en particulier, on cherche à avoir la propriété du témoin, qui dit que si $\vdash \exists x, \varphi$ alors il existe $t$ tel que $\vdash \varphi[t/x]$). Comme le principe du tiers exclu est non constructif, on se place dans le cadre de la logique intuitionniste.

On suppose qu'on a un ensemble de propositions $\Phi$ et un ensemble de témoins, $\bX$. L'interprétation BHK donne une façon de penser une relation signifiant \textit{être une preuve de}, qu'on notera $x\reali \varphi$~:
\begin{itemize}
\item on suppose qu'on a défini ce que signifie $x\reali\varphi$ pour $\varphi$ atomique
\item on n'a jamais $x\reali \bot$
\item $x\reali\psi \to\chi$ signifie que $x$ est une fonction (en un sens intuitif pour le moment) qui à $y\in \bX$ tel que $y\reali \psi$, associe $x(y)\in \bX$ tel que $x(y)\reali \chi$.
\item $x\reali \psi\land \chi$ signifie que $x$ est une paire de deux éléments $y\reali \psi$ et $z\reali \chi$~: prouver la conjonction de deux formules signifie avoir une preuve de chaque formule.
\item $x\reali\psi\lor \chi$ signifie que $x$ s'écrit $(i,y)$ où, si $i = 1$, $y\reali \psi$ et si $i = 2$, $y\reali \chi$. Cela correspond à une union disjointe (en particulier, une preuve d'une disjonction garde la trace de laquelle des deux propositions est prouvée).
\item $x\reali\forall a, \psi$ signifie que $x$ représente une fonction qui, pour tout $v$ du domaine de discours, renvoie $x(a)\reali \psi[v/a]$. Une preuve d'une quantification universelle est ainsi une fonction qui, au terme auquel on veut appliquer la proposition, renvoie une preuve de cette proposition appliquée au terme.
\item $x\reali \exists a, \psi$ signifie que $x$ s'écrit comme une paire $(t,y)$ où $t$ est un terme et $y\reali \psi[t/a]$. Ainsi, donner une preuve d'une proposition existentielle signifie donner un témoin (quel objet $t$ est tel que $\psi[t/a]$) et une preuve que ce témoin vérifie effectivement la proposition (une preuve de $\psi[t/a]$).
\end{itemize}

Dans cette interprétation, dire qu'une formule $\varphi$ est vraie signifie
\[\exists x \in \bX, x \reali \varphi\]
On souhaite donc que cette notion de vérité définisse une théorie logique, c'est-à-dire que la propriété ``être vraie'' soit stable par déduction naturelle. On appellera adéquation la propriété correspondant à cette stabilité par déduction naturelle.

\subsection{Sur l'adéquation}

Rappelons les règles de déduction naturelle, et profitons-en pour clarifier les choix syntaxiques que nous faisons. Tout d'abord, on considère une logique multi-sortée, qui nous force à avoir une définition plus chargée d'une signature~:
\begin{defi}[Signature]
    Une signature $\Sigma$ est la donnée~:
    \begin{itemize}
    \item d'un ensemble $\mathcal S$ de sortes.
    \item d'un ensemble $\mathcal F$ de symboles de fonctions, et d'une fonction $\alpha : \mathcal F\to \mathcal S^* \times \mathcal S$ donnant à chaque symbole son arité.
    \item d'un ensemble $\mathcal R$ de symboles de relations, et d'une fonction $\alpha : \mathcal R \to \mathcal S^*$ donnant à chaque symbole son arité.
    \end{itemize}
\end{defi}

On se fixe donc une signature $\Sigma$ pour le reste de cette présentation de la syntaxe utilisée. On fixe de plus un ensemble $\mathcal X_1$, dénombrable, de variables du premier ordre, qu'on notera $\bx,\by,\ldots$ (on gardera les lettres non en gras pour désigner d'autres variables). L'ensemble des termes typés est défini par les règles suivantes, où $S,S_1,\ldots$ désignent des éléments de $\mathcal S$ et $\Gamma$ désigne un contexte de typage du premier ordre (une liste de couples $(x,S)$ où $x \in \mathcal X_1$ et $S\in\mathcal S$), et $f \in \mathcal F$~:
\begin{center}
    \begin{prooftree}
        \infer0{\Gamma, \bx : S \vdash \bx : S}
    \end{prooftree}
    \qquad
    \begin{prooftree}
        \hypo{\Gamma\vdash \bt_1 : S_1}
        \hypo{\cdots}
        \hypo{\Gamma\vdash \bt_n : S_n}
        \hypo{\alpha(f) = (S_1,\ldots,S_n,S)}
        \infer4{\Gamma\vdash f(\bt_1,\ldots,\bt_n) : S}
    \end{prooftree}
\end{center}

On définit maintenant les propositions, mais comme celles-ci contiennent des constructions d'ordre $2$, on doit d'abord introduire des variables d'ordre $2$. On notera $\mathcal X_2$ l'ensemble des variables d'ordre $2$ (ces variables seront notées en majuscules), et on appellera contexte de typage d'ordre deux une liste de couples $(X,S)$ où $X \in \mathcal X_2$ et $S \in \mathcal S^*$. On notera $X \subseteq (S)$ pour $(X,S)$.

\begin{defi}[Propositions]
    On définit les propositions (bien formées) par les règles de typage suivantes~:
    \begin{center}
        \begin{prooftree}
            \hypo{\Gamma\vdash \bt_1 : S_1}
            \hypo{\cdots}
            \hypo{\Gamma\vdash \bt_n : S_n}
            \infer3{\Gamma\mid\Delta, X \subseteq (S_1,\ldots,S_n) \vdash X(\bt_1,\ldots,\bt_n) : \Propo}
        \end{prooftree}
        \quad
        \begin{prooftree}
            \hypo{\Gamma\vdash \bt_1 : S_1}
            \hypo{\cdots}
            \hypo{\Gamma\vdash \bt_n : S_n}
            \hypo{\alpha(R) = (S_1,\ldots,S_n)}
            \infer4{\Gamma\mid\Delta\vdash R(\bt_1,\ldots,\bt_n) : \Propo}
        \end{prooftree}

        \vspace{0.5cm}
        
        \begin{prooftree}
            \hypo{\Gamma\mid\Delta\vdash\varphi : \Propo}
            \hypo{\Gamma\mid\Delta\vdash \psi : \Propo}
            \infer2{\Gamma\mid\Delta\vdash \varphi\to\psi : \Propo}
        \end{prooftree}
        \quad
        \begin{prooftree}
            \hypo{\Gamma, \bx : S\mid \Delta\vdash \varphi : \Propo}
            \infer1{\Gamma\mid\Delta\vdash \forall \bx^S, \varphi : \Propo}
        \end{prooftree}
        \quad
        \begin{prooftree}
            \hypo{\Gamma\mid \Delta, X \subseteq(S_1,\ldots,S_n)\vdash \varphi : \Propo}
            \infer1{\Gamma\mid\Delta\vdash \forall X^{S_1,\ldots,S_n}, \varphi : \Propo}
        \end{prooftree}
    \end{center}
\end{defi}

On définit de façon usuelles les notions de variables libres dans les termes et les propositions, on notera $\varlib{\bt}$ (respectivement $\varlib{\varphi}$) pour cela. Les variables libres d'une proposition contiennent potentiellement des éléments de $\mathcal X_1$ et de $\mathcal X_2$. On considère aussi que toutes nos définitions sont faite à $\alpha$-renommage près, de façon standard.

On définit aussi la substitution d'ordre $1$ et d'ordre $2$.

\begin{defi}[Substitution du premier ordre]
    Soit $\Gamma, \bx : S\vdash \bt : S'$ et $\Gamma\vdash \bu : S$, on définit alors $\Gamma\vdash \bt[\bu/\bx] : S'$ par induction sur $\bt$~:
    \begin{itemize}
        \item si $\bt = \bx$ alors $\bt[\bu/\bx] = \bu$.
        \item si $\bt = \by$, où $\by \neq \bx$, alors $\bt[\bu/\bx] = \bt$.
        \item si $\bt = f(\bt_1,\ldots,\bt_n)$ alors $\bt[\bu/\bx] = f(\bt_1[\bu/\bx],\ldots,\bt_n[\bu/\bx]$.
    \end{itemize}
    On peut maintenant, pour $\Gamma, \bx : S\mid\Delta\vdash \varphi : \Propo$ et $\Gamma\vdash \bt : S$, définir $\Gamma\mid\Delta\vdash\varphi[\bt/\bx] : \Propo$~:
    \begin{itemize}
        \item si $\varphi = X(\bt_1,\ldots,\bt_n)$ alors $\varphi[\bt/\bx] = X(\bt_1[\bt/\bx],\ldots,\bt_n[\bt/\bx])$.
        \item si $\varphi = R(\bt_1,\ldots,\bt_n)$ alors $\varphi[\bt/\bx] = R(\bt_1[\bt/\bx],\ldots,\bt_n[\bt/\bx])$.
        \item si $\varphi = \psi \to \chi$ alors $\varphi[\bt/\bx] = \psi[\bt/\bx] \to \chi[\bt/\bx]$.
        \item si $\varphi = \forall \by^{S'}, \psi$ et que $\by\notin \varlib{\bt}$, alors $\varphi[\bt/\bx] = \forall \by^{S'}, \psi[\bt/\bx]$.
        \item si $\varphi = \forall X^{S_1,\ldots,S_n}, \psi$ alors $\varphi[\bt/\bx] = \forall X^{S_1,\ldots,S_n}, \psi[\bt/\bx]$.
    \end{itemize}
\end{defi}

Par une induction sur les termes et les propositions, on peut montrer que la substitution préserve le typage, c'est-à-dire qu'on a bien la relation de typage décrite dans la définition~: $\Gamma\vdash \bt[\bu/\bx] : S'$ et $\Gamma\mid\Delta\vdash \varphi[\bt/\bx] : \Propo$.

\begin{defi}[Substitution du second ordre]
    Soit $\Gamma\mid \Delta, X : S_1,\ldots,S_n\vdash \varphi : \Propo$ et $\Gamma, \bx_1 : S_1,\ldots, \bx_n : S_n\mid\Delta\vdash \psi : \Propo$, alors on définit $\Gamma\mid\Delta\vdash\varphi[X/\psi(\bx_1,\ldots,\bx_n)]$ par induction sur $\varphi$~:
    \begin{itemize}
        \item si $\varphi = X(\bt_1,\ldots,\bt_n)$ alors $\varphi[X/\psi(\bx_1,\ldots,\bx_n)] = \psi[\bt_1/\bx_1,\ldots,\bt_n/\bx_n]$.
        \item si $\varphi = Y(\bt_1,\ldots,\bt_n)$
        \item si $\varphi = R(\bt_1,\ldots,\bt_n$ alors $\varphi[X/\psi(\bx_1,\ldots,\bx_n)] = \varphi$.
        \item si $\varphi = \chi \to \theta$ alors $\varphi[X/\psi(\bx_1,\ldots,\bx_n)] = \chi[X/\psi(\bt_1,\ldots,\bt_n)] \to \theta[X/\psi(\bt_1,\ldots,\bt_n)]$.
        \item si $\varphi = \forall \bx^S, \chi$ et $\bx \notin \varlib{\psi}$, alors $\varphi[X/\psi(\bx_1,\ldots,\bx_n)] = \forall \bx^S, \chi[X/\psi(\bt_1,\ldots,\bt_n)]$.
        \item si $\varphi = \forall Y^{S'_1,\ldots,S'_n}, \chi$ et $Y\notin\varlib{\psi}$, alors $\varphi[X/\psi(\bx_1,\ldots,\bx_n)] = \forall Y^{S'_1,\ldots,S'_n}, \chi[X/\psi(\bx_1,\ldots,\bx_n)]$.
    \end{itemize}
\end{defi}

Là encore, une induction nous permet de vérifier que cette substitution interagit correctement avec le typage.

Cela nous permet enfin d'introduire la déduction naturelle. Nos règles sont très proches de la déduction naturelle usuelle, mais la volonté de maîtriser précisément les variables du premier ordre et du second ordre nous oblige à considérer trois contextes~: ceux du premier et second ordre, et le contexte logique des formules prises pour hypothèses.

\begin{defi}[Déduction naturelle]
    On définit la relation $\Gamma\mid\Delta\mid\Xi\vdash\varphi$, où $\Gamma$ est un contexte de typage du premier ordre, $\Delta$ du second ordre, $\Xi$ un contexte logique, c'est-à-dire ici une liste de propositions, et $\varphi$ une proposition, par les règles suivantes (toutes les formules dans $\Xi$ et $\varphi$ sont considérées bien typées dans les contextes $\Gamma, \Delta$)~:
    \begin{center}
        \begin{prooftree}
            \hypo{\varphi \in \Xi}
            \infer1[Ax]{\Gamma\mid\Delta\mid\Xi\vdash \varphi}
        \end{prooftree}
        \qquad
        \begin{prooftree}
            \hypo{\Gamma\mid\Delta\mid\Xi, \varphi\vdash \psi}
            \infer1[$\to_\mathrm i$]{\Gamma\mid\Delta\mid\Xi\vdash \varphi \to \psi}
        \end{prooftree}
        \qquad
        \begin{prooftree}
            \hypo{\Gamma\mid\Delta\mid\Xi\vdash \varphi\to \psi}
            \hypo{\Gamma\mid\Delta\mid\Xi\vdash \varphi}
            \infer2[$\to_\mathrm e$]{\Gamma\mid\Delta\mid\Xi\vdash \psi}
        \end{prooftree}

        \vspace{0.5cm}

        \begin{prooftree}
            \hypo{\Gamma, \bx : S\mid\Delta\mid\Xi\vdash\varphi}
            \infer1[$\forall_\mathrm i^1$]{\Gamma\mid\Delta\mid\Xi\vdash \forall \bx^S, \varphi}
        \end{prooftree}
        \qquad
        \begin{prooftree}
            \hypo{\Gamma\mid\Delta\mid\Xi\vdash \forall x^S, \varphi}
            \hypo{\Gamma\vdash \bt : S}
            \infer2[$\forall_\mathrm e^1$]{\Gamma\mid\Delta\mid\Xi\vdash \varphi[\bt/\bx]}
        \end{prooftree}

        \vspace{0.5cm}

        \begin{prooftree}
            \hypo{\Gamma\mid\Delta, X \subseteq S_1,\ldots,S_n\mid\Xi\vdash \varphi}
            \infer1[$\forall_\mathrm i^2$]{\Gamma\mid\Delta\mid\Xi\vdash \forall X^{S_1,\ldots,S_n}, \varphi}
        \end{prooftree}
        \qquad
        \begin{prooftree}
            \hypo{\Gamma\mid\Delta\mid\Xi\vdash \forall X^{S_1,\ldots,S_n}, \varphi}
            \hypo{\Gamma, \bx_1 : S_1,\ldots, \bx_n : S_n\mid \Delta\vdash \psi : \Propo}
            \infer2[$\forall_\mathrm e^2$]{\Gamma\mid\Delta\mid\Xi\vdash \varphi[X/\psi(\bx_1,\ldots,\bx_n)]}
        \end{prooftree}
    \end{center}
\end{defi}

Dire que notre interprétation est adéquate signifie alors que $\reali$ est stable par les mêmes règles. Cependant, il n'est pas clair comment interpréter au niveau de la relation $\reali$ le séquent $\Xi\vdash \varphi$ (le contexte de typage, lui, peu être considéré comme extérieur à l'étude logique). Pour l'interpréter, on va considérer que nos éléments $x$ de $\bX$ s'interprètent comme des fonctions $f_x$, $n$-aires (où $\Xi = \varphi_1,\ldots,\varphi_n$), telles que
\[\forall x_1,\ldots,x_n, (\forall i \in \{1,\ldots,n\}, x_i \reali \varphi_i)\implies f_x(x_1,\ldots,x_n)\reali\varphi\]

Dans ce contexte, l'adéquation signifie que notre langage a besoin des constructions suivantes~:
\begin{itemize}
    \item à cause des règles $\to_\mathrm i$ et $\to_\mathrm e$, il doit y avoir un moyen d'abstraire une variable et d'en instancier une, permettant de transformer un élément $x\in\bX$ interprété comme une fonction $(x_1,\ldots,x_n) \mapsto f_x(x_1,\ldots,x_n)$ en une fonction $(x_1,\ldots,x_{n-1})\mapsto (y\mapsto f_x(x_1,\ldots,x_{n-1},y))$ et une fonction $(x_1,\ldots,x_n)\mapsto f_x(x_1,\ldots,x_n)$ avec un objet $y\in\bX$ en une fonction $(x_1,\ldots,x_{n-1})\mapsto f_x(x_1,\ldots,x_{n-1},y)$.
    \item les règles sur $\forall^1$ suggèrent de pouvoir représenter nos termes à l'intérieur même de notre ensemble $\bX$~: on veut par exemple avoir un objet de $\bX$ pour chaque entier $n\in\bN$.
    \item nous reviendrons plus tard sur les quantification du second ordre, qui seront interprétées par des contextes supplémentaires.
\end{itemize}

Un élément important de cette façon de faire est la modularité~: dans ce qui est prescrit ici, on ne donne aucune condition sur la forme globale de $\bX$ mais seulement sur des constructions qui doivent exister, ce qui signifie que l'on pourra modifier $\bX$ en gardant la propriété d'adéquation tant que nos constructions restent présentes. Comme on veut considérer $\bX$ comme un langage de programmation, ce point est essentiel, puisqu'on a très souvent envie d'ajouter des constructions ou des primitives à un langages en souhaitant préserver les propriétés déjà établies.

\subsection{A propos du second ordre}

Le choix de la logique du second ordre nous a permis d'alléger considérablement le nombre de constructeurs introduits pour construire des formules. En effet, la combinaison de $\to$ et $\forall^2$ permet d'encoder les constantes $\top$ et $\bot$ ainsi que $\lor$ et $\land$, mais aussi les quantifications existentielles.

Donnons donc ces différents encodages~:
\begin{align*}
    \top &\defeq \forall X, X \to X\\
    \bot &\defeq \forall X, X\\
    \varphi \land \psi &\defeq \forall X, (\varphi \to \psi \to X) \to X\\
    \varphi \lor \psi &\defeq \forall X, (\varphi \to X) \to (\psi \to X) \to X\\
    \exists x^S, \varphi &\defeq \forall X, (\forall x^S, (\varphi \to X)) \to X\\
    \exists X^{S_1,\ldots,S_n}, \varphi &\defeq \forall Y, (\forall X^{S_1,\ldots,S_n}, (\varphi \to Y))\to Y
\end{align*}

\begin{rmk}
    On n'écrit pas d'arité sur les $X$ et $Y$ dans ces définitions car ceux-ci sont d'arité nulle~: ils sont remplacés par des formules closes.
\end{rmk}

Nous ne donnons pas ici les règles liées à ces différents constructeurs, mais on peut prouver que les règles logique usuelles sont dérivables à partir de notre encodage et des règles déjà introduites. Donnons au moins un exemple avec $\exists_\mathrm e^1$~:

\begin{center}
    \begin{prooftree}
        \hypo{\Gamma\mid\Delta\mid\Xi\vdash \exists x^S, \varphi}
        \infer0{\Gamma\mid\Delta\vdash \psi : \Propo}
        \infer2[$\forall_\mathrm e^2$]{\Gamma\mid\Delta\mid\Xi\vdash (\forall x^S, \varphi \to \psi)\to \psi}
        \hypo{\Gamma, x : S\mid\Delta\mid\Xi, \varphi\vdash \psi}
        \infer1[$\to_\mathrm i$]{\Gamma, x : S\mid\Delta\mid\Xi\vdash \varphi\to\psi}
        \infer1[$\forall_\mathrm i^1$]{\Gamma\mid\Delta\mid\Xi\vdash \forall x^S, \varphi\to\psi}
        \infer2[$\to_\mathrm e$]{\Gamma\mid\Delta\mid\Xi\vdash \psi}
    \end{prooftree}
\end{center}

Passons maintenant à comment interpréter le second ordre dans notre modèle de réalisabilité. Une variable du second ordre représente une partie de l'univers de discours. Par exemple, $X^\bN$ représente une partie de $\mathbb N$. Les parties d'un ensemble $X$, en logique classique, donc exactement les éléments de $2^X$~: cela est dû au fait que le sens d'une proposition est toujours un élément de $2$. Comme, dans notre cas, le sens d'une proposition doit être l'ensemble de ses témoins, on remplace $2$ par $\mathcal P(\bX)$. On obtient donc comme interprétation d'une variable $X^\bN$ une fonction $\mathbb N \to \mathcal P (\bX)$ Plus généralement, en ayant défini une interprétation $\bS$ de chaque sorte $S$, une variable $X^{S_1,\ldots,S_n}$ sera interprétée par une fonction $\bS_1\times\cdots\times\bS_n \to \mathcal P(\bX)$.

Pour permettre à notre relation de réalisabilité de prendre en compte le second ordre, nous allons paramétrer cette relation par une assignation (partielle) des variables du second ordre à des interprétations comme décrites plus haut.

\'Etant donnée une interprétation des variables du second ordre, qu'on notera $\rho$, on notera $x\reali_\rho \varphi$ pour dire que $x$ est une preuve de $\varphi$ dans laquelle on interprète les variables libres du second ordre de $\varphi$ par $\rho$. Cela nous permet alors de définir \[x\reali_\rho \forall X^{S_1,\ldots,S_n}, \varphi \defeq x\in \bigcap_{A : \bS_1\times\cdots\times \bS_n \to \mathcal P(\bX)} \{x \in \bX\mid x\reali_{\rho[X\leftarrow A]} \varphi\}\]

Cette interprétation ne fonctionne que dans le cas où $\varphi$ est close pour les variables du premier ordre, mais on peut adapter l'idée précédente au premier ordre~: on ajoute une assignation $\sigma$ de $\mathcal X_1$ vers l'ensembles des $\bS$ pour $S\in\mathcal S$. Ce faisant, on peut définir $x\reali_\rho^\sigma \varphi$. Cependant, le sens de $x\reali \forall x^S, \varphi$ change alors~: cela signifie que $x$ est une preuve de tous les $\varphi[\bt/\bx]$, et non une fonction associant $s\in \bS$ à une preuve de $\varphi[s/\bx]$. On a donc deux possibilités d'interprétation du $\forall^1$~: comme une réalisation uniforme et comme une réalisation fonctionnelle. Nous verrons plus tard qu'on peut en fait exprimer la seconde à travers la première.

\subsection{Le langage de programmation}

Maintenant que nous avons donné les idées principales à propos de la relation de réalisabilité, précisons l'ensemble $\bX$ que nous étudions. Comme nous avons parlé d'interpréter nos éléments comme des fonctions, il semble naturel de considérer le $\lambda$-calcul, dans lequel tout terme peut se voir comme une fonction. On présentera succinctement le $\lambda$-calcul, principalement pour s'accorder sur les conventions et les notations.

\begin{defi}[$\lambda$-termes]\label{def.lam}
    On fixe un ensemble dénombrable $\mathcal X_\Lambda$ de $\lambda$-variables, qu'on notera $x,y,\ldots$ On définit l'ensemble $\Lambda$ des $\lambda$-termes comme l'ensemble engendré par la grammaire suivante~:
    \[t,u ::= x \mid \lambda x. t\mid t\;u\]
\end{defi}

Pour le parenthésage, on prendra la convention de minimiser le nombre de parenthèses en les écrivant seulement quand nécessaire, et en prenant l'application (la construction $t\;u$) comme associative à gauche, c'est-à-dire que $t\;u\;v$ se lit $(t\;u)\;v$. On considère toujours les $\lambda$-termes à $\alpha$-conversion près, c'est-à-dire que changer le nom de variables liées par de nouvelles variables (dans le $\lambda$ qui lie cette variable y compris) ne change pas le terme. On définit la substitution simultanée et la simple substitution~:

\begin{defi}[Substitution de termes]
    Soit $\nu : \mathcal X_\Lambda \rightharpoonup \Lambda$ une substitution de termes et $t\in \Lambda$, on notera $\nu(t)$ le terme défini par induction sur $t$ par~:
    \begin{itemize}
        \item si $t = x$ et $x \in \domm(\nu)$ alors $\nu(t) = \nu(x)$.
        \item si $t = x$ et $x\notin\domm(\nu)$ alors $\nu(t) = x$.
        \item si $t = \lambda x.u$ et $x\notin\bigcup_{y\in\domm(\nu)}\varlib{\nu(y)}$ alors $\nu(t) = \lambda x.\nu(u)$.
        \item si $t = u\;v$ alors $\nu(t) = (\nu(u))\;(\nu(v))$.
    \end{itemize}

    Si $\nu$ est définie uniquement sur un élément $x$, alors pour $u = \nu(x)$ on notera $t[u/x] = \nu(t)$.
\end{defi}

On définit aussi la $\beta$-réduction.

\begin{defi}[$\beta$-réduction]
    On définit d'abord la relation $\mapsto\subseteq \Lambda\times\Lambda$ par la règle
    \[(\lambda x.t)u \mapsto t[u/x]\]
    La relation $\rhd\subseteq \Lambda\times\Lambda$ est alors la plus petite relation contenant $\mapsto$ et stable par les règles
    \begin{center}
        \begin{prooftree}
            \hypo{t \rhd u}
            \infer1{\lambda x.t \rhd \lambda x.u}
        \end{prooftree}
        \qquad
        \begin{prooftree}
            \hypo{t\rhd t'}
            \infer1{t\;u\rhd t'\;u}
        \end{prooftree}
        \qquad
        \begin{prooftree}
            \hypo{u \rhd u'}
            \infer1{t\;u\rhd t\;u'}
        \end{prooftree}
    \end{center}
\end{defi}

Commençons maintenant à présenter un premier modèle de réalisabilité, basé sur ce $\lambda$-calcul. Nous utiliserons les notions d'environnement définies plus tôt à la fois pour le premier et le second ordre. On se place dans le cas d'une signature sans symbole de relation, et on considère que $\Nat$ est l'unique sorte, pour simplifier notre modèle dans un premier temps. On possède les fonctions habituelles de l'arithmétique~: $S,+,\times$ et la constante $0$. On interprétera $\Nat$ par l'ensemble $\bN$. On écrira cependant nos définitions en toute généralité, en particulier pour les sortes $S$ que l'on notera $S$ et leur interprétation sémantique $\bS$, pour que les définitions puissent être appliquées de façon identique pour les autres signatures que l'on peut choisir.

Précisons d'abord les interprétations de variables qui nous intéressent. On dira qu'une interprétation $\sigma$ du premier ordre, c'est-à-dire une fonction partielle $\sigma : \mathcal X_1 \rightharpoonup \bN$, est adéquate vis à vis d'un contexte du premier ordre $\Gamma$ si pour tout $(x,S) \in \Gamma$, on a $\sigma(x) \in \bS$.

Moralement, une variable du second ordre doit pouvoir être remplacée par la valeur de vérité d'une formule, c'est-à-dire dans la logique classique une partie des objets du premier ordre. Ici, on a dit que la valeur de vérité d'une formule est, plutôt qu'une valeur dans $\{0,1\}$, une partie de notre langage de programmation $\Lambda$. On est donc tentés d'interpréter une formule par un élément $A \in \mathcal P(\Lambda)$, mais on verra plus tard que cette interprétation cause des problèmes. On considère donc à la place que $A \in \bP$, où $\bP$ sera une famille de parties de $\Lambda$ que l'on décrira plus tard, mais qui se rapproche grandement de $\mathcal P(\Lambda)$ dans l'interprétation qu'on en fait.

Une interprétation $\rho$ du second ordre, alors, est une fonction partielle $\rho$ qui, à un élément de $\mathcal X_2$, associe une fonction $\bS_1\times\cdots\times\bS_n \to \bP$. On dira que $\rho$ est adéquat pour une proposition $\Gamma\mid\Delta\vdash \varphi : \Propo$ si, pour tout $(X, (S_1,\ldots,S_n))\in \Delta$, $\rho(X) : \bS_1\times\cdots\times\bS_n \to \bP$.

On peut maintenant définir la relation de réalisabilité.

\begin{defi}[Relation de réalisabilité]
    Soit une proposition $\varphi$, $\sigma\models \varphi$ et $\rho\models \varphi$. On définit $\trad\varphi_\rho^\sigma \in \mathcal \bP$ par induction sur $\varphi$~:
    \begin{itemize}
        \item si $\varphi = X(\bt_1,\ldots,\bt_n)$ alors $\trad\varphi_\rho^\sigma = \rho(X)(\sigma(\bt_1),\ldots,\sigma(\bt_n))$.
        \item si $\varphi = \psi \to \chi$ alors $\trad{\varphi\to\chi}_\rho^\sigma = \{t \in \Lambda\mid \forall u \in \trad{\psi}_\rho^\sigma, t\;u \in \trad{\chi}_\rho^\sigma\}$.
        \item si $\varphi = \forall \bx^S, \psi$ alors $\displaystyle\trad{\forall \bx^S, \psi}_\rho^\sigma = \bigcap_{n \in \bS} \trad{\psi}_\rho^{\sigma[\bx\leftarrow n]}$
        \item si $\varphi = \forall X^{S_1,\ldots,S_n}, \psi$ alors $\displaystyle\trad{\forall X^{S_1,\ldots,S_n},\psi}_\rho^\sigma = \bigcap_{F : \bS_1\times\cdots\times\bS_n \to \mathcal P(\Lambda)} \trad\psi_{\rho[X\leftarrow F]}^\sigma$
    \end{itemize}

    On notera $t\reali_\rho^\sigma \varphi$ si $t\in\trad\varphi_\rho^\sigma$ et $t\reali\varphi$ pour $t\reali_\varnothing^\varnothing\varphi$, et on dira alors que $t$ réalise (ou est un réaliseur de) $\varphi$.
\end{defi}

On souhaite maintenant montrer l'adéquation. L'interprétation de $t\in \Lambda$ comme une fonction $n$-aire peut se faire en prenant $(t_1,\ldots,t_n) \mapsto t[t_1/x_1,\ldots,t_n/x_n]$ en considérant qu'on a ordonné nos variables. Avec cette définition, l'interprétation de $\varphi_1,\ldots,\varphi_n\vdash \varphi$ devient
\[\forall t_1,\ldots,t_n \in \Lambda, (\forall i \in \{1,\ldots,n\}, t_i \reali \varphi_i)\implies t[t_1/x_1,\ldots,t_n/x_n]\reali\varphi\]

On voit alors que la règle d'axiome est vérifiée directement en prenant le terme $x_i$, qui vaudra $t_i$ après substitution et par hypothèse $t_i\reali \varphi_i = \varphi$. Cette règle est exactement celle qu'on attend du typage des $\lambda$-termes, et la prouvabilité est une relation syntaxique~: il convient donc de travailler directement avec un système de type et de montrer que ce système de type est adéquat en ce qu'un terme typé est un réaliseur.

\begin{defi}[Système de type]
    On définit la relation $\Gamma\mid\Delta\mid\Xi\vdash t : \varphi$, où $\Gamma$ et $\Delta$ sont comme introduits plus tôt, et $\Xi$ est une liste de couples $(x,\varphi)$ où $x \in \mathcal X_\Lambda$ et $\varphi \in \Propo$, par les règles suivantes~:
    \begin{center}
        \begin{prooftree}
            \hypo{(x : \varphi) \in \Xi}
            \infer1[Ax]{\Gamma\mid\Delta\mid\Xi\vdash x : \varphi}
        \end{prooftree}
        \qquad
        \begin{prooftree}
            \hypo{\Gamma\mid\Delta\mid\Xi, x : \varphi\vdash t : \psi}
            \infer1[$\to_\mathrm i$]{\Gamma\mid\Delta\mid\Xi\vdash \lambda x. t : \varphi \to \psi}
        \end{prooftree}
        \qquad
        \begin{prooftree}
            \hypo{\Gamma\mid\Delta\mid\Xi\vdash t : \varphi\to \psi}
            \hypo{\Gamma\mid\Delta\mid\Xi\vdash u : \varphi}
            \infer2[$\to_\mathrm e$]{\Gamma\mid\Delta\mid\Xi\vdash t\;u : \psi}
        \end{prooftree}

        \vspace{0.5cm}

        \begin{prooftree}
            \hypo{\Gamma, \bx : S\mid\Delta\mid\Xi\vdash t : \varphi}
            \infer1[$\forall_\mathrm i^1$]{\Gamma\mid\Delta\mid\Xi\vdash t : \forall \bx^S, \varphi}
        \end{prooftree}
        \qquad
        \begin{prooftree}
            \hypo{\Gamma\mid\Delta\mid\Xi\vdash t : \forall x^S, \varphi}
            \hypo{\Gamma\vdash \bt : S}
            \infer2[$\forall_\mathrm e^1$]{\Gamma\mid\Delta\mid\Xi\vdash t : \varphi[\bt/\bx]}
        \end{prooftree}

        \vspace{0.5cm}

        \begin{prooftree}
            \hypo{\Gamma\mid\Delta, X : S_1,\ldots,S_n\mid\Xi\vdash t : \varphi}
            \infer1[$\forall_\mathrm i^2$]{\Gamma\mid\Delta\mid\Xi\vdash t : \forall X^{S_1,\ldots,S_n}, \varphi}
        \end{prooftree}
        \qquad
        \begin{prooftree}
            \hypo{\Gamma\mid\Delta\mid\Xi\vdash t : \forall X^{S_1,\ldots,S_n}, \varphi}
            \hypo{\Gamma, \bx_1 : S_1,\ldots, \bx_n : S_n\mid \Delta\vdash \psi : \Propo}
            \infer2[$\forall_\mathrm e^2$]{\Gamma\mid\Delta\mid\Xi\vdash t : \varphi[X/\psi(\bx_1,\ldots,\bx_n)]}
        \end{prooftree}
    \end{center}
\end{defi}

Nous avons donc vu que la règle Ax est directement vérifiée par la forme du lemme d'adéquation que nous voulons montrer. Passons maintenant à la règle $\to_\mathrm i$. Pour celle-ci, supposons qu'on a $\nu$ tel que $t_i = \nu(x_i) \reali \varphi_i$, $\nu(x) \reali \varphi$ et $t[t_1/x_1,\ldots,t_n/x_n, \nu(x)/x]\reali \varphi$, on veut en déduire que $\lambda x.t[t_1/x_1,\ldots,t_n/x_n]\reali \varphi\to\psi$. Pour cela, si on a $u \reali \varphi$, alors on veut que $(\lambda x.\nu(t))\;u\reali \psi$~: il n'y a pas de raison que cela soit vrai. Cependant, en utilisant l'hypothèse d'induction avec $\nu(x_i) = t_i$ et $\nu(x) = u$, on obtient $t[t_i/x_i, t/x]$ et on voit que $(\lambda x.t[t_i/x_i])u \rhd t[t_i/x_i, t/x]$ (quitte à faire de l'$\alpha$-conversion), on souhaite donc que $\trad\varphi$ soit stable par antiréduction pour permettre la validité de cette règle.

Cette exigence peut sembler artificielle, mais elle vient simplement du fait que le $\lambda$-calcul est un système syntaxique, et il contient donc plusieurs termes qui peuvent avoir le même sens~: en l'occurrence, ici, $(\lambda x.t)u$ et $t[u/x]$ ont tous les deux le sens de $f(x)$ dans un cadre plus sémantique. On pourrait donc décider de travailler dans $\Lambda/=_\beta$, mais l'analyse donnée plus tôt nous montre que cette exigence n'est en fait pas nécessaire, puisqu'il suffit de conserver la stabilité par antiréduction. Une partie stable par antiréduction est donc, en un sens, une partie ayant un contenu sémantique. Remarquons quand même que ce choix dépend largement du formalisme utilisé (ici le $\lambda$-calcul) mais aussi de la réduction considérée~: on aurait pu considérer la réduction call-by-value, cherchant à évaluer les arguments avant de faire une substitution, et les parties saturées auraient dû être définies différemment en obligeant l'évaluation de l'argument.

Une autre motivation rendant cette condition plus naturelle est la stabilité qu'elle impose au fait d'être un réaliseur. Avec cette condition, en considérant que $t \reali \varphi$ signifie que $t$ vérifie les spécifications imposées par $\varphi$ (en un sens, qu'il calcule correctement), alors savoir que $t \rhd^* u$ et que $u$ vérifie les spécifications suffit, puisque $t$ va alors calculer comme $u$, qui calcule correctement.

\begin{defi}[Partie saturée]
    On dit qu'une partie $S\subseteq \Lambda$ est saturée si la condition suivante est vérifiée~:
    \[\forall t,u\in \Lambda, t \rhd u \land u \in S \implies t \in S\]

    On note $\SAT$ l'ensemble des parties saturées.
\end{defi}

On peut montrer que pour avoir $\trad\varphi \in \SAT$, il suffit de l'imposer sur les interprétations des variables du second ordre (et des symboles de relations, mais nous n'en avons pas ici). Pour cela, on montre d'abord que les opérations ensemblistes usuelles préservent la stabilité.

\begin{lem}
  $(\SAT,\subseteq,\Lambda,\varnothing,\bigcap,\bigcup)$ est un treillis complet.
\end{lem}

\begin{proof}
  Il est évident que $S\in \SAT \implies \varnothing\subseteq S \subseteq \Lambda$ et que ces deux parties sont stables par antiréduction. Supposons que $\{S_i\}_{i\in I}$ est une famille de parties saturées. Si $t\reduc u$ et $u \in \bigcap S_i$, alors pour tout $i \in I$, $t \in S_i$ par saturation de $S_i$, donc $t\in \bigcap S_i$, donc $\bigcap S_i$ est saturée. De même si $u \in \bigcup S_i$, alors on trouve $i$ tel que $u \in S_i$ donc $t\in S_i$ par saturation. Donc $\bigcup S_i$ est saturée. Ainsi $(\SAT,\subseteq,\Lambda,\varnothing, \bigcap,\bigcup)$ est un treillis complet.
\end{proof}

On décide maintenant d'instancier l'interprétation définie précédemment en posant $\bP \defeq \SAT$. De plus, $\SAT$ se comporte correctement vis à vis de notre interprétation, puisque (comme on le montre ci-après) exiger seulement que $\rho(X) \in \SAT$ sur les variables $X$ suffit à ce que $\trad\varphi_\rho^\sigma \in \SAT$.

\begin{lem}[Saturation]
  Pour toute formule $\varphi$, $\trad\varphi_\rho^\sigma\in \SAT$.
\end{lem}

\begin{proof}
  On procède par induction sur $\varphi$~:
  \begin{itemize}
  \item si $\varphi = X(t_1,\ldots,t_n)$ alors $\trad\varphi_\rho^\sigma=\rho(X)(t_1^\sigma,\ldots,t_n^\sigma)\in \SAT$.
  \item si $\varphi = \psi \to \chi$, supposons que $t\reali_\rho^\sigma \psi \to \chi$, $t'\reduc t$ et $u\reali_\rho^\sigma\psi$. Par hypothèse, $t\;u\reali_\rho^\sigma \chi$, et comme par hypothèse d'induction $\trad\chi_\rho^\sigma\in\SAT$ et $t'\;u\reduc t\;u$, on en déduit que $t'\;u\reali_\rho^\sigma \chi$. Ainsi pour tout $u\reali_\rho^\sigma\psi$, $t'\;u\reali_\rho^\sigma\chi$, ce qui signifie que $t'\reali_\rho^\sigma\psi\to\chi$.
  \item les deux interprétations données par des intersections donnent des ensembles saturés car $\SAT$ est un treillis complet.
  \end{itemize}
  Donc par induction $\trad\varphi_\rho^\sigma\in\SAT$.
\end{proof}

On a donc traité les règles Ax et $\to_\mathrm i$ pour l'adéquation. La règle $\to_\mathrm e$ est automatique~: la définition même de $\trad{\psi\to\chi}$ nous assure que la règle est vérifiée. Pour les deux quantificateurs, par contre, il nous faut introduire des lemmes de substitution, pour prouver qu'enrichir l'environnement $\sigma$ (respectivement $\rho$) revient à faire une substitution dans le terme (respectivement la proposition) qu'on interprète.

\begin{lem}\label{lem.subst.1}
  Pour tous $\Gamma,\Delta$, $\varphi$ telle que $\Gamma\mid\Delta\vdash \varphi : \Propo$ et interprétations $\sigma\models \Gamma, \rho \models \Delta$, pour tout terme $\Gamma\vdash \bt : S$, on a
  \[\trad\varphi_\rho^{\sigma[\bx \leftarrow \bt^\sigma]}\iff \trad{\varphi[\bt/\bx]}\reali_\rho^\sigma\]
\end{lem}

\begin{proof}
  Tout d'abord, par une induction sur $\bu$ (que nous ne détaillerons pas), on peut quitte à renommer nos variables considérer que $\forall \bu, \bu^{\sigma[\bx\leftarrow \bt^\sigma]} = (\bu[\bt/\bx])^\sigma$. On raisonne maintenant par induction sur $\varphi$~:
  \begin{itemize}
  \item si $\varphi = X(\bt_1,\ldots,\bt_n)$ alors
    \begin{align*}
      \trad\varphi_\rho^{\sigma[\bx\leftarrow \bt^\sigma]} &= \rho(X)(\bt_1^{\sigma[\bx\leftarrow \bt^\sigma]},\ldots,\bt_n^{\sigma[\bx\leftarrow \bt^\sigma]})\\
      &= \rho(X)((\bt_1[\bt/\bx])^\sigma,\ldots,(\bt_n[\bt/\bx])^\sigma)\\
      &= \trad{\varphi[\bt/\bx]}_\rho^\sigma
    \end{align*}
  \item si $\varphi = \psi \to \chi$ alors
    \begin{align*}
      \trad{\psi\to\chi}_\rho^{\sigma[\bx\leftarrow\bt^\sigma]} &= \{t\in\Lambda\mid \forall u\in\trad\psi_\rho^{\sigma[\bx\leftarrow\bt^\sigma]}, t\;u \in \trad\chi_\rho^{\sigma[\bx\leftarrow\bt^\sigma]}\}\\
      &= \{t\in\Lambda\mid\forall u\in \trad{\psi[\bt/\bx]}_\rho^\sigma, t\;u\in\trad{\chi[\bt/\bx]}_\rho^\sigma\}\\
      &= \trad{\psi[\bt/\bx]\to\chi[\bt/\bx]}_\rho^\sigma\\
      &= \trad{(\psi\to\chi)[\bt/\bx]}_\rho^\sigma
    \end{align*}
  \item si $\varphi = \forall \by^S, \psi$ alors quitte à renommer $\by$ pour s'assurer que la variable n'appartienne pas aux variables libres de $\bt$~:
    \begin{align*}
      \trad{\forall \by^S,\psi}_\rho^{\sigma[\bx\leftarrow\bt^\sigma]} &= \bigcap_{s\in \bS}\trad{\psi}_\rho^{\sigma[\bx\leftarrow\bt^\sigma,\by\leftarrow s]}\\
      &= \bigcap_{s\in\bS}\trad\psi_\rho^{\sigma[\by\leftarrow s, \bx\leftarrow \bt^\sigma]}\\
      &= \bigcap_{s\in\bS}\trad{\psi[\bt/\bx]}_\rho^{\sigma[\by\leftarrow s]}\\
      &= \trad{\forall \bx, \psi[\bt/\bx]}_\rho^\sigma\\
      &= \trad{(\forall \bx, \psi)[\bt/\bx]}_\rho^\sigma
    \end{align*}
  \item si $\varphi = \forall X^{S_1,\ldots,S_n}, \psi$ alors
    \begin{align*}
      \trad{\forall X^{S_1,\ldots,S_n}, \psi}_\rho^{\sigma[\bx\leftarrow \bt^\sigma]} &= \bigcap_{F : \bS_1\times\cdots\times \bS_n\to \SAT}\trad{\psi}_{\rho[X\leftarrow F]}^{\sigma[\bx\leftarrow \bt^\sigma]}\\
      &= \bigcap_{F : \bS_1\times\cdots\times \bS_n \to \SAT}\trad{\psi[\bt/\bx]}_{\rho[X\leftarrow F]}^\sigma\\
      &= \trad{(\forall X^{S_1,\ldots,S_n}, \psi)[\bt/\bx]}_\rho^\sigma
    \end{align*}
  \end{itemize}
  Donc, par induction, $\trad\varphi_\rho^{\sigma[\bx\leftarrow\bt]} = \trad{\varphi[\bt/\bx]}_\rho^\sigma$, d'où le résultat.
\end{proof}

\begin{lem}\label{lem.subst.2}
  Pour tous $\Gamma,\Delta$, si $\Gamma,\bx_1 : S_1,\ldots,\bx_n : S_n\mid\Delta\vdash \psi : \Propo$ alors pour tous $\gamma\models\Gamma, \rho\models \Delta$ on a, en notant $F : (s_1,\ldots,s_n) \mapsto \trad\psi_\rho^{\sigma[\bx_1\leftarrow s_1,\ldots,\bx_n\leftarrow s_n]}$~:
  \[t\reali_{\rho[X\leftarrow F]}^\sigma \varphi \iff t\reali_\rho^\sigma \varphi[\psi(\bx_1,\ldots,\bx_n)/X]\]
\end{lem}

\begin{proof}
  Par induction sur $\varphi$~:
  \begin{itemize}
  \item si $\varphi = X(\bt_1,\ldots,\bt_n)$, alors
    \begin{align*}
      \trad\varphi_{\rho[X\leftarrow F]}^\sigma &= F(\bt_1^\sigma,\ldots,\bt_n^\sigma) \\
      &= \trad\psi_\rho^{\sigma[\bx_1\leftarrow \bt_1^\sigma,\ldots,\bx_n\leftarrow \bt_n^\sigma]} \\
      &= \trad{\psi[\bt_1/\bx_1,\ldots,\bt_n/\bx_n]}_\rho^\sigma\\
      &= \trad{X(\bt_1,\ldots,\bt_n)[\psi(\bx_1,\ldots,\bx_n)/X]}_\rho^\sigma
    \end{align*}
  \item si $\varphi = Y(\bt_1,\ldots,\bt_p)$ où $X\neq Y$ alors $\trad{\varphi}_{\rho[X\leftarrow F]}^\sigma = \trad\varphi_\rho^\sigma = \trad{\varphi[\psi(\bx_1,\ldots,\bx_n)/X]}_\rho^\sigma$.
  \item si $\varphi = \chi \to \chi'$ alors
    \begin{align*}
      \trad{\chi\to\chi'}_{\rho[X\leftarrow F]}^\sigma &= \{t\in\Lambda\mid \forall u \in \trad\chi_{\rho[X\leftarrow F]}^\sigma, t\;u\in \trad{\chi'}_{\rho[X\leftarrow F]}^\sigma\}\\
      &= \{t\in\Lambda\mid \forall u \in \trad{\chi[\psi(\bx_1,\ldots,\bx_n)/X]}_\rho^\sigma, t\;u\in\trad{\chi'[\psi(\bx_1,\ldots,\bx_n)/X]}_\rho^\sigma\}\\
      &= \trad{\chi[\psi(\bx_1,\ldots,\bx_n)/X] \to \chi'[\psi(\bx_1,\ldots,\bx_n)/X]}_\rho^\sigma\\
      &= \trad{(\chi\to\chi')[\psi(\bx_1,\ldots,\bx_n)/X]}_\rho^\sigma
    \end{align*}
  \item si $\varphi = \forall \bx^S, \chi$ alors
    \begin{align*}
      \trad{\forall \bx^S, \chi}_{\rho[X\leftarrow F]}^\sigma &= \bigcap_{s\in\bS}\trad\chi_{\rho[X\leftarrow F]}^{\sigma[\bx\leftarrow s]}\\
      &= \bigcap_{s\in\bS}\trad{\chi/[\psi(\bx_1,\ldots,\bx_n)/X]}_\rho^{\sigma[\bx\leftarrow s]}\\
      &= \trad{\forall \bx^S, \chi[\psi(\bx_1,\ldots,\bx_n)/S]}_\rho^\sigma\\
      &= \trad{(\forall \bx^S, \chi)[\psi(\bx_1,\ldots,\bx_n)/S]}_\rho^\sigma
    \end{align*}
  \item si $\varphi = \forall Y^{S_1,\ldots,S_p},\chi$ alors
    \begin{align*}
      \trad{\forall Y^{S_1,\ldots,S_p}, \chi}_{\rho[X\leftarrow F]}^\sigma &= \bigcap_{G : \bS_1\times\cdots\times\bS_p \to \SAT}\trad\chi_{\rho[X\leftarrow F, Y\leftarrow G]}^\sigma\\
      &= \bigcap_{G : \bS_1\times\cdots\times\bS_p\to\SAT}\trad\chi_{\rho[Y\leftarrow G, X\leftarrow F]}^\sigma\\
      &= \bigcap_{G : \bS_1\times\cdots\times\bS_p\to\SAT}\trad{\chi[\psi(\bx_1,\ldots,\bx_n)/X]}_{\rho[Y\leftarrow G]}^\sigma\\
      &= \trad{\forall Y^{S_1,\ldots,S_n}, \chi[\psi(\bx_1,\ldots,\bx_n)/X]}_\rho^\sigma\\
      &= \trad{(\forall Y^{S_1,\ldots,S_n}, \chi)[\psi(\bx_1,\ldots,\bx_n)/X]}_\rho^\sigma
    \end{align*}
  \end{itemize}
  Donc, par induction, $\trad\varphi_{\rho[X\leftarrow F]}^\sigma = \trad{\varphi[\psi(\bx_1,\ldots,\bx_n)/X]}_\rho^\sigma$.
\end{proof}

Il nous reste alors à montrer le lemme d'adéquation. Là encore, il nous faut rajouter des notations et de la syntaxe, pour pouvoir relier les différents contextes de typages et logique.

\begin{defi}[Valuation de réalisabilité]
  Soit un contexte de typage du premier ordre $\Gamma$, un contexte de typage du second ordre $\Delta$ et un contexte de typage de termes $\Xi$. On dit que des valuations $\sigma, \rho, \nu$ où $\nu : \mathcal X_\Lambda \to \Lambda$ sont adéquates pour $\Gamma,\Delta,\Xi$, ce que l'on note $\nu\reali_\rho^\sigma \Gamma,\Delta,\Xi$, si $\sigma$ est adéquate pour $\Gamma$, $\rho$ est adéquate pour $\Delta$ et si
  \[\forall (x : \varphi) \in \Xi, \nu(x)\reali_\rho^\sigma\varphi\]
\end{defi}

\begin{lem}[Adéquation]
  Soient des contextes $\Gamma,\Delta,\Xi$ et des valuations $\nu\reali_\rho^\sigma \Gamma,\Delta,\Xi$. Soit $t\in\Lambda$ et $\varphi\in\Propo$ tels que $\Gamma\mid\Delta\mid\Xi\vdash t : \varphi$. Alors $\nu(t) \reali_\rho^\sigma \varphi$.
\end{lem}

\begin{proof}
  La preuve se fait par induction sur la relation de typage $\vdash$~:
  \begin{itemize}
  \item Cas Ax~: Pour le cas d'une variable, par hypothèse sur $\nu$, si $\varphi = \varphi_i$ et $x = x_i$ alors $\nu(t) = \nu(x_i)\reali_\rho^\sigma \varphi_i$.
  \item Cas $\to_\mathrm i$~: Supposons que pour tout $\nu \reali_\rho^\sigma \Gamma,\Delta,(\Xi, x : \varphi)$, on a $\nu(t) \reali_\rho^\sigma \psi$. Soit $\nu \reali_\rho^\sigma \Gamma,\Delta,\Xi$, montrons que $\nu(\lambda x.t)\reali_\rho^\sigma \varphi \to \psi$. Soit $u \reali_\rho^\sigma \varphi$, alors $\nu[x \leftarrow u](t)\reali_\rho^\sigma \psi$ puisque $\nu[x\leftarrow u]\reali_\rho^\sigma \Gamma,\Delta,(\Xi, x : \psi)$. De plus, quitte à renommer, on a $\nu[x\leftarrow u](t) = \nu(t)[u/x]$ donc $\nu(t)[u/x]\reali_\rho^\sigma \psi$, donc par saturation $(\lambda x.\nu(t)) u \reali_\rho^\sigma \psi$, donc $\lambda x.\nu(t)\reali_\rho^\sigma \varphi\to\psi$.
  \item Cas $\to_\mathrm e$~: Supposons que $\nu(t)\reali_\rho^\sigma \varphi \to \psi$ et $\nu(u)\reali_\rho^\sigma \varphi$, alors par définition $\nu(t\;u)\reali_\rho^\sigma \psi$.
  \item Cas $\forall_\mathrm i^1$~: On suppose que pour tout $\nu\reali_\rho^\sigma (\Gamma, \bx : S),\Delta,\Xi$, $\nu(t)\reali_\rho^\sigma \varphi$. Soit alors $\nu\reali_\rho^\sigma \Gamma,\Delta\Xi$ et $s \in \bS$, comme $\nu\reali_\rho^{\sigma[\bx\leftarrow s]} (\Gamma,\bx : S), \Delta,\Xi$, on en déduit que $\nu(t)\reali_\rho^{\sigma[\bx\leftarrow s]} \varphi$, ce qui est valide pour tout $s\in \bS$, donc par définition $\nu(t)\reali_\rho^\sigma \forall \bx^S, \varphi$.
  \item Cas $\forall_\mathrm e^1$~: On suppose que $\nu(t)\reali_\rho^\sigma \forall \bx, \varphi$, donc pour tout $\bt : S$, $\sigma(\bt) \in \mathbb S$ donc $\nu(t)\reali_{\rho[\bx\leftarrow \bt^\sigma]}^\sigma \varphi$, c'est-à-dire $\nu(t)\reali_\rho^\sigma \varphi[\bt/\bx]$ par le lemme de substitution du premier ordre.
  \item Cas $\forall_\mathrm i^2$~: On suppose que pour tout $\nu\reali_\rho^\sigma \Gamma, (\Delta, X : S_1,\ldots,S_n), \Xi$, $\nu(t) \reali_\rho^\sigma \varphi$. Soit alors des valuations $\nu\reali_\rho^\sigma\Gamma, \Delta,\Xi$. On voit que pour tout $F : S_1\times\ldots\times S_n \to \SAT$, $\nu\reali_{\rho[X\leftarrow F]}^\sigma \Gamma, (\Delta, X : S_1,\ldots,S_n),\Xi$ donc $\nu(t)\reali_{\rho[X\leftarrow F]}^\sigma \varphi$. Comme cela tient pour tout $F : S_1\times\ldots\times S_n \to \SAT$, on en déduit que $\nu(t)\reali_\rho^\sigma \forall X^{S_1,\ldots,S_n}, \varphi$.
  \item Cas $\forall_\mathrm e^2$~: Si $\nu(t)\reali_\rho^\sigma \forall X^{S_1,\ldots,S_n}, \varphi$ et $\Gamma,x_1 : S_1,\ldots,x_n : S_n\mid\Delta\vdash \psi : \Propo$, alors en particulier $F : (s_1,\ldots,s_n) \mapsto \trad\psi_\rho^{\sigma[x_1\leftarrow s_1,\ldots, x_n \leftarrow s_n]}$ définit une fonction $S_1\times\ldots\times S_n \to\SAT$, donc $\nu(t)\reali_{\rho[X\leftarrow F]}^\sigma \varphi$, c'est-à-dire $\nu(t)\reali_\rho^\sigma \varphi[\psi(\bx_1,\ldots,\bx_n)/X]$ par le lemme de substitution du second ordre.
  \end{itemize}

  On en déduit par induction le résultat.
\end{proof}

On a donc maintenant notre premier modèle de réalisabilité, qui définit effectivement une théorie
\[\mathcal T_\reali \defeq \{\varphi \in \Propo\mid \exists t \in \Lambda, t \reali \varphi\}\]
qui est stable par déduction naturelle. De plus, comme $\varnothing \in \SAT$, d'après notre interprétation, on en déduit que
\[\trad\bot = \bigcap_{S \in \SAT} S = \varnothing\]
donc $\bot$ n'est pas prouvable par $\mathcal T_\reali$~: c'est une théorie cohérente.

Malheureusement, notre modèle n'a pas beaucoup de principes logique intéressants. Pour une théorie parlant de $\bN$, on pourrait s'attendre à avoir le principe d'induction~: ça n'est pas le cas ici. La raison derrière est l'absence d'accès au contenu calculatoire des arguments. Réaliser une proposition de la forme $\forall x^{\Nat}, \psi(x)$ signifie que l'on peut montrer $\psi(x)$ indépendamment de $x$, donc que la preuve de $\psi$ ne changera pas selon $x$. Au contraire, dans la démonstration par récurrence, en considérant $x\reali \psi(0)$ et $s \reali \psi(n) \to \psi(n+1)$, la démonstration qui en est construite, pour $2$, est $s\;(s\;x)$, quand pour $1$ on a $s\;x$. On a donc besoin d'un moyen d'accéder au contenu calculatoire d'un terme. Pour cela, il nous faut encoder nos éléments par des $\lambda$-termes.

\subsection{Relativisation et codage}

Une façon habituelle de représenter les nombres entiers en $\lambda$-calcul est de considérer les entiers de Church, c'est-à-dire
\[\overline n \defeq \lambda f.\lambda x.f^n\;x\]
on peut montrer que toutes les fonctions calculables $\bN\to\bN$ peuvent se représenter (dans un sens fort) comme des $\lambda$-termes. On a donc une façon de représenter les objets du premier ordre (les entiers) au sein du langage. Comme le codage exact des entiers dans le langage de programmation n'importe pas réellement, on se contente de dire qu'il existe, pour tout $n\in \bN$, un terme $\overline n \in \Lambda$, avec un terme $\overline 0$ représentant $0$, un terme $S$ représentant la fonction successeur, ainsi qu'un terme $\rec_{\Nat}$, tel que $\rec_{\Nat}\;x\;y \overline 0 \rhd^* x$ et $\rec_{\Nat}\;x\;y\;\overline{S\;n}\rhd^* y\;\overline n (\rec_{\Nat}\;x\;y\;\overline n)$.

Au niveau logique, on va ajouter un prédicat unaire, $\Nat$, qui exprime d'un terme qu'il représente un entier pris avec son contenu calculatoire, c'est-à-dire qu'on devra fournir à $\Nat(\bt)$ une preuve, qui est l'encodage de l'entier correspondant à $\bt$.

Par exemple, reprenons l'exemple de la récurrence. Supposons qu'on possède $x\reali \psi(0)$ et $s\reali \forall \bn, \Nat(\bn) \to \psi(\bn) \to \psi(\bn+1)$. Ici, notre $s$ a besoin de l'information du $\bn$ auquel il s'applique. Cela signifie en particulier qu'on peut maintenant définir un programme qui, étant donné $\bn$ et son encodage $\overline{\bn}$, peut retourner une preuve de $\psi(\bn)$ en itérant $s$ et en l'appliquant à $x$. On a donc récupéré, depuis notre interprétation par intersection, une interprétation fonctionnelle de la quantification universelle. On peut se demander si la même chose est possible pour une quantification du second ordre~: c'est une idée que nous avons exploré dans ce stage pour avoir diverses version de $\KL$, mais la syntaxe que nous avons finalement adoptée nous permet de ne pas introduire ce genre de quantification supplémentaire. Un prédicat $\psi^{\Nat}$ qui serait possible à représenter par un code serait un prédicat décidable, et avoir des fonctions comme objets du premier ordre nous suffira alors (puisqu'il nous suffira donc de considérer une fonction $\Nat\to\Bool$).

On augmente donc la syntaxe des propositions avec la construction d'encodage de terme~:
\begin{center}
    \begin{prooftree}
        \hypo{\Gamma\vdash \bt : S}
        \infer1{\Gamma\mid\Delta\vdash S(\bt) : \Propo}
    \end{prooftree}
\end{center}

De plus, on définit la relation de réalisabilité pour ce constructeur~:
\[\trad{\Nat(\bt)}_\rho^\sigma \defeq \{t\in \Lambda\mid t \rhd^* \overline{\sigma(\bt)}\}\]
Remarquons que le choix est fait de considérer les termes se réduisant en un entier donné, plutôt que simplement l'entier. Cela se justifie par notre volonté d'avoir des ensembles saturés~: celui-ci est clairement saturé.

Ainsi, pour la preuve d'adéquation, tout fonctionne de la même manière~: il nous suffit de montrer que les lemmes de substitutions sont encore valides. En particulier, comme nous n'avons pas changé les règles de typage, la preuve d'adéquation elle-même ne change pas.

Supposons donc que $\varphi = \Nat(\bu)$ et prouvons le lemme \ref{lem.subst.1}~:
\begin{align*}
    \trad{\Nat(\bu)}_\rho^{\sigma[\bx\leftarrow \sigma(\bt)]} &= \{t \in \Lambda\mid t \rhd^* \overline{\sigma[\bx\leftarrow\sigma(\bt)](\bu)}\}\\
    &= \{ t \in \Lambda\mid t \rhd^* \overline{\sigma(\bu[\bt/\bx])}\}\\
    &= \trad{\Nat(\bu[\bt/\bx])}_\rho^\sigma
\end{align*}

Dans le cas du lemme \ref{lem.subst.2}, le résultat est direct étant donné que l'interprétation de $\Nat(\bt)$ ne dépend pas de l'interprétation du second ordre.

On souhaite en déduire une nouvelle théorie, qui considère la version dite relativisée des quantificateurs~: plutôt que de considérer $\forall \bx^S, \varphi$, on considère $\forall \bx^S, S(\bx) \to \varphi$, ce qu'on notera $\forall \bx^{\{S\}}, \varphi$ pour alléger la lecture. L'adéquation fonctionne encore correctement, mais à condition de considérer seulement les objets qui possèdent des codes~: on force donc notre modèle à se restreindre à une forme d'éléments standards. Pour les entiers, cela ne change pas notre interprétation, qui est $\bN$, mais lorsque nous introduirons des fonctions, une quantification relativisée force la fonction introduite à être calculable.

On possède donc une nouvelle théorie. Pour une proposition du second ordre $\varphi$, on notera $\{\varphi\}$ sa version relativisée. On peut donc définir la nouvelle théorie comme~:
\[\{\mathcal T_\reali\} \defeq \{\varphi \in \Propo\mid \exists t \in \Lambda, t \reali\{\varphi\}\}\]

Cette théorie a un sens calculatoire bien plus profond~: par exemple, on peut vérifier que dans notre modèle actuel, l'axiome du choix est vérifié (dans un sens relativisé)~:
\[\reali \forall R^{\Nat,\Nat},(\forall \bx^{\{\Nat\}}, \exists \by^{\{\Nat\}}, R(\bx,\by))\to (\exists \bbf^{\{\Nat\to\Nat\}}, \forall \bx^{\{\Nat\}}, R(\bx,\bbf(\bx)))\]
Nous n'avons pas défini la sorte $\Nat\to\Nat$~: on verra plus tard comment parler des fonctions au premier ordre, mais on donne ici une idée informelle de la véracité de l'axiome du choix (dénombrable).

On fixe $R^{\Nat,\Nat}$ et on suppose qu'il existe $r\reali \forall \bx^{\{Nat\}}, \exists \by^{\{\Nat\}}, R(\bx,\by)$~: cela signifie que pour tout code $n$ de $\bx$, on peut trouver un code $m$ d'un certain $\by$ tel que $R(\bx,\by)$~: $r$ définit une fonction calculable qui aux codes de $\bx$ associe les codes de $\by$ correspondant. Remarquons que, pour l'encodage que nous avons choisi pour $\exists$, on peut définir deux fonctions $\pi_1,\pi_2$ telles que si $t \reali \exists \bx^{\{S\}}, \varphi$, alors on a $\pi_1\;t\reali S(\bx)$ et $\pi_2\;t\reali \varphi$. Avec ces fonctions, $\lambda x.\pi_1\;(r\;x)$ code une fonction $\Nat\to\Nat$ et $\lambda x.\pi_2\;(r\;x)$ montre que cette fonction réalise bien le choix qu'on attend~: pour tout $\bx^{\{\Nat\}}$, elle trouve un $\by^{\{\Nat\}}$ tel que $R(\bx,\by)$.

En réalité, le système de type permet plus que de montrer que $\{\mathcal T_\reali\}$ est close par déduction~: elle permet de construire des réaliseurs de façon simple et efficace. En effet, l'adéquation nous dit qu'un terme typé est un réaliseur, mais là où la réalisabilité est sémantique (la proposition $t\reali \varphi$ est liée à la sémantique de $t$ et non à sa forme), le typage est syntaxique. En particulier, il est facile de vérifier qu'un terme $t$ donné peut bien se typer avec un type $\varphi$ donné~: il nous suffit d'exhiber une dérivation de typage. On souhaite donc rajouter des règles de typage, pas seulement pour leur aspect logique, mais aussi pour simplifier les constructions de réaliseurs. Nous introduisons donc $3$ règles à propos de $\Nat$, qui s'apparentent largement aux règles de typage des constructeurs et du destructeur de $\Nat$ en système T~:
\begin{center}
    \begin{prooftree}
        \infer0[$\bN_\mathrm i^0$]{\Gamma\mid\Delta\mid\Xi\vdash \overline 0 : \Nat(0)}
    \end{prooftree}
    \qquad
    \begin{prooftree}
        \hypo{\Gamma\mid\Delta\mid\Xi\vdash t : \Nat(\bt)}
        \infer1[$\bN_\mathrm i^S$]{\Gamma\mid\Delta\mid\Xi\vdash S\;t : \Nat(S\;\bt)}
    \end{prooftree}

    \vspace{0.5cm}

    \begin{prooftree}
        \infer0[$\bN_\mathrm e$]{\Gamma\mid\Delta\mid\Xi\vdash \rec_{\Nat} : \forall X^{\Nat}, X(0)\to (\forall \bx^{\{\Nat\}}, X(\bx) \to X(S\;\bx))\to \forall \bx^{\{\Nat\}}, X(\bx)}
    \end{prooftree}
\end{center}

L'adéquation vis à vis de ces règles se prouve~:
\begin{itemize}
    \item on sait que $\overline 0 = \overline 0$ donc le terme se réduit bien en $\overline 0$
    \item supposons que $t\rhd^* \overline{\sigma(\bt)}$, alors $S\;t \rhd^* S\;\overline{\sigma(\bt)}$, ce qui est le résultat attendu.
    \item on suppose que $x_0 \reali X(0)$ et que $x_s \reali \forall \bx^{\{\Nat\}}, X(\bx) \to X(S\;\bx)$, montrons par récursion sur $n$ que pour tout $n$, $\rec_{\Nat}\;x_0\;x_s\;\overline n \reali X(\bx)$ (par saturation, si on sait seulement que $t \rhd^* \overline n$ cela suffit à dire que $\rec_{\Nat}\;x_0\;x_s\;t \reali X(\bx)$)~:
    \begin{itemize}
        \item si $n = 0$, alors $\rec_{\Nat}\;x_0\;x_s\;\overline 0 \rhd^* x_0 \reali X(0)$, d'où le résultat par saturation.
        \item supposons que $\rec_{\Nat}\;x_0\;x_s\;\overline n \reali X(n)$, alors
            \[\rec_{\Nat}\;x_0\;x_s\;\overline{n+1} \rhd^* x_s\;\overline n\;(\rec_{\Nat}\;x_0\;x_s\;\overline n)\]
        or $x_s \reali \Nat(n) \to X(n) \to X(n+1)$, $\overline n \reali \Nat(n)$ de façon évidente, et l'hypothèse d'induction nous dit que $\rec_{\Nat}\;x_0\;x_s\;\overline n \reali X(n)$, donc le terme total réalise $X(n+1)$.
    \end{itemize}
    D'où le résultat par récurrence.
\end{itemize}

Nous n'avons pas introduit de valuations $\sigma, \rho$ ni de substitution $\nu$ pour garder la preuve la plus lisible possible, mais celle-ci fonctionne évidemment tout aussi bien en ajoutant ces éléments.

De plus, cette preuve s'adapte tout à fait à n'importe quel codage des entiers avec un codage de la fonction successeur et un récurseur vérifiant les relations décrites précédemment. Dans la suite, on décidera de travailler directement avec un terme $0$, un terme $S$ et un terme $\rec_{\Nat}$ vérifiant ce qu'on souhaite~: la preuve précédente fonctionnera encore pour montrer l'adéquation de ces règles. Un point important, cependant, est que si nous avions décidé de créer une fonction $\rec_{\Nat}$ qui, pour des paramètres $x$ et $y$, réduit à partir de $\overline n$ vers $y^n\;x$ (en ignorant le paramètre dans $\bN$ de $y$), il ne serait pas possible d'appliquer seulement la saturation.

On voit donc que la théorie $\{\mathcal T_\reali\}$ est plus intéressante~: elle vérifie le principe de récurrence, au prix de ne parler que des entiers standards.

\subsection{Encoder les fonctions et le reste}

Pour finir, afin de pouvoir interpréter le fan theorem et le lemme de König, nous aurons besoin d’étendre notre modèle à d’autres types de données que les entiers. Nous allons donc introduire d'autres sortes dans notre modèle~: les booléens, les listes, les fonctions et les paires. Notre $\lambda$-calcul sera, lui aussi, enrichi de constructeurs pour permettre de parler plus facilement des listes, des booléens et des paires (ainsi que des entiers).

Ainsi, notre ensemble $\mathcal S$ des sortes est engendré par la grammaire suivante~:
\[S,S' ::= \Nat\mid\Bool\mid\List(S)\mid S\times S'\mid S\to S'\]

Plutôt que de donner explicitement la signature, on donne les règles de typages des termes du premier ordre~:
\begin{center}
    \begin{prooftree}
      \infer0{\Gamma, \bx : S \vdash \bx : S}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \infer0{\Gamma\vdash \bZ : \Nat}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash \bt : \Nat}
      \infer1{\Gamma\vdash \bfS\;\bt : \Nat}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash \bt : S}
      \hypo{\Gamma\vdash \bu : \Nat \to S \to S}
      \hypo{\Gamma\vdash \bv : \Nat}
      \infer3{\Gamma\vdash \rec_{\Nat}\;\bt\;\bu\;\bv : S}
    \end{prooftree}

    \vspace{0.5cm}

    \begin{prooftree}
      \infer0{\Gamma\vdash \bbtt : \Bool}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \infer0{\Gamma\vdash \bbff : \Bool}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash \bt : \Bool}
      \hypo{\Gamma\vdash \bu : S}
      \hypo{\Gamma\vdash \bv : S}
      \infer3{\Gamma\vdash \rec_{\Bool}\;\bt\;\bu\;\bv : S}
    \end{prooftree}

    \vspace{0.5cm}
    
    \begin{prooftree}
      \infer0{\Gamma\vdash \bnil : \List(S)}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash \bt : S}
      \hypo{\Gamma\vdash \bu : \List(S)}
      \infer2{\Gamma\vdash \bt \bcons \bu : \List(S)}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash \bt : S}
      \hypo{\Gamma\vdash \bt : S' \to \List(S') \to S \to S}
      \hypo{\Gamma\vdash \bv : \List(S')}
      \infer3{\Gamma\vdash \rec_{\List}\;\bt\;\bu\;\bv : S}
    \end{prooftree}

    \vspace{0.5cm}

    \begin{prooftree}
      \hypo{\Gamma, \bx : S \vdash \bt : S'}
      \infer1{\Gamma\vdash \blam \bx.\bt : S \to S'}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash \bt : S \to S'}
      \hypo{\Gamma\vdash \bu : S}
      \infer2{\Gamma\vdash \bt\;\bu : S'}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash \bt : S}
      \hypo{\Gamma\vdash \bu : S'}
      \infer2{\Gamma\vdash \langle \bt,\bu \rangle : S \times S'}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash \bt : S_1 \times S_2}
      \infer1{\Gamma\vdash \bpi_i\;\bt : S_i}
    \end{prooftree}
\end{center}

On définit l'interprétation de chaque sorte $S$ par un ensemble qu'on notera $\trad S$ ou $\bS$, ainsi que l'interprétation d'un terme $\bt$ tel que $\Gamma\vdash \bt : S$ pour une valuation $\sigma$ du premier ordre adéquate pour $\Gamma$~:
\begin{itemize}
    \item $\trad \Nat = \bN$
    \item $\trad \Bool = \{0,1\}$
    \item $\trad{\List(S)} = \bS^*$, l'ensemble des suites finies d'éléments de $\bS$
    \item $\trad{S\times S'} = \bS\times \bS'$, le produit cartésien
    \item $\trad{S\to S'} = \bS'^{\bS}$, l'espace fonctionnel
    \item l'interprétation de $\bx$ est $\sigma(\bx)$
    \item l'interprétation de $\bZ$ est $0\in \bN$
    \item l'interprétation de $\bS$ est $n\mapsto n + 1$
    \item l'interprétation de $\rec_{\Nat}\;\bt\;\bu$ est l'unique fonction $f : \bN \to \bS$ valant $\sigma(\bt)$ en $0$ et vérifiant la relation $f(n+1) = \sigma(\bu)(n)(f(n))$, et $\rec_{\Nat}\;\bt\;\bu\;\bv$ est donc sa valeur en $\sigma(\bv)$
    \item l'interprétation de $\bbtt$ est $1\in \{0,1\}$
    \item l'interprétation de $\bbff$ est $0\in\{0,1\}$
    \item l'interprétation de $\rec_{\Bool}\;\bt\;\bu\;\bv$ est celle d'un branchement conditionnel~: si $\sigma(\bt) = 1$ alors l'expression vaut $\sigma(\bu)$ et sinon l'expression vaut $\sigma(\bv)$
    \item l'interprétation de $\bnil$ est $\varepsilon \in \bS^*$
    \item l'interprétation de $\bt\bcons\bu$ est $\star : \bS \times \bS^* \to \bS^*$, la concaténation d'une lettre à un mot
    \item l'interprétation de $\rec_\List$ est définie de façon analogue aux autres interprétations de $\rec$
    \item l'interprétation de $\blam \bx.\bt$ est la fonction qui à $x \in \bX$ associe $\sigma[\bx\leftarrow x](\bt)$
    \item l'interprétation de $\bt\;\bu$ est l'application fonctionnelle
    \item l'interprétation des paires et des projections est évidente.
\end{itemize}

On souhaite aussi définir un ensemble de codes pour nos termes, mais il nous faut d'abord, pour cela, introduire le langage de programmation augmenté avec des outils pour les différentes sortes.

\begin{defi}[$\lambda$-calcul augmenté]
    Notre nouveau $\lambda$-calcul est engendré, à partir de la grammaire de la définition \ref{def.lam}, par~:
    \[t,u ::= [\cdots] \mid\langle t,u\rangle\mid \pi_1\;t\mid\pi_2\;t\mid 0\mid S\;t \mid\rec_\bN\mid \btt\mid\bff\mid\rec_\bB\mid[\,]\mid t::u\mid\rec_\bL\]
\end{defi}

On étend la substitution de façon évidente, et on ajoute dans $\mapsto$ les règles suivantes~:
\begin{center}
    \begin{prooftree}
        \infer0[$i\in\{1,2\}$]{\pi_i\;\langle t_1,t_2\rangle \mapsto t_i}
    \end{prooftree}
    \quad
    \begin{prooftree}
        \infer0{\rec_\bN\;t\;u\;0 \mapsto t}
    \end{prooftree}
    \quad
    \begin{prooftree}
        \infer0{\rec_\bN\;t\;u\;(S\;v)\mapsto u\;v\;(\rec_\bN\;t\;u\;v)}
    \end{prooftree}

    \vspace{0.5cm}

    \begin{prooftree}
        \infer0{\rec_\bB\;\btt\;t\;u\mapsto t}
    \end{prooftree}
    \quad
    \begin{prooftree}
        \infer0{\rec_\bB\;\bff\;t\;u\mapsto u}
    \end{prooftree}
    \quad
    \begin{prooftree}
        \infer0{\rec_\bL\;t\;u\;[\,]\mapsto t}
    \end{prooftree}
    \quad
    \begin{prooftree}
        \infer0{\rec_\bL\;t\;u\;(v::w)\mapsto u\;v\;w\;(\rec_\bL\;t\;u\;w)}
    \end{prooftree}
\end{center}

$\rhd$ est définie comme la plus petite relation contenant $\mapsto$ et étant compatible (c'est-à-dire qu'on s'autorise à effectuer une réduction à l'intétieur d'un terme).

On peut désormais définir les codes des objets du premier ordre~: caque objet du premier ordre $x$ sera associé à un ensemble de codes $\ceil x$ (possiblement vide~: c'est ce qu'il se passe pour une fonction non calculable). On raisonne par induction sur la structure des sortes~:
\begin{itemize}
    \item pour $n\in\bN$, $\ceil n \defeq \{t\in\Lambda \mid t \rhd^* S^n\,0\}$
    \item pour $b \in \bB$, $\ceil 0 \defeq \{t\in\Lambda\mid t \rhd^* \bff\}$ et $\ceil 1 \defeq \{t \in \Lambda\mid t \rhd^* \btt\}$
    \item pour $l \in \bS^*$, $\ceil{\varepsilon} \defeq \{t\in\Lambda \mid t\rhd^* [\,]\}$ et $\ceil{s\star l} \defeq \{t \in \Lambda\mid\exists u \in \ceil s,\exists v \in \ceil l,t \rhd^* u::v\}$
    \item pour $(s,s') \in \bS\times \bS'$, $\ceil{(s,s')} \defeq \{t\in \Lambda\mid \exists u \in \ceil s, \exists v \in \ceil{s'}, t \rhd^* \langle u,v\rangle\}$
    \item pour $f \in \bS\to \bS'$, $\ceil f \defeq \{t \in \Lambda\mid \forall s \in \bS, \forall u \in \ceil s, t\;u \in \ceil{f(s)}\}$
\end{itemize}
Par construction, pour $s\in \bS$, $\ceil s$ est un ensemble saturé.

On définit alors simplement, 
\[\trad{S(\bt)}_\rho^\sigma\defeq \ceil{\sigma(\bt)}\]

La preuve du lemme \ref{lem.subst.1} est la même que pour simplement $\Nat(\bt)$, et celle du lemme \ref{lem.subst.2} est toujours le fait que ces formules sont indépendantes du contexte du second ordre.

On récupère ainsi, très facilement, une théorie où les fonctions, listes et booléens sont plus faciles à exprimer. On a vu les règles de typage liées à $\Nat$~: on peut faire les mêmes pour les autres sortes.

\begin{center}
    \begin{prooftree}
      \infer0[$\bB_\mathrm i ^{\btt}$]{\Gamma\mid\Delta\mid\Xi\vdash \btt : \Bool(\bbtt)}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \infer0[$\bB_\mathrm i^{\bff}$]{\Gamma\mid\Delta\mid\Xi\vdash \bff : \Bool(\bbff)}
    \end{prooftree}

    \vspace{0.5cm}
    
    \begin{prooftree}
      \infer0[$\bB_\mathrm e$]{\Gamma\mid\Delta, X : \Bool\mid\Xi\vdash \rec_\bB : X(\bbtt) \to X(\bbff) \to \forall \bx^{\{\Bool\}}, X(\bx)}
    \end{prooftree}

    \vspace{0.5cm}
    
    \begin{prooftree}
      \infer0[$\bL_\mathrm i^{[\;]}$]{\Gamma\mid\Delta\mid\Xi\vdash [\:] : \List(S)(\bnil)}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\mid\Delta\mid\Xi\vdash t : S(\bt)}
      \hypo{\Gamma\mid\Delta\mid\Xi\vdash u : \List(S)(\bu)}
      \infer2[$\bL_\mathrm i^{::}$]{\Gamma\mid\Delta\mid\Xi\vdash t :: u : \List(S)(\bt \bcons \bu)}
    \end{prooftree}

    \vspace{0.5cm}
    
    \begin{prooftree}
      \infer0[$\bL_\mathrm e$]{\Gamma\mid\Delta, X : \List\mid\Xi\vdash \rec_\bL : X(\bnil) \to (\forall \bs^{\{S\}}\;\bx^{\{\List(S)\}}, X(\bx) \to X(\bs \bcons \bx)) \to \forall \bx^{\{\List(S)\}},X(\bx)}
    \end{prooftree}

    \vspace{0.5cm}

    \begin{prooftree}
      \hypo{\Gamma, \bx : S\mid\Delta\mid\Xi, x : S(\bx)\vdash t : S'(\bt)}
      \infer1[$\blam_\mathrm i$]{\Gamma\mid\Delta\mid\Xi\vdash \lambda x.t : (S\to S')(\blam \bx. \bt)}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\mid\Delta\mid\Xi\vdash t : (S\to S')(\bt)}
      \hypo{\Gamma\mid\Delta\mid\Xi\vdash u : S(\bu)}
      \infer2[$\blam_\mathrm e$]{\Gamma\mid\Delta\mid\Xi\vdash t\;u : S'(\bt(\bu))}
    \end{prooftree}

    \vspace{0.5cm}

    \begin{prooftree}
        \hypo{\Gamma\mid\Delta\mid\Xi\vdash t : S(\bt)}
        \hypo{\Gamma\mid\Delta\mid\Xi\vdash u : S'(\bu)}
        \infer2[$\times_\mathrm i$]{\Gamma\mid\Delta\mid\Xi\vdash \langle t,u\rangle : (S\times S')(\langle \bt,\bu\rangle)}
    \end{prooftree}
    \quad
    \begin{prooftree}
        \hypo{\Gamma\mid\Delta\mid\Xi\vdash t : (S_1\times S_2)(\bt)}
        \infer1[$\times_\mathrm e^1$]{\Gamma\mid\Delta\mid\Xi\vdash \pi_i\;t : S_1(\bpi_i\;\bt)}
    \end{prooftree}
\end{center}

La preuve de l'adéquation pour ces termes est parfaitement similaire au cas des entiers.

Nous avons donc construit un premier modèle de réalisabilité permettant de manipuler de façon efficace les objets dont nous aurons besoin plus tard~: entiers, listes, booléens. Nous mettons en annexe différentes fonctions utiles mais dont la définition est inintéressante. Notre modèle de réalisabilité étant construit, intéressons-nous maintenant aux énoncés d'intérêts étudiés en mathématiques à rebours.

\section{Mathématiques à rebours}

\subsection{Pourquoi l'arithmétique ?}

Nous avons déjà mentionné l'idée de base des mathématiques à rebours dans l'introduction~: c'est un programme mathématique cherchant à étudier le contenu et la force logique de théorèmes que l'on sait déjà vrais. La question typique à laquelle on souhaite répondre est ``quels sont les axiomes les plus faibles suffiants à entraîner un théorème $T$ ?'' Pour y répondre, on peut essayer de s'intéresser à une notion classique de théorie de la démonstration, celle de l'algèbre de Lindenbaum-Tarski (\cite{Tarski1983-TARLSM-5})~: étant donnée une théorie $\mathcal T$, on peut définir une relation de pré-ordre $\varphi \vdash_{\mathcal T} \psi$ par $\mathcal T\vdash \varphi \to \psi$. L'ensemble des propositions est alors, quotienté par l'équivalence $\dashv\vdash_{\mathcal T}$, une algèbre de Boole. Les classes d'équivalence, qui sont constituées de formules équivalentes modulo $\mathcal T$, sont alors l'objet d'étude des mathématiques à rebours~: on s'intéresse, étant donné un théorème fixé, à la classe d'équivalence (appelons cette classe degré logique, par analogie aux degrés Turing) de ce théorème.

La question vient alors du choix de $\mathcal T$~: il nous faut avant tout que $\mathcal T$ soit faible, car plus la théorie est forte et plus les degrés sont gros. Le cas extrême, de $\ZFC$, met tous les théorèmes intéressants dans le degré de $\top$. On pourrait donc souhaiter que $\mathcal T$ soit vide~: le problème, alors, est que les symboles n'ont plus de sens. Dans le cas de l'arithmétique, par exemple, ne pas considérer d'axiomes entraîne que $0,S,+,\times$ sont des objets quelconques~: ça n'est pas ce que nous voulons. Il nous faut donc un entre-deux~: une théorie suffisamment faible pour être incapable de prouver la plupart des théorèmes mathématiques usuels, mais assez forte pour contenir une partie substantielle des fondements mathématiques (les entiers, le fait qu'ils se comportent correctement, etc). Une solution à cette question est donnée en partie par les travaux de Hilbert et Bernays dans \cite{Hilbert1935-HILGDM-8} et \cite{Hilbert1974-HILGDM-5}~: la puissance de la théorie des ensembles est superflue pour la plupart des résultats mathématiques, et l'arithmétique du second ordre suffit à les prouver. Le cadre de l'arithmétique du second ordre, utilisant des entiers et des ensembles d'entiers, semble donc un bon cadre. On considère donc un système plus faible d'axiomes (puisqu'on veut avoir un maximum d'énoncés indécidables dans notre théorie) mais contenant suffisamment d'informations pour donner un sens à nos objets.

En particulier, puisqu'on souhaite coder les nombres réels et les fonctions continues (choses souhaitables pour formaliser les mathématiques usuelles), il nous faut une théorie permettant de définir ces objets. Le choix historique, qu'on peut voir dans \cite{Simpson_2009} par exemple, est de considérer le sous-système $\RCAO$~: ce sous-système est celui des mathématiques calculables. Cette théorie contient, en plus de l'arithmétique de Robinson, le schéma d'axiomes de compréhension sur les formules $\Delta_1^0$ et le schéma d'induction sur les formules $\Sigma_1^0$. Le schéma de compréhension est l'ensemble des formules de la forme $\exists X^{\Nat}, \forall \bx^{\Nat}, X(\bx) \leftrightarrow \psi(\bx)$ où $\psi$ est $\Delta_1^0$, tandis que le schéma d'induction est l'ensemble des formules de la forme $\psi(0)\land (\forall \bx^{\Nat}, \psi(\bx) \to \psi(S\;\bx)) \to \forall \bx^{\Nat}, \psi(\bx)$ où $\psi$ est $\Sigma_1^0$. $\RCAO$ est donc une restriction de ces schémas pour certaines familles de formules $\psi$. Cette théorie contient l'expressivité des fonctions calculables~: les nombres réels peuvent se représenter comme certaines fonctions calculables, et on peut représenter jusqu'aux fonctions mesurables.

Une conclusion importante de \cite{Simpson_2009} est le phénomène du \textit{Big Five}~: la plupart des théorèmes mathématiques usuels sont équivalents à l'un des cinq sous-systèmes de l'arithmétique du second ordre (modulo $\RCAO$). Parmi ces sous-systèmes se trouve l'un des principes de choix mentionnés dans l'introduction, le lemme de König (en fait, sa version faible).

\subsection{Les principes de choix}

Nous en venons maintenant à l'élément essentiel de notre étude~: les principes de choix. Un principe de choix est un axiome permettant, en partant d'une forme locale de choix, de dériver une forme globale de choix (le tout de façon interne). Reprenons notre formulation précédente~:
\[(\forall x^A, \exists y^B, R(x,y)) \to \exists f^{A\to B}, \forall x^A, R(x,f(x))\]
La prémisse indique que si $x$ est présenté, alors il est possible de choisir un $y$ en relation avec ce $x$, la conclusion indique qu'il devient possible de généraliser ce choix local en une fonction, qui applique le choix uniformément à tout $x$. Ce principe, intuitif, a de nombreuses ramifications. Il est déjà connu que l'axiome du choix dans toute sa généralité mène à des résultats défiant fortement l'intuition (\cite{Banach1924-BANSLD}), mais il semble tout aussi peu intuitif de ne pas considérer l'axiome du choix, encore plus sous une forme faible~: si l'on est capable d'associer, pour chaque élément $x$, un élément $y$, comment ne pas en déduire une fonction faisant cette association ? Dans le modèle de réalisabilité que nous avons vu, l'axiome du choix est vérifié justement pour cette raison~: comme les fonctions considérées sont les fonctions calculables, qui est aussi le cadre de notre logique (être vrai signifie qu'on peut donner une trace calculatoire de cette véracité, par notre interprétation de réalisabilité), chercher le $y$ correspondant au $x$, de façon uniforme en $x$, signifie exactement construire une fonction calculable. Cependant, les méthodes de réalisabilité nous indiquent aussi comment contredire ce principe~: en ajoutant un principe non-déterministe, comme un lancer de pièce, on peut montrer (comme dans \cite{COHEN201987}) que l'axiome du choix n'est plus vérifié, car si le choix local dépend du résultat d'un lancer, il n'est pas vrai que la fonction globale donnera des résultats cohérents.

Donnons une liste des principes de choix importants pour notre étude, qui seront organisés par force logique croissante en logique classique, strictement pour le symbole $>$ et de même force pour le symbole $=$~:
\begin{itemize}
    \item \textit{Weak Fan Theorem} ($\WFT$) qui est comme $\FT$ mais où $C$ ne contient que des éléments dans $\{0,1\}^*$.
    \item $=$~: \textit{Weak König's Lemma} ($\WKL$), qui est comme $\KL$ exprimé plus tôt mais où $T$ ne contient que des éléments dans $\{0,1\}^*$
    \item $>$ : \textit{Fan Theorem} ($\FT$)
    \item $=$~: \textit{König's Lemma} ($\KL$)
    \item $>$~: Axiome du choix dénombrable ($\ACN$) dans le cas où $A = \bN$ et $B$ est quelconque
    \item $>$~: Axiome du choix dépendant ($DC$), $\DC \defeq \forall x^A, \exists y^A, R(x,y) \implies \exists u^{\bN\to A}, \forall n^{\bN}, R(u_n,u_{n+1})$
    \item $>$ : Axiome du choix ($\AC$) en toute généralité.
\end{itemize}

Dans l'article \cite{DBLP:journals/corr/abs-2105-08951}, une façon plus systématique est détaillée pour traiter les principes de choix. Cette présentation se base en partie sur les notions d'arbre, que nous allons maintenant détailler.

\subsection{Arbres et prédicats sur les listes}

Nous avons jusque là évité de rentrer dans les détails du sens de $\KL$ et $\FT$, car leur version formelle nécessite des explications. Nous allons donc détailler le fonctionnement de nos prédicats sur les arbres, en considérant à chaque fois des arbres branchant sur les entiers, de profondeur potentiellement infinie.

Deux présentations classiques existent pour parler d'arbres (au moins dans leur version finie, nous ne nous intéressons pas à leurs versions indicées par des ordinaux)~: \cite{DBLP:journals/corr/abs-2105-08951} parle de présentation intentionnelle et extensionnelle. La présentation intentionnelle, bien plus utilisée en théorie des types et en programmation, considère l'ensemble $\mathbb T$ construit par induction par
\[\mathbb T ::= \langle\rangle \mid \mathrm{Node}\;(\bN \to \mathbb T)\]
Ainsi, on peut par exemple construire un arbre par $\mathrm{Node}(0 \mapsto \mathrm{Node}(\_\mapsto \langle\rangle), n \mapsto \langle\rangle)$, qui représente alors l'arbre
\begin{center}
    \begin{tikzcd}
        & & & & & \mathrm{Node}\ar[dlll,"0"]\ar[dl,"1"]\ar[dr,"n"]\\
        & &\mathrm{Node}\ar[dll,"0"]\ar[dl,"1"]\ar[dr,"n"] & & \langle\rangle & \cdots & \langle\rangle\\
        \langle\rangle & \langle\rangle & \cdots & \langle\rangle
    \end{tikzcd}
\end{center}

En supprimant les n\oe uds $\langle\rangle$, on a donc un arbre contenant le chemin $0$ et aucun autre chemin.

Au contraire, l'approche extensionnelle va considérer les chemins (finis) appartenant à l'arbre. Dans le cas de notre exemple précédent, l'arbre associé est donc $\{(0)\}$. A chaque arbre défini de façon intentionnelle, on peut naturellement associer l'ensemble de ses chemins finis. On remarque que la propriété essentielle de ces ensembles, qui sont donc des parties de $\bN^*$ (puisque des listes finies d'entiers) est d'être stable par préfixe~: étant donné un arbre $T$, un mot $u \in T$ et un mot $v\preceq u$, $v\in T$ puisque le chemin parcouru par $u$ dans l'arbre a aussi parcouru $v$.

On définit donc un arbre extensionnel comme une partie de $\bN^*$ stable par préfixe. La question se pose alors de si nos deux présentations coïncident. Dans un sens, on peut associer à un arbre l'ensemble de ses chemins. Dans l'autre sens, cependant, construire un arbre intentionnel à partir d'un arbre extensionnel n'est pas évident~: il nécessite ce qu'on appelle la \textit{bar induction}\footnote{Pour construire l'arbre intentionnel, il est nécessaire, \textit{a priori}, de parcourir tous les chemins possibles. C'est donc une forme de principe d'omniscience, permettant d'accéder à une information infinie directement. La \textit{bar induction} permet, si pour tout chemin infini il est possible de déterminer la taille maximale à partie de laquelle celui-ci n'est plus dans l'arbre, de déduire une hauteur finie à l'arbre. On n'a alors plus qu'une quantité finie d'information à traiter.}, dont le \textit{fan theorem} est un cas particulier, où l'arbre est finiment branchant. Le cas d'un arbre finiment branchant peut se traiter de deux façons~: on peut au choix considérer des arbres comme des parties de $B^*$ stables par préfixe, où $B$ est un ensemble fini, ou garder la notion d'arbres sur $\bN^*$ et exprimer qu'à un $u \in T$ donné, il existe un majorant des $n$ tels que $u \star n \in T$. La seconde option est la plus générale, mais elle est calculatoirement plus complexe puisqu'il faut être capable de calculer ce majorant (et ça n'est pas toujours possible) pour construire un nouvel arbre finiment branchant. Au total, on décide donc de s'intéresser à la définition extensionnelle des arbres et aux arbres finiment branchants dans le premier sens évoqué, de parties de $B^*$.

Un chemin infini, que nous appellerons désormais simplement chemin (nous préciserons à chaque fois "chemin fini'' quand c'est le cas) dans un arbre $T \subseteq A^*$ est une fonction $\alpha : \bN \to A$ telle que pour tout $n\in \bN$, le mot fini $\alpha_0\ldots\alpha_{n-1}$ est un élément de $T$. On notera $\alpha \in_\infty T$ pour dire que $\alpha$ est un chemin de $T$. Nous définissons aussi la notation $\in_n$, pour $n \in \bN$~:
\[\alpha \in_n T \defeq \alpha_0\ldots\alpha_{n-1} \in T\]
Ainsi $\alpha\in_\infty T$ peut se voir comme $\forall n\in \bN, \alpha\in_n T$.

Les théorèmes nous intéressant, $\WKL$, $\KL$ et $\FT$, s'énoncent alors ainsi en donnant toutes les conditions (on considère que $A$ est un ensemble fini)~:
\begin{multline*}
    \WKL \defeq \forall T \subseteq\{0,1\}^*, (\forall u,v\in \bN^*, u\preceq v \implies v \in T \implies u \in T)\land (\forall n \in \bN, \exists u \in T, |u| = n)\\ \implies \exists \alpha : \bN\to\bN, \alpha \in_\infty T
\end{multline*}
\begin{multline*}
    \KL \defeq \forall T \subseteq A^*, (\forall u,v\in \bN^*, u\preceq v \implies v \in T \implies u \in T)\land (\forall n \in \bN, \exists u \in T, |u| = n) \\ \implies \exists \alpha : \bN\to\bN, \alpha \in_\infty T
\end{multline*}
\begin{multline*}
    \FT \defeq \forall C \subseteq \bN^*, (\forall u,v\in \bN^*, u\preceq v \implies u \in C \implies v \in C) \land (\forall \alpha : \bN\to\bN, \exists n \in \bN, \alpha \in_n C) \\\implies \exists n \in \bN, \forall \alpha : \bN\to\bN, \alpha \in_n C
\end{multline*}

Les énoncés sont longs et difficilement lisibles, nous allons donc donner des noms aux différentes propriétés essentielles. Tout d'abord, être un arbre~: cela signifie être stable par préfixe (la première condition), ce que l'on résumera par $\isTree(T)$. Dans le cas de $\WKL$, on définit $\isBinTree(T)$, qui et la même condition mais où $T\subseteq\{0,1\}^*$. Ensuite, être le complémentaire d'un arbre~: comme un arbre est clos par préfixe, son complémentaire est clos par extension (on dit alors que cette partie est croissante, ou monotone), ce qu'on notera $\isMono(C)$. La condition commune à $\WKL$ et $\KL$ est d'avoir des chemins finis de longueur arbitrairement grande, ce qui peut se comprendre par ``il y a une infinité de sommets dans l'arbre'', d'où le fait de nommer cette condition $\isInfinite(T)$. Pour la condition sur $\FT$, il nous faut un peu de vocabulaire~: on appelle barre pour $C$ une fonction $b : (\bN \to \bN) \to \bN$ qui vérifie que $\alpha \in_{b(\alpha)} C$. Par analogie, on dit que $C$ est barré si pour tout $\alpha : \bN \to \bN$, on peut trouver $n_\alpha$ tel que $\alpha \in_{n_\alpha} C$. On notera donc $\isBarred(C)$ l'hypothèse.

On définit alors les notations $\forall T^{\Tree}$, $\forall T^{\BinTree}$, et $\forall T^{\Mono}$ pour alléger notre écriture. Les axiomes sont alors~:
\begin{align*}
    \WKL &= \forall T^{\BinTree}, \isInfinite(T) \implies \exists \alpha : \bN \to \bN, \alpha\in_\infty T\\
    \KL &= \forall T^{\Tree},\isInfinite(T) \implies \exists \alpha : \bN\to\bN, \alpha \in_\infty T\\
    \FT &= \forall C^{\Mono},\isBarred(C) \implies \exists n \in \bN, \forall \alpha : \bN \to \bN, \alpha \in_n C
\end{align*}

La conclusion de $\FT$ est ce qu'on appelle une \textit{bar} uniforme~: le même $n$ fonctionne pour tous les chemins. Remarquons que le fait de considérer que $C$ est croissant signifie que l'on peut toujours prendre un $n$ plus grand ensuite pour obtenir une nouvelle \textit{bar} uniforme.

\subsection{Vient l'intuitionnisme}

Notre distinction entre $\FT$ (respectivement $\WFT$) et $\KL$ (respectivement $\WKL$) n'est intéressante que parce que les résultats ne sont plus équivalents en logique intuitionniste, logique dans laquelle nous menons notre étude. Dans ce cadre, les degrés logiques sont encore plus restreints, et plus de résultats peuvent être séparés. Cependant, au contraire des mathématiques à rebours classiques, une volonté de prouver des résultats d'indépendance existe~: en logique intuitionniste, des résultats habituellement faibles mais non constructifs deviennent des résultats bien plus forts (nous le verrons avec $\WKL$). Dans ces cas-là, prendre une théorie plus faible n'est pas forcément le meilleur moyen puisqu'être indépendant d'une théorie plus forte est un meilleur résultat. Dans ce rapport, nous nous contentons d'une sorte d'arithmétique du second ordre, la question se pose donc de si notre résultat d'indépendance principal, que $\FT$ n'implique pas $\WKL$, est encore vrai dans une théorie plus forte. Dans \cite{lubarsky2015realizabilitymodelsseparatingvarious}, une construction montre que cette indépendance est vraie même dans la version intuitionniste de $\ZF$.

Cet article, néanmoins, semble répondre à notre question, puisqu'il construit un modèle de réalisabilité séparant $\FT$ et $\WKL$ dans un cadre intuitionniste. Il est ce qui nous a donné l'idée de considérer la continuité comme le principe calculatoire validant $\FT$. Cependant, la construction considérée dedans est largement plus complexe que ce que nous faisons ici~: on part d'un modèle de $\ZF$ pour considérer une structure de Kripke dans laquelle chaque monde est un modèle de réalisabilité indicée par un oracle, et les oracles sont strictement plus forts (pour la réduction Turing) quand on considère une arrête $v < w$ entre deux mondes. Au contraire, notre modèle est un simple modèle de réalisabilité sur un lambda-calcul, comme on peut en trouver facilement dans la littérature.

Un autre point important, à propos de la construction que nous proposons, est que celle-ci semble très proche de la réalisabilité de Kleene pour l'algèbre $K_1$. Nous avons décrit plus tôt comment la réalisabilité peut s'imaginer à partir d'un ensemble $\bX$ muni de certaines structure. Historiquement, la réalisabilité a été construite depuis un cas particulier de $\bX$~: l'ensemble $\bN$ représentant à la fois les entiers et les codes de machines de Turing. Ainsi, on peut attribuer un sens à $n\reali \varphi \to \psi$ en disant que pour tout $m\reali \varphi$, $n\cdot m \reali\psi$, où $n\cdot m$ représente l'application de la machine de code $n$ à l'argument $m$. Dans ce cas-là, on peut interpréter les quantificateurs de façon fonctionnelle, où $\forall x, \varphi$ se réalise par une fonction qui à $n$ associe un réaliseur de $\varphi[n/x]$. Kleene a ainsi construit un modèle de réalisabilité pour l'arithmétique de Heyting (la version intuitionniste de l'arithmétique de Peano), mais ce modèle ne vérifie pas $\FT$ (\cite{Kleene1965-KLETFO-3}). Un autre modèle de réalisabilité, l'algèbre $K_2$ (ou seconde algèbre de Kleene), vérifie bien $\FT$, mais celle-ci est largement plus complexe. Notre modèle nous permet ainsi d'avoir une présentation relativement simple, plus proche de $K_1$, mais vérifiant quand même $\FT$.

Enfin, notons que, en vue d'un approfondissement du sujet, la logique intuitionniste rend beaucoup plus de résultats indépendants ou non équivalents dans notre hiérarchie plus haut. Par exemple, on a vu que $\DC$ était encore réalisé par notre modèle de réalisabilité, tandis que, nous le verrons, $\WKL$ ne l'est pas. Une conjecture intéressante dans ce sens nous a été partagée par les auteurs de \cite{DBLP:conf/fscd/HerbelinK24} qui, à l'issu de leur travail, ont remarqué que les principes de maximalité (tels que le lemme de Zorn ou celui de Teichmuller-Tukey) ne permettent de dériver une preuve de $\AC$ qu'en utilisant le tiers exclu, pour passer d'une fonction partielle maximale à une fonction totale. Ainsi il est possible que le lemme de Zorn, dans le cadre intuitionniste, donne une force logique très élevée pour construire par exemple une clôture algébrique, mais ne construise pas entièrement la logique classique (en particulier, peut-être que certains principes d'omniscience, c'est-à-dire des constructions nécessitant d'avoir une vision complète sur un objet infini, échouent).

\section{La preuve d'indépendance}

Nous en venons maintenant à notre résultat principal~: $\WKL$ est indépendant de $\FT$ en logique intuitionniste. Le fait que $\WKL\vdash \FT$ est impossible découle directement du fait que $\KL$ est strictement plus fort que $\WKL$ en logique classique, donc dans une logique plus faible $\WKL$ ne peut pas dériver $\FT$. Ce qui importe ici est l'autre sens~: on montre qu'on a un modèle de $\FT$ qui ne satisfait pas $\WKL$.

\subsection{Satisfaire le fan theorem}

Commençons par le point le plus important, montrer qu'on satisfait $\FT$. Pour cela, donnons d'abord l'idée de la preuve.

Celle-ci est inspirée de la preuve trouvée dans \cite{lubarsky2015realizabilitymodelsseparatingvarious}~: étant donné un prédicat $C$ croissant et une barre $b$, on décide de construire un nouveau prédicat $C'$ contenant $C$, mais décidable, qui contient une barre uniforme. Pour cela, on parcourt tous les chemins finis par longueur croissante~: étant donné un chemin fini $p$, on considère $(p 0^\infty)_{|b(p0^\infty)}$, qu'on notera $p'$ pour alléger les notations. Si $p'\preceq p$, alors on ajoute $p$ et toutes ses extensions à $C'$. Si $p\prec p'$, alors on ajoute $p'$ et toutes ses extensions à $C'$ et on exclut $p$ de $C'$. Ainsi le prédicat est décidable puisqu'il suffit pour calculer $p$ de vérifier tous les mots de longueur inférieure à $p$ (pas seulement ses préfixes, puisque si pour vérifier $0010$ on a en fait que $b(0110^\infty) = 0$, notre mot est dans $C'$ sans être détecté par un préfixe). Le fait que $C'\subseteq C$ est une conséquence directe du fait que $b$ est une barre (associée à $C$, donc retournant des éléments de $C$) et que $C$ est croissant.

Il faut encore montrer que $C'$ contient une barre uniforme. Pour cela, on peut construire un algorithme qui vérifie, par $n$ croissant, si $C'$ admet $n$ comme barre uniforme. La véracité de $\FT$ tient donc dans la terminaison de cet algorithme. L'idée développée dans \cite{lubarsky2015realizabilitymodelsseparatingvarious} est de raisonner par l'absurde~: si l'algorithme ne termine pas, alors on a dans $\overline{C'}$, pour chaque $n \in \bN$, un mot $u$ de taille $n$. Avec le lemme de König, on extrait alors un chemin $\alpha \in_\infty \overline{C'}$, qui n'est pas calculable a priori (donc pas dans le modèle de réalisabilité en lui-même, ne vérifiant pas le lemme de König). C'est ici que notre approche diffère de celle de l'article cité ci-dessus~: là où l'article utilise des notions d'oracles de force croissante pour permettre d'internaliser le chemin $\alpha$ dans un futur possible, nous travaillons directement avec notre seul modèle.

Pour montrer que l'existence d'un tel $\alpha$ mène à une contradiction, nous décidons alors de nous plonger dans un nouveau contexte~: si l'existence de $\alpha$ mène à une contradiction dans ce nouveau contexte, et que plonger dans ce nouveau contexte peut toujours se faire, alors nous avons bien là une contradiction. L'objectif est de trouver un contexte dans lequel $b$ peut retourner une valeur sur $\alpha$, ce qui n'est pas possible pour l'instant puisque $b$ est une fonction définie seulement sur les chemins calculables (rien n'est assuré sur les chemins non calculables). Une fois que l'on a ce contexte, on effectue la même manipulation que dans \cite{lubarsky2015realizabilitymodelsseparatingvarious}~: on sait que $b$ est une fonction calculable, donc pour calculer $b(\alpha)$ elle doit utiliser un préfixe fini, mais ce préfixe fini, lui, a déjà été traité dans la définition de $C'$, menant à ce que $\alpha\notin_\infty \overline{C'}$.

Notre nouveau contexte est celui des fonctions continues et des ensembles ordonnés complets, au sens de \cite{Amadio_Curien_1998}. L'avantage de cette interprétation est clair~: on peut interpréter toute fonction calculable dans l'équivalent de $\bN\to\bN$, mais celui-ci contient aussi n'importe quel chemin que nous pourrions obtenir dans la meta-théorie. Nous n'avons à construire que des interprétations des termes du premier ordre pour cela~: l'objectif est alors de construire une interprétation de chaque objet du premier ordre $f \in \bS$ en un objet $\tilde f \in \tilde S$ de telle sorte que si $t\in\ceil f$ alors l'interprétation canonique de $t$ est $\tilde f$.

Nous rappelons les définitions élémentaires et les résultats les plus basiques sur les ensembles ordonnés complets. Pour plus d'informations sur les preuves, voir \cite{Amadio_Curien_1998}.

\begin{defi}[Ensemble ordonné complet]
    Soit $(X,\leq)$ un ensemble ordonné. On dit que $\Delta\subseteq X$ est une partie filtrante, ce que l'on note $\Delta\subdir X$ (pour \textit{directed subset}), si
    \[\forall x,y \in \Delta, \exists z \in \Delta, \left\{\begin{array}{c}
        x \leq z\\
        y \leq z
    \end{array}\right.\]
    On dit que $(X,\leq)$ est un ensemble ordonné complet si tout $\Delta\subdir X$ a une borne supérieure. On dira que $X$ est un cpo pour abréger (\textit{complete partial order}).
\end{defi}

\begin{expl}
    L'exemple le plus basique est le suivant~: si $X$ est un ensemble, alors $X_\bot = X \sqcup \{\bot\}$ muni de l'ordre où tous les éléments de $X$ sont incomparables et $\bot \leq x$ pour tout $x \in X$, est un ensemble ordonné complet~: on l'appelle le domaine plat associé à $X$. Les domaines plats servent à représenter les ensembles de valeurs ayant pour but d'être observées~: la seule information pertinente est la présence ou l'absence d'une valeur, et quelle valeur on a si présence il y a.
\end{expl}

On définit une notion de morphisme adaptée à cette structure~: les fonctions continues (au sens de Scott).

\begin{defi}[Fonction continue]
    Soient $(X,\leq), (Y,\leq)$ deux cpos. Une fonction $f : X \to Y$ est dite continue si elle est croissante et préserve les bornes supérieures de parties filtrantes.
\end{defi}

La collection des cpos et des fonctions continues forme une catégorie cartésienne fermée, ce qui signifie en particulier que l'ensemble des fonctions continues de $X$ dans $Y$ peut être muni d'une structure de cpo, avec l'ordre
\[f\leq g \defeq \forall x \in X, f(x) \leq g(x)\]
On cite aussi le résultat suivant, nous donnant la continuité des fonctions qui nous intéressent~:
\begin{prop}
    Soient $X,Y$ deux fonctions et $f : X \to Y$ une fonction. Alors $f_\bot : X_\bot \to Y_\bot$ est une fonction continue.
\end{prop}

On peut même montrer qu'on a un foncteur entre la catégorie $\mathbf{Set}$ et la catégorie $\mathbf{Cpo}$ décrite plus haut, avec $X \mapsto X_\bot$ et $f \mapsto f_\bot$.

On souhaite donc assigner à chaque sorte $S$ un cpo $\tilde S$, et à chaque terme $t$ tel que $t\reali S(\bt)$ un élément $\tilde t \in \tilde S$. L'existence d'une telle assignation dans le cas du $\lambda$-calcul non typé est encore une question ouverte (du moins nous n'avons pas trouvé de preuves d'une telle chose), et semble difficile\footnote{Dans le cas typé, cette assignation est un résultat classique que l'on peut trouver dans \cite{Amadio_Curien_1998} par exemple. On sait aussi interpréter un terme $t$ dans un domaine qui contient tous les termes. Le problème se pose dans l'entre-deux d'un terme $t$ dont on sait que $t\reali \varphi$, pour en déduire une interprétation dans un domaine lié à $\varphi$.}. Nous n'avons pas réussi à construire une telle assignation, bien que nous pensions fortement que celle-ci existe. Pour pouvoir néanmoins obtenir un résultat positif à propos de $\FT$, nous construisons un nouveau $\lambda$-calcul pour faire un modèle de réalisabilité.

Un résultat standard, que l'on retrouve par exemple dans \cite{Amadio_Curien_1998}, est que l'on peut interpréter le langage PCF dans $\mathbf{Cpo}$, où PCF désigne une variante du $\lambda$-calcul simplement typé avec un opérateur de point fixe $Y : (A \to A) \to A$ tel que $Y(t) \rhd t(Y(t))$. Ce langage est Turing-complet, comme le $\lambda$-calcul précédent, renforçant notre confiance en la possibilité d'une assignation n'utilisant que $t\reali S(\bt)$\footnote{On peut aussi relever que la preuve d'adéquation calculatoire repose sur des arguments semblables à ceux décrits par $t\reali \varphi$.}.

On définit donc $\LamP$, notre variant de PCF utilisée pour notre modèle. Tout d'abord, on ajoute un contructeur $Y$, unaire, à notre langage $\Lambda$. On introduit maintenant $\LamP$ comme un ensemble de termes typés.

\begin{defi}[$\LamP$]
  On définit l'ensemble des types de $\LamP$ par
  \[S,S' ::= \Nat\mid\Bool\mid\List\mid S \to S' \mid S \times S'\]
  Les règles de typage sont les suivantes~:
  \begin{center}
    \begin{prooftree}
      \infer0{\Gamma, x : S \vdash x : S}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma, x : S \vdash t : S'}
      \infer1{\Gamma\vdash \lambda x.t : S \to S'}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash t : S \to S'}
      \hypo{\Gamma\vdash u : S}
      \infer2{\Gamma\vdash t\;u : S'}
    \end{prooftree}

    \vspace{0.5cm}

    \begin{prooftree}
      \hypo{\Gamma\vdash t : S}
      \hypo{\Gamma\vdash u : S'}
      \infer2{\Gamma\vdash \langle t,u\rangle : S \times S'}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash t : S_1\times S_2}
      \infer1{\Gamma\vdash \pi_i\;t : S_i}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash t : S \to S}
      \infer1{\Gamma\vdash Y\;t : S}
    \end{prooftree}

    \vspace{0.5cm}

    \begin{prooftree}
      \infer0{\Gamma\vdash 0 : \Nat}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash t : \Nat}
      \infer1{\Gamma\vdash S\;t : \Nat}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash t : S}
      \hypo{\Gamma\vdash u : \Nat \to S \to S}
      \hypo{\Gamma\vdash v : \Nat}
      \infer3{\Gamma\vdash \rec_\bN\;t\;u\;v}
    \end{prooftree}

    \vspace{0.5cm}

    \begin{prooftree}
      \infer0{\Gamma\vdash \btt : \Bool}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \infer0{\Gamma\vdash \bff : \Bool}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash t : S}
      \hypo{\Gamma\vdash u : S}
      \hypo{\Gamma\vdash v : \Bool}
      \infer3{\Gamma\vdash \rec_\bB\;t\;u\;v : S}
    \end{prooftree}

    \vspace{0.5cm}
    
    \begin{prooftree}
      \infer0{\Gamma\vdash [\:] : \List}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash t : \Nat}
      \hypo{\Gamma\vdash u : \List}
      \infer2{\Gamma\vdash t :: u : \List}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash t : S}
      \hypo{\Gamma\vdash u : \Nat \to \List \to S \to S}
      \hypo{\Gamma\vdash v : \List}
      \infer3{\Gamma\vdash \rec_\bL\;t\;u\;v : S}
    \end{prooftree}
  \end{center}

  Les règles de réduction sont celles induites par $\Lambda$. On définit aussi $\SATP$ comme l'ensemble des parties saturées de $\LamP$.

  Pour chaque type $S$, on définit
  \[\LamS{S}\defeq \{t\in\LamP\mid \vdash t : S\}\]
\end{defi}

On redéfinit aussi l'ensemble de code, de façon simple~: toute la définition reste la même, mais on remplace chaque fois $t\in \Lambda$ par $t\in \LamS{S}$ (où $S$ est la sorte dans laquelle on définit l'ensemble de codes). Ce changement fonctionne bien grâce à notre séparation de chaque couche dans le modèle (termes du premier ordre, proposition, langage de programmation)~: en particulier, en associant à la sorte $S$ le type de PCF $S$, les termes dans un code de $s\in \bS$ sont bien de type $S$.

On modifie maintenant la relation $\reali$ en un seul endroit~:
\[t\reali_\rho^\sigma S(\bt) \defeq \{ t \in \Lambda\mid \exists u \in \ceil{\sigma(\bt)}t\rhd^* u\}\]

On voit sans trop de problèmes que la relation de réalisabilité n'est pas trop fortement changée~: les cas inductifs du lemme d'adéquation sont toujours vérifiés ($\trad{S(\bt)}_\rho^\sigma$ est toujours un ensemble saturé), et les lemmes de substitution fonctionnent toujours par les mêmes arguments. On a donc bien un modèle de réalisabilité comme on le souhaite.

On définit maintenant notre plongement dans $\mathbf{Cpo}$~:
\begin{itemize}
    \item on associe à $\Nat$ le cpo $\bN_\bot$
    \item on associe à $\Bool$ le cpo $\bB_\bot$
    \item si on a associé à $S$ le cpo $\tilde S$, alors on associe à $\List(S)$ le cpo $(\tilde S)^*_\bot$
    \item on associe à $S \to S'$ le cpo $\tilde S \to_{\mathbf{Cpo}} \tilde{S'}$
    \item on associe à $S \times S'$ le cpo $\tilde S \times \tilde{S'}$ muni de l'ordre
    \[(s,s') \leq (t,t') \defeq \left\{\begin{array}{c}
        s \leq t \\
        s' \leq t'
    \end{array}\right.\]
\end{itemize}

On construit maintenant une interprétation des termes de $\LamP$ dans nos cpos. Pour cela, on construit par induction, à partir d'un jugement de typage $\Gamma\vdash t : S$ une fonction $\trad t : \left(\prod_{\gamma \in \Gamma} \tilde\gamma\right) \to \tilde S$~:
\begin{itemize}
    \item si $t$ est la variable $x_i$ alors $\trad t$ est la $i$ème projection depuis $\prod \tilde \gamma$.
    \item si $t$ est $\lambda x.u$ alors $\trad{\Gamma\vdash \lambda x. t : S \to S'} = \Lambda(\trad{\Gamma, x : S\vdash t : S'}$ où $\Lambda$ est l'opération d'abstraction, continue (car la catégorie $\mathbf{Cpo}$ est cartésienne fermée), transformant un terme de type $a\times b \to c$ en un terme $a \to (b \to c)$.
    \item si $t$ est $u\;v$ alors $\trad{\Gamma\vdash u\;v : S} = \mathrm{eval}\circ \langle \trad{\Gamma\vdash u : S' \to S}, \trad{\Gamma\vdash v : S'}\rangle$ où eval est la fonction $((a\to b)\times a) \to b$ et $\langle -,-\rangle$ représente le pairing de fonctions.
    \item si $t$ est $0,\btt,\bff,[\,]$ alors son interprétation est (la fonction renvoyant) directement l'élément correspondant, $0 \in \bN_\bot$, $\btt, \bff \in \bB_\bot$, $[\,] \in \tilde S^*_\bot$ (attention, $\bff \neq \bot$ et $[\,] \neq \bot$).
    \item le pairing $\langle -,-\rangle$ ainsi que les projections sont définis de façon évidente.
    \item $S$ est la fonction associée à la fonction ensembliste $n \mapsto n + 1$, qui est donc continue.
    \item $::$ est aussi la fonction associe à la fonction ensembliste $(a,\ell) \mapsto a :: \ell$.
    \item $Y$ est défini comme l'opération de point fixe de Kleene~: $f \mapsto \bigvee_{n \in \bN} f^n(\bot)$. C'est une fonction continue, cf\cite{Amadio_Curien_1998}.
    \item les récurseurs sont définis comme on l'attend~: on admet que ces fonctions sont continues (la preuve est longue et peu instructive, et le résultat est standard).
\end{itemize}

On peut prouver par induction  sur $S$ que si $s \in \bS$ possède deux codes $t,u \in \ceil s$, alors $\trad t = \trad u$. Cela nous permet de dire que si $s \in \bS$ est un objet possédant un code, alors on peut lui associer un unique objet $\tilde s \in \tilde S$ qui l'objet associé à ses codes.

\begin{rmk}
    Il est essentiel que $s$ possède un code~: on a par exemple des fonctions $(\bN\to\bN) \to \bN$ non continues, mais qui ne sont alors pas calculables.
\end{rmk}

On peut maintenant prouver le résultat de cette sous-section~: notre modèle valide $\FT$.

\begin{them}\label{them.FT}
    Il existe $t \in \Lambda$ tel que
    \[t \reali \FT\]
\end{them}

\begin{proof}
    Soit un prédicat croissant $C$ sur un ensemble fini $\{0,\ldots,N-1\}$. On suppose qu'on a un réaliseur
    \[r \reali \isBarred(C)\]

    Comme on l'a dit plus tôt, on peut construire à partir de $r$ un réaliseur
    \[\langle r',p\rangle \reali \exists f^{\{(\Nat\to\Nat)\to\Nat)}, \forall \alpha^{\{\Nat\to\Nat\}}, \alpha\in_{f(\alpha)} C\]

    Où $r'$ représente la fonction $f$ et $p$ la preuve que $f$ réalise la propriété souhaitée.
    
    On construit le prédicat décidable $C : \List(A) \to \Bool$ dont l'idée est de parcourir tous les chemins les uns à la suite des autres en ajoutant chaque fois pour une liste $\ell$ la liste obtenue en appliquant $r$ à $\ell 0^\infty$. On se réfère à l'appendice \ref{app.A} pour les codes de fonctions que l'on veut utiliser.

    La construction de $C$ est alors la suivante~:
    \begin{multline*}
        C \defeq \lambda \ell. (Y(\lambda\;f\;n.\mathrm{if}\;(|\enumer\;N\;n| =_{\Nat} |\ell|)\;\mathrm{then}\;\bff \;\mathrm{else}\;\\(\mathrm{if}\;(r\;(\toinfty\;(\enumer\;N\;n))\preceq \ell)\;\mathrm{then}\;\btt\;\mathrm{else}f\;(n+1)))\;0)\| (r\;(\toinfty\;\ell)\preceq\ell)
    \end{multline*}

    Pour trouver le $n$ correspondant, il nous suffit alors de regarder pour chaque $n$ si toutes les listes de taille $n$ sont dans $C$~:
    \[\mathrm{fan}\defeq Y(\lambda f\; n. \rec_{\List}\;(\btt)\;(\lambda \ell\;b.C\;\ell\;\&\!\&\;b)\;(\forall\;N\;n))\;0\]

    On souhaite donc montrer que cet algorithme termine. Par l'absurde, si cet algorithme ne termine pas, alors pour tout $n$ il existe au moins un $\ell_n \in \List(A)$ tel que $\ell_n \notin C'$. Notons $T$ le complémentaire de $C'$~: c'est un arbre car $C'$ est croissant. On peut alors appliquer le lemme de König sur cette suite $(\ell_n)$~: on en déduit une fonction $\alpha : \bN \to A$ telle que $\alpha \in_\infty T$. On note alors $\alpha_\bot$ la fonction continue associée, $\alpha_\bot : \bN_\bot \to A_\bot$, valant $\bot$ en $\bot$.

    Comme $f$ est une fonction possédant un code $r'$, celle-ci existe en tant que fonction continue dans $(\bN_\bot\to\bN_\bot)\to\bN_\bot$. On peut donc considérer $f(\alpha_\bot)\in\bN_\bot$. On sait que $\alpha_\bot$ est la borne supérieure de ses approximations finies, dont on notera l'ensemble $\Delta$. Par continuité de $f$ on en déduit que
    \[\bigvee_{\delta \in \Delta} f(\delta) = f\left(\bigvee \Delta\right)\]

    Or, comme $\bN_\bot$ est un domaine plat, seuls deux cas sont possibles~: soit pour une certaine approximation $\alpha_n$ on a $f(\alpha_n) \in \bN$, soit $f(\delta) = \bot$ pour tout $\delta\in\Delta$. Dans les deux cas, cela signifie que la suite des $f(\alpha_0\cdots \alpha_n)$ (en considérant $\alpha_0\cdots\alpha_n$ comme la fonction définie uniquement sur $[0,n]$) est une suite stationnaire. On en déduit qu'il existe $n$ tel que $f(\alpha) = f(\alpha_0\cdots \alpha_n)$, vu que cette suite prend au maximum deux valeurs, on fixe maintenant ce $n$. Comme $f$ est croissante, cela signifie aussi que $f(\alpha_0\ldots\alpha_n 0^\infty) = f(\alpha)$. Or $\alpha_0\cdots\alpha_n 0^\infty$ possède un code, donc cette valeur n'est pas vide (une fonction possédant un code est totale, donc $f$ ne peut pas renvoyer $\bot$ sur une fonction possédant un code).

    De plus, on sait que pour tout chemin $\beta$ possédant un code, $\beta_{f(\beta)} \in C'$, par construction même de $C'$. Soit $m = f(\alpha)$. Si $m \leq n$, alors on sait que $\alpha_0\cdots \alpha_m$ est retourné au moment d'inclure $\alpha_0\cdots\alpha_m 0^\infty$ dans $C'$, donc $\alpha \in_\infty C'$, ce qui est absurde. On en déduit que $n < m$, mais alors en considérant $\alpha_0\cdots \alpha_m 0^\infty$, le résultat est $m$ puisque $\alpha_0\cdots \alpha_n$ est un préfixe de ce mot~: on ajoute donc $\alpha_0\cdots \alpha_m$ à $C'$ à ce moment-là, donc $\alpha \in_\infty C'$. Tous les cas sont donc absurdes.

    On aboutit à une contradiction~: l'algorithme termine donc, et retourne une barre uniforme.
\end{proof}

\subsection[Lemme faible de König]{Ne pas satisfaire le lemme faible de König}

On montre maintenant que notre modèle ne vérifie pas $\WKL$. Pour cela, on utilise une construction classique de calculabilité~: l'arbre de Kleene, qui est un arbre dont tous les chemins finis sont calculables et dont tous les chemins finis sont incalculables. Nous nous basons sur la présentation de \cite{bauer-kleene-tree}, qui contient tous les éléments essentiels que nous redonnons ici.

On commence par se fixer une énumération des fonctions calculables, $\Phi$, où $\Phi_e$ est la fonction calculable de code $e$. On sait que toutes les fonctions calculables ont un code (par leur définition) et notre langage de termes $\Lambda$ ne contient naturellement que des fonctions calculables. On admet donc la thèse de Church, version interne de la thèse de Church-Turing, disant qu'on peut trouver une fonction $U$ universelle dans le sens qu'elle représente toutes les fonctions $\bN \to \bN$~:

\[\mathrm{CT} \defeq\exists U^{\{\Nat\to\Nat\to\Nat\to\Nat\}}, \forall f^{\{\Nat\to\Nat\}}, \exists e^{\Nat}, \forall x^{\{\Nat\}}, \exists t^{\Nat},
  \left\{\begin{array}{c}
  \forall t' < t, U(e,x,t') = 0\\
  U(e,x,t) = S(f(x))\\
  \forall t' > t, U(e,x,t') = U(e,x,t)
  \end{array}\right.\]

$U$ est une fonction prenant trois paramètres~: le code $e$ de la fonction qu'on veut simuler, l'entrée $x$ sur laquelle on veut simuler la fonction, et le temps de calcul $t$. Ainsi, la fonction $\Phi_e$ correspond d'une certaine  façon à $\lim_{t \to \infty} U(e,-,t)$. Ici, on considère que la valeur retournée par $U(e,x,t)$ est potentiellement vide~: c'est ce que signifie le fait de retourner $0$. On décide donc que la valeur $\Phi_e(x)$, si elle existe, est ici le prédécesseur de $\lim_{t \to\infty} U(e,x,t)$. Pour facilité la lecture, nous utiliserons la notation $\Phi_e$ pour les descriptions intuitives et $U$ pour la partie réellement formelle. On notera $\Phi_e(x) \downarrow= n$ pour signifier que le calcul converge et vaut $n$, et $\Phi_e(x) \uparrow$ pour signifier qu'il diverge. On ajoutera $[t]$ pour signifier que ces notions sont en se limitant à $t$ étapes de calcul (donc $\Phi_e(x)[t]\downarrow$ signifie que l'on a un résultat en moins de $t$ étapes, et $\Phi_e(x)[t]\uparrow$ que l'on n'a aucun résultat même en allant jusqu'à $t$ étapes).

L'idée de l'arbre de Kleene se base sur la notion de fonction diagonalement non calculable, que nous définissons donc.

\begin{defi}[Fonction DNC]
    Une fonction $f : \bN \to \bN$ (totale) est dite diagonalement non calculable (DNC) si elle vérifie
    \[\forall n^{\Nat}, \forall t^{\Nat}, U(n,n,t) \neq 0 \implies U(n,n,t) \neq S(f(n))\]
\end{defi}

Une fonction DNC est donc une fonction qui, pour chaque code $e$ tel que $\Phi_e(e)$ est défini, diffère de cette valeur. Une telle fonction ne peut pas être calculable, puisque par définition elle diffère de toutes les fonctions calculables. On définit maintenant l'arbre de Kleene comme un arbre binaire dont les éléments sont des approximations possibles d'une fonction DNC. On se fixe maintenant une fonction $F$, partielle mais DNC, et calculable~:
\[F(n)\defeq \left\{\begin{array}{l}
    1\text{ si } \Phi_e(e) \downarrow= 0\\
    0\text{ si } \Phi_e(e) \downarrow \neq 0\\
    \bot\text{ si } \Phi_e(e) \uparrow
\end{array}\right.\]
On notera $e$ le code de cette fonction.

On définit aussi les approximations de $F$, qu'on notera $F^{(n)}$, par~:
\[F^{(n)}(x) \defeq \left\{\begin{array}{l}
    F(x) \text{ si } F(x)[n]\downarrow\\
    \bot\text{ sinon}
\end{array}\right.\]
Chaque $F^{(n)}$ est calculable, on notera $e_n$ leur code respectif.

On définit maintenant l'arbre de Kleene~:

\begin{defi}[Arbre de Kleene]
    L'arbre $K\subseteq \{0,1\}^*$ est l'ensemble
    \[K \defeq \{\ell \in \{0,1\}^* \mid \forall i \in \{0,|\ell|\}, F^{(|\ell|)}(i) \neq \bot \implies \ell_i = F(i)\}\]
\end{defi}

Un mot est donc dans l'arbre s'il approche $F$ calculée sur sa longueur. On voit que $K$ est stable par préfixe, puisque si $u\preceq v$ alors $u$ et $v$ ont la même valeur sur les $i$ tels que $F^{(|v|)}(i)\neq \bot$, qui est plus défini que $F^{(|u|)}(i)$.

On peut montrer que $K$ peut être défini dans notre modèle par $f(\ell) = \btt$ pour une certaine fonction $f : \List(\Bool) \to \Bool$, cela nous indique qu'en plus d'être un prédicat sur $\{0,1\}$, $K$ est un prédicat décidable (on pourrait donc décider de limiter $\WKL$ sur des arbres décidables et le résultat resterait non prouvable dans notre modèle).

Prouvons maintenant le résultat souhaité.

\begin{them}
    Il n'existe pas de terme $r$ tel que
    \[r\reali \WKL\]
\end{them}

\begin{proof}
    Supposons que $r \reali \WKL$. On considère alors l'arbre $K$. Pour celui-ci, on peut trouver une suite de listes de taille $n$ appartenant à l'arbre~: pour $n\in \bN$, on peut considérer $F^{(n)}$ pour lequel on remplace la valeur $\bot$ par $0$ et dont on ne prend que les $n$ premiers symboles. On peut donc appliquer $r$ à cette suite, pour obtenir un chemin $\alpha \in_\infty K$. Cela signifie donc que $\alpha$ approche $F$ sur tous les points où $F$ est définie, puisque si $F$ est définie au bout d'un temps $t$ de calcul, alors $F^{(t)}$ est définie en ce point, donc, comme $\alpha \in_t K$, $\alpha_x$ est défini et vaut $F^{(t)}(x)$ (ou $F^{(x)}(x)$ si $x > t$). Ainsi $\alpha$ est une fonction DNC (en particulier, totale), ce qui n'est pas possible puisqu'elle est alors distincte de toute fonction $\bN \to \bN$. C'est donc absurde~: il n'existe pas de tel $r$.
\end{proof}

\section{Conclusion}

Nous avons donc construit un modèle de réalisabilité permettant de séparer $\WKL$ et $\FT$. Résumons les idées essentielles tirées de ce modèle, et les possibilités d'extension à d'autres modèles de réalisabilité~:
\begin{itemize}
    \item pour ne pas réaliser $\WKL$, il suffit de pouvoir utiliser l'argument de l'arbre de Kleene. Cet argument repose majoritairement sur la thèse de Church interne, mais on peut aussi relativiser cette construction~: on peut construire le même arbre et la notion de DNC relative en ajoutant un oracle. L'argument semble donc pouvoir s'adapter si les fonctions calculables pour le modèle (celles ayant un code) ont un degré Turing majoré.
    \item pour réaliser $\FT$ comme nous l'avons fait, deux points sont importants~: avoir $\KL$ dans la méta-théorie, c'est-à-dire pouvoir l'appliquer à un objet non interne (qui n'a pas de code a priori, ni en fait a posteriori), et avoir une notion de continuité sur les fonctions calculables. En fait, comme il semble important de pouvoir appliquer le réaliseur de $\FT$ à un chemin qui n'a pas de code, et donc auquel on ne devrait pas pouvoir appliquer le réaliseur, la continuité seule n'est pas forcément suffisante. Si l'on arrive à prouver que la suite des $\alpha_0\cdots\alpha_n 0^\infty$, pour reprendre nos notations, est stationnaire, alors il semble possible de réutiliser l'argument. Pour l'instant, il est au moins réaliste de croire que $\FT$ peut être réalisé à partir du moment où on peut associer à tous les objets du premier ordre un cpo et à toutes les fonctions ayant un code une fonction continue entre cpos. Catégoriquement, cela représente un foncteur entre nos objets du premier ordre et la catégorie \textbf{Cpo}.
\end{itemize}

Pour conclure ce rapport, nous allons aborder les conjectures et projets ultérieurs pouvant découler de ce travail.

\subsection{Aborder les evidenced frames}

Les \textit{evidenced frames}, introduites dans \cite{DBLP:conf/lics/0001MT21}, permettent de généraliser la réalisabilité et d'en unifier les usages. Partant d'une structure combinatoire simple, elle permet de récupérer ce que nous appelons en général des ``modèle de réalisabilité'', et offre aussi une voie pour en faire un traitement systématique. L'un des objectifs de ce stage est d'utiliser cette structure pour créer des méta-théorèmes généraux sur le fait de réaliser ou non des principes de choix. Le résultat idéal (que nous n'avons pas encore) est d'extraire, pour un certain principe de choix $C$, une propriété $P$ sur l'evidenced frame telle qu'on peut prouver
\[\forall \mathcal E \in \mathrm{Evidenced Frames}, P(\mathcal E) \implies \exists t, t\reali C\]
voire que l'on puisse prouver une équivalence au lieu d'une implication. Le sens d'une telle propriété serait alors de donner un sens calculatoire à $C$~: vérifier $C$ correspond à vérifier une notion liée au calcul, à travers la relation de réalisabilité.

Un potentiel travail ultérieur serait donc de montrer qu'une telle propriété $P$, pour $\FT$, est la continuité des fonctions calculables, et que $\WKL$ serait automatiquement invalidé si l'on traduit dans les evidenced frames le fait d'avoir des fonctions de degré Turing borné.

\subsection{Réaliser sans un langage typé}

Notre utilisation du langage PCF est particulièrement artificielle, puisqu'elle répondait à un besoin immédiat de construire une interprétation vers \textbf{Cpo} de notre langage, chose que l'on sait facilement faire avec un langage typé comme PCF mais pas avec un langage non typé. Cependant, nous pensons qu'il est très probable que l'on puisse interpréter notre langage actuel, sans termes typés, dans \textbf{Cpo}. Le faire semble relativement difficile, puisque l'argument habituel pour montrer qu'on peut interpréter PCF dans \textbf{Cpo} repose sur l'induction des dérivations de typage $\Gamma\vdash t : P$, ce que nous ne pouvons pas faire ici. Il reste que nos fonctions contiennent simplement un modèle de calcul, et devraient donc être continue, puisqu'elles peuvent être simulées par une machine de Turing et que cette machine ne peut traiter qu'un nombre fini d'informations.

\appendix

\section{Réaliseurs classiques}\label{app.A}

Nous donnons ici différents termes qui permettent la preuve du théorème \ref{them.FT}.

\subsection{Booléens}

On donne les opérations ``et'', ``ou'' et ``non''.

\[b \&\& b' \defeq \rec_{\Bool}\;(\lambda x.x)\;\bff\;b\;b'\]

\[b \| b'\defeq \rec_{\Bool}\;\btt\;(\lambda x.x)\]

\[\sim b \defeq \rec_{\Bool}\;\bff\;\btt\;b\]

\subsection{\'Egalités}

On définit les fonctions booléennes décidant l'égalité des booléens, des entiers et des listes d'un type $S$ où l'égalité sur $S$ est décidable.

\[b =_{\Bool} b' \defeq \rec_{\Bool}\;(\rec_{\Bool}\;\btt\;\bff)\;(\rec_{\Bool}\;\bff\;\btt)\;b\;b'\]

\[n =_{\Nat} n' \defeq \rec_{\Nat} \;(\rec_{\Nat}\;\btt\;(\lambda \_\;\_.\bff))\;(\lambda f\;m.\rec_{\Nat}\;\bff\;(\lambda\_\;m'.f\;m'))\;n\;n'\]

\[\ell =_{\List(S)} \ell' \defeq \rec_{\List}\;(\rec_{\List}\;\btt\;(\lambda\;\_\;\_.\bff))\;(\lambda f\;s.\rec_{\List}\;\bff\;(\lambda f'\;s'.(\ell =_S \ell')\&\& (f\;s')))\;\ell\;\ell'\]

On notera maintenant if / then / else plutôt que $\rec_{\Bool}$, par souci de lisibilité. De plus, on se permettra de faire des définitions récursives, telles que (pour reprendre les cas précédents)~:
\begin{align*}
    0 =_{\Nat} 0 &\defeq \btt \\
    S\;n =_{\Nat} 0 &\defeq \bff \\
    0 =_{\Nat} S\;n' &\defeq \bff\\
    S\;n=_{\Nat} S\;n' &\defeq n =_{\Nat} n'
\end{align*}
en considérant que la traduction vers la définition précédente peut toujours se faire, mais est moins lisible et plus laborieuse à écrire.

\subsection{Nombres entiers}

On définit la fonction $\leq$ décidant l'inégalité des entiers.

\begin{align*}
    0 \leq 0 &\defeq \btt \\
    0 \leq S\;n' &\defeq \btt \\
    S\;n \leq 0 &\defeq \bff \\
    S\;n \leq S\;n' &\defeq n \leq n'
\end{align*}

On admet l'existence des fonctions $/$ et $\%$ donnant le quotient et le reste de la division euclidienne, ainsi que de $+,-\times$ ($n-m$ retournant $\max(n-m,0)$). On définit la décomposition en base $p$ de $n$, notée $\basep{n}{p}$, par la liste des restes successifs de la division euclidienne de $n$ par $p$.

\[\basep{n}{p} \defeq Y(\lambda f\;x\;\ell.\mathrm{if} \;(x =_{\Nat} 0) \;\mathrm{then} \;\ell\;\mathrm{else} \;f\;(x\%p)\;((x/p)::\ell))\;n\;[\,]\]

\subsection{Listes}

On définit la fonction de préfixe sur des listes de données où l'égalité est décidables..

\begin{align*}
    [\,]\preceq [\,] &\defeq \btt \\
    [\,]\preceq h'::t' &\defeq \btt \\
    h::t \preceq [\,] &\defeq \bff \\
    h::t \preceq h'::t' &\defeq (h = h') \&\& (t \preceq t')
\end{align*}

Le même schéma permet de définir $\leq$ sur les entiers, en supprimant simplement $(h = h')$.

La fonction $|\ell|$ donnant la taille d'une liste~:

\begin{align*}
    |[\,]| &\defeq 0\\
    |h::t| &\defeq S\;|t|
\end{align*}

On définit une fonction prenant en paramètre $p$, qui énumère toutes les listes dans $\{0,\ldots,p-1\}^*$, par longueur croissante.

\begin{align*}
    \enumer\;p\;0 &\defeq [\,]\\
    \enumer\;p\;(S\;n) &\defeq \basep{n}{p}
\end{align*}

On a aussi besoin d'une fonction transformant une liste $\ell$ en la fonction $\bN \to A$ définie par $\ell 0^\infty$~:
\[\toinfty \defeq \rec_{\List}\;(\lambda n.0)\;(\lambda n.\lambda x.\lambda f.\rec_{\Nat}\;n\;(\lambda m.\lambda g.f\;m))\]

Enfin, on définit la fonction $\mathrm{forall}\;p\;n$ qui retourne l'ensemble des listes à valeurs dans $\{0,\ldots,p-1\}$ de taille $n$.

\bibliographystyle{plain}
\bibliography{biblio}

\end{document}
