\documentclass{article}
\usepackage{prelude}
\usepackage{ulem}
\usepackage[draft,inline,nomargin]{fixme}
\fxusetheme{color}
\fxuseenvlayout{color}
\FXRegisterAuthor{em}{aem}{\color{blue}[E]}

\title{Choix, réalisabilité et evidenced frames}

\author{Titouan Leclercq}

\date{April 15$^{\mathrm{th}}$ -- July 15$^{\mathrm{th}}$}

\begin{document}

\maketitle

\tableofcontents

\section{Réalisabilité}

L'élément central de ce stage est la réalisabilité. Nous nous devons donc d'en donner une présentation un peu détaillée. Pour cela, nous allons faire un détour légèrement historique en présentant l'idée à l'origine de ce concept.

\subsection{L'interprétation BHK}

L'interprétation de Brouwer-Heying-Kolmogorov (BHK) désigne une interprétation de la sémantique d'une proposition, non en termes de valeurs de vérité, mais en termes de l'ensemble de ses preuves.\emnote{je changerais le ``en termes''}

La volonté derrière l'interprétation BHK est de définir une logique constructiviste, en ce qu'une preuve d'une proposition doit explicitement construire les outils auxquels elle fait appel (en particulier, on cherche à avoir la propriété du témoin, qui dit que si $\vdash \exists x, \varphi$ alors il existe $t$ tel que $\vdash \varphi[t/x]$). Comme le principe du tiers-exclu est non constructif, on se place dans le cadre de la logique intuitionniste.

On suppose qu'on a un ensemble de propositions $\Phi$ et un ensemble de témoins, $X$. L'interprétation BHK donne une façon de penser une relation signifiant \textit{être une preuve de}, qu'on notera $x\reali \varphi$~:
\begin{itemize}
\item on suppose qu'on a défini ce que signifie $x\reali\varphi$ pour $\varphi$ atomique
\item on n'a jamais $x\reali \bot$
\item si $\varphi$ est de la forme $\psi\to \chi$, alors $x\reali\psi \to\chi$ signifie que $x$ représente une fonction qui, pour tout $y\reali\psi$, associe $x(y)\reali\chi$. Prouver que $\psi$ implique $\chi$ est donc une fonction qui peut renvoyer une preuve de $\chi$ dès qu'on a une preuve de $\psi$.\emnote{Je ferais attention à nuancer la notion de ``fonction'', ou en tout cas dire dans quel monde on parle de ``fonction'' (qui a son importance par la suite quand on ``implémente'' cette interprétation en réalisabiltié via un langage de termes potentiellement avec effets))}
\item si $\varphi$ est de la forme $\psi\land\chi$, alors $x\reali \psi\land \chi$ signifie que $x = \langle y,z\rangle$ \emnote{attention à ne pas utiliser de syntaxe avant de l'avoir introduite.. là tu peux dire  ``paire'' c'est suffisant je pense !} où $y\reali \psi$ et $z\reali \chi$. Ainsi \sout{être} une preuve d'une conjonction est une paire de preuves, donnant la preuve de chaque composante.
\item si $\varphi$ est de la forme $\psi\lor\chi$, alors $x\reali\psi\lor \chi$ signifie que $x = \langle i,y\rangle$ où $i\in\{1,2\}$ \emnote{même remarque} et, de plus, si $i = 1$ alors $y\reali \psi$ et si $i = 2$ alors $y\reali \chi$. Être une preuve d'une disjonction est donc une preuve d'une des deux propositions, accompagnée de l'information de laquelle des propositions est prouvée (c'est une union disjointe en terme ensembliste).
\item si $\varphi$ est de la forme $\forall a, \psi$ alors $x\reali\forall a, \psi$ signifie que $x$ représente une fonction qui, pour tout $v$, associe $x(a)\reali \psi[v/a]$. Une preuve d'une quantification universelle est ainsi une foncion qui, au terme auquel on veut appliquer la proposition, renvoie une preuve de cette proposition appliquée au terme.
\item si $\varphi$ est de la forme $\exists a, \psi$ alors $x\reali \exists a, \psi$ signifie que $x = \langle t,y\rangle$ où $t$ est un terme et $y\reali \psi[t/a]$. Ainsi, donner une preuve d'une proposition existentielle signifie donner un témoin du caractère existentiel (quel terme est désigné par le $\exists$) et une preuve que ce témoin vérifie effectivement la proposition.
\end{itemize}

On voudrait alors que la relation $\reali\subseteq X \times \Phi$ ainsi construite définisse une théorie
\[\mathcal T_\reali \defeq \{\varphi\in\Phi\mid \exists x \in X, x\reali \varphi\}\]

En fait, on voit que cette théorie est close par les règles de la déduction naturelle intuitionnistes sous quelques hypothèses sur la structure de $X$~:
\begin{itemize}
\item si on suppose qu'on a des preuves de $\varphi_1,\ldots,\varphi_n$ alors on a évidemment une preuve de $\varphi_i$ pour chaque $i\in\{1,\ldots,n\}$, d'où \begin{center}\begin{prooftree}\hypo{\varphi\in\Gamma}\infer1{\Gamma\vdash \varphi}\end{prooftree}\end{center}
\item si dans le contexte $\varphi_1,\ldots,\varphi_n$ on a une preuve de $\bot$, comme il n'y a pas de preuve de $\bot$ alors par principe d'explosion on déduit qu'on a une preuve de n'importe quelle proposition
\item si dans le contexte où on a des preuves de $\Gamma,\varphi$ on peut construire une preuve de $\psi$, alors on peut considérer la fonction, dans le contexte $\Gamma$, qui à une preuve de $\varphi$ associe la preuve de $\psi$ construite, d'où \begin{center}\begin{prooftree}\hypo{\Gamma, \varphi\vdash \psi}\infer1{\Gamma\vdash\varphi\to\psi}\end{prooftree}\end{center}
\item si dans le contexte $\Gamma$ on a une preuve de $\varphi \to \psi$ et une preuve de $\varphi$, comme une preuve de $\varphi\to\psi$ est une fonction, on peut l'appliquer en la preuve de $\varphi$ pour obtenir une preuve de $\psi$, d'où \begin{center}\begin{prooftree}\hypo{\Gamma\vdash \varphi\to\psi}\hypo{\Gamma\vdash\varphi}\infer2{\Gamma\vdash \psi}\end{prooftree}\end{center}
\end{itemize}

On peut faire de même pour $\land$, $\lor$, $\forall$ et $\exists$. Pour pouvoir construire notre interprétation, il nous faut donc un ensemble $X$ muni de structure~:
\begin{itemize}
\item chaque $x\in X$ doit pouvoir se voir comme une forme de fonction dont on peut abstraire les entrées, au sens où si $t\in X$ possède des entrées $a_1,\ldots,a_n$ alors on veut avoir $\lambda a_i. t\in X$ qui est une entrée sur $a_1,\ldots,a_n \setminus a_i$ donnant la fonction $a_i \mapsto t$. En ayant cela, l'introduction et l'élimination de $\to$ devient automatique.
\item on doit être capable de créer des paires $\langle x,y\rangle \in X$ d'éléments de $X$, et des projections pour récupérer les informations sur chaque coordonnée. Dans ce cas l'introduction et l'élimination de $\land$ est directe.
\item on doit pouvoir représenter au moins un ensemble $\{0,1\}$ pour pouvoir définir l'introduction de $\lor$. Pour l'élimination de $\lor$, il nous faut une instruction if / then / else sur cet ensemble $\{0,1\}$ représenté pour faire une disjonction de cas sur la première coordonnée d'une preuve de $\varphi\lor \psi$.
\item on doit pouvoir représenter les termes du premier ordre au sein même de $X$, pour pouvoir considérer une fonction qui à un terme du premier ordre associe un élément de $X$ et avoir des paires dont une coordonnée est un terme du premier ordre.
\end{itemize}

Ces conditions donnent une estimation assez fidèle de ce que l'on va rechercher dans un modèle de réalisabilité. On va maintenant voir le premier exemple de réalisabilité, qui est une interprétation comme on l'a définie ici utilisant les fonctions calculables~: la réalisabilité de Kleene.

\subsection{Réalisabilité de Kleene}

Ce modèle de réalisabilité est en fait le point de départ même de la réalisabilité. Il se base sur le fait que pour raisonner sur les entiers, les fonctions calculables donnent une classe de fonctions largement raisonnable. Cependant, on a vu qu'il fallait, en plus de fonctions, pouvoir internaliser les termes du premier ordre. Heureusement, un théorème de calculabilité nous dit qu'il existe une fonction de codage $\varphi$ qui est surjective dans les fonctions calculables (et prend un entier en paramètre).

On considère donc $X = \bN$, et on peut vérifier que les conditions sont vérifiées~:
\begin{itemize}
\item si $e$ représente une fonction calculable sur $n + m$ entrées, on peut construire un code $e'$ représentant cette fonction calculable sur $n$ entrées dont les autres $m$ entrées ont été abstraites~: c'est le théorème $S_m^n$\emnote{ref}. De plus, on a une opération d'application d'une fonction calculable à un argument.
\item la bijection de Cantor nous donne une fonction $(n,m) \mapsto \langle n,m\rangle$ calculable dont les projections sont calculables.
\item l'ensemble $\{0,1\}$ est un sous-ensemble de $\bN$, il est donc évident qu'on peut l'utiliser. Pour la construction if / then / else, c'est une construction élémentaire en programmation, et on peut la combiner à l'égalité à $0$ pour obtenir l'élimination de $\lor$.
\item on peut représenter l'entier $n\in \bN$ dans $X$, par $n \in \bN$.
\end{itemize}

On peut donc construire une interprétation de $\bN$ comme un ensemble de preuves pour le langage de l'arithmétique. Il se trouve qu'on obtient alors un modèle de l'arithmétique de Heyting (qui est l'arithmétique de Peano mais dans la logique intuitionniste).

\subsection{Ordre supérieur}

Les conditions décrites plus haut peuvent se simplifier grandement dans le cas où l'on étudie une théorie non pas d'ordre $1$, mais d'ordre $2$ (ou d'ordre supérieur)\emnote{dire pourquoi !}. Présentons succinctement la syntaxe d'une théorie d'ordre $2$ (avec des termes d'ordre 1).

On ajoute un ensemble de variables du second ordre, qu'on notera par des lettres majuscules, chacune ayant une arité. Une variable $X$ d'arité $n$ représente un prédicat à $n$ variables libres quelconque. On construit alors nos propositions comme pour celle du premier ordre, mais en ajoutant la prise en compte (et la quantification) du deuxième ordre~:
\[\varphi,\psi ::= \cdots\mid X(t_1,\ldots,t_n)\mid \forall^2 X, \varphi\mid \exists^2 X, \varphi\]

En fait, on se rend compte qu'il est possible de largement diminuer le nombre de constructeurs pour nos propositions en encodant par exemple $\land$ et $\lor$ à partir de $\to$ et de $\forall^2$. On définit donc simplement
\[\varphi,\psi ::= X(t_1,\ldots,t_n)\mid \varphi\to\psi\mid \forall^1 x, \varphi\mid \forall^2 X, \varphi\]

On donne l'ensemble des encodages qu'on peut alors faire~:
\begin{itemize}
\item $t = u \defeq \forall X, X(t) \to X(u)$
\item $\varphi\land \psi \defeq \forall X, (\varphi \to \psi \to X) \to X$
\item $\varphi \lor \psi \defeq \forall X, (\varphi \to X) \to (\psi \to X) \to X$
\item $\exists x, \varphi \defeq \forall X, (\forall x, \varphi \to X) \to X$
\item $\exists X, \varphi \defeq \forall Y, (\forall X, \varphi \to Y) \to Y$
\end{itemize}

Ainsi, si l'on arrive à avoir une interprétation de l'ordre $2$, il nous suffit seulement d'interpréter $\to$, $\forall^1$ et $\forall^2$. Remarquons que l'interprétation de $\to$ \emnote{?} force $X$ à pouvoir internaliser une notion de fonction (avec un procédé d'abstraction et d'application) \emnote{pas clair je crois, et attention $X$ est surchargée en tant que notation? $\mathbb{X}/\mathcal{X}$ ? }·.

\subsection{Intersection et relativisation}

On a vu qu'il était nécessaire de représenter les termes du premier ordre dans $X$, mais ceci peut être évité en changeant notre façon de considérer la quantification universelle. Plutôt que de voir $\forall x, \varphi$ comme une fonction, on peut aussi considérer cela comme une intersection. Dans ce cas, il nous faut pouvoir définir ce que signifie $t\reali\varphi$ avec $x\leftarrow n$ pour un certain $n$, ce qui nous pousse à généraliser notre relation $\reali$ avec des contextes. On notera $t\reali^\sigma \varphi$ pour dire que $t$ est une preuve de $\varphi$ dans le cas où $\sigma$ est utilisé pour interpréter $\varphi$. Dans ce cas, $\sigma$ va associer un entier à chaque variable libre dans $\varphi$. On introduit de même un contexte pour les variables du second ordre qui pourraient être libres dans $\varphi$, qu'on notera $\rho$, et où une variable $X$ d'arité $n$ est associée à une fonction $\bN^n \to \mathcal P(X)$. On notera alors $t\reali_\rho^\sigma \varphi$ pour noter nos deux contextes.

On arrive alors à une définition alternative de $\reali$ considérant les quantifications comme des preuves uniformes~:
\begin{itemize}
\item $t\reali^\sigma \forall x,\varphi \defeq \forall n \in \bN, (t\reali^{\sigma[x \leftarrow n]} \varphi)$
\item $t\reali_\rho \forall X, \varphi \defeq \forall S : \bN \to \mathcal P(X), (t\reali_{\rho[X\leftarrow S]}\varphi)$
\end{itemize}

C'est avant tout cette interprétation du second ordre qu'on considère, où réaliser une quantification signifie réaliser uniformément toutes les instances possibles.

On voit qu'alors il n'est plus nécessaire d'internaliser les notions du premier (ni même du second) ordre, et il nous suffit alors d'avoir un langage dans lequel on peut abstraire et appliquer des fonctions~: il est dur de ne pas penser au $\lambda$-calcul dans ce contexte, mais nous y reviendrons plus tard.

Il reste cependant souhaitable de pouvoir considérer le $\forall$ comme fonctionnel. En effet, il y a des cas dans lesquels realiser $\varphi(x)$ demande explicitement de savoir de quel $x$ on parle, et le réaliseur peut dépendre directement de ce $x$. L'avantage de l'interprétation uniforme est qu'elle permet de récupérer cela dans un deuxième temps~: il suffit de construire un prédicat dont l'utilité est de récupérer le $x$ que l'on veut. Dans le cas des entiers, on peut construire un prédicat $\bN(x)$ tel que $n\reali_\rho^\sigma\bN(x)$ exactement quand $x$ est interprété dans $\sigma$ par $n$. Dans ce cas, écrire $\forall x, \bN(x) \implies \varphi$ nous dit exactement que pour n'importe quel $n$, obtenir l'information de ce $n$ permet de prouver $\varphi$.

On a donc deux sortes de quantifications~: les quantifications uniformes et les quantification relativisées, l'une considérant l'intersection et l'autre l'espace fonctionnel.

En fait, on verra que la notion de relativisation peut se voir comme un cas particulier d'une construction catégorique plus générale (la construction \textit{tripos to topos} dans le cas d'un tripos de réalisabilité).

\subsection{Lambda-calcul et saturation}

On a mentionné plus tôt qu'il était naturel, dans le contexte donné, de considérer le $\lambda$-calcul comme candidat pour faire un modèle de réalisabilité. En reprenant ce qui a été dit plus tôt, en considérant comme langage l'arithmétique du second ordre, on définit donc une interprétation en utilisant pour $X$ l'ensemble $\Lambda$ des $\lambda$-termes, engendré par
\[t,u ::= x\mid \lambda x. t\mid t\;u\]

On donne donc une première tentative de définition de $t\reali_\rho^\sigma \varphi$ où $\rho$ associe toute variable libre du premier ordre de $\varphi$ à une fonction $\bN^n \to \mathcal P(\Lambda)$ et $\sigma$ associe toute variable libre du premier ordre de $\varphi$ à un entier $n \in \bN$. On utilisera la notation $\trad\varphi_\rho^\sigma \defeq \{t\in\Lambda\mid t \reali_\rho^\sigma \varphi\}$~:
\begin{itemize}
\item $\trad{X(t_1,\ldots,t_n)}_\rho^\sigma \defeq \rho(X)(t_1^\sigma,\ldots,t_n^\sigma)$
\item $\trad{\varphi \to \psi}_\rho^\sigma\defeq \{t\in \Lambda \mid \forall u\reali_\rho^\sigma \varphi, t\;u\reali_\rho^\sigma \psi\}$
\item $\trad{\forall x, \varphi}_\rho^\sigma \defeq \bigcap_{n \in \bN}\trad\varphi_\rho^{\sigma[x\leftarrow n]}$
\item $\trad{\forall X, \varphi}_\rho^\sigma \defeq \bigcap_{S : \bN^n \to \mathcal P(\Lambda)}\trad\varphi_{\rho[X\leftarrow S]}^\sigma$
\end{itemize}

Tout semble bien fonctionner, mais il y a en fait un problème~: notre interprétation précédente considérait $X$ comme un objet sémantique et non syntaxique. On entend par cela que dans $\Lambda$, il y a une différence entre $(\lambda x.t)u$ et $t[u/x]$ qui sont des termes différents, mais sont dans la même classe de $\Lambda/=_\beta$. On pourrait donc décider de ne considérer non pas $\Lambda$ mais $\Lambda/=_\beta$, mais il existe une façon plus élégante de régler ce problème.

L'endroit où cette distinction apparait est dans la preuve que les règles de la déduction naturelle intuitionniste sont vérifiées par notre interprétation de réalisabilité, plus précisément dans la règle d'introduction de $\to$. En considérant un système de types associant une proposition à un terme, de la forme $x_1 : \varphi_1,\ldots,x_n : \varphi_n \vdash t : \varphi$. Dans ce cas, la validité d'un tel séquent est\emnote{pas convaincu pas les substitutions en exposant, c'est peu standard}
\[t_1\reali_\rho^\sigma \varphi_1,\ldots,t_n \reali_\rho^\sigma \varphi_n \implies t^{x_1 \leftarrow t_1,\ldots, x_n\leftarrow t_n}\reali_\rho^\sigma \varphi\]
et on peut réécrire $\to_\mathrm i$ par la règle
\begin{center}
  \begin{prooftree}
    \hypo{\Gamma, x : \varphi\vdash t : \psi}
    \infer1{\Gamma\vdash \lambda x.t : \varphi \to \psi}
  \end{prooftree}
\end{center}

On veut donc unifier d'un côté $t^{x_1\leftarrow t_1,\ldots,x_n\leftarrow t_n, x\leftarrow u}$ et $(\lambda x.t^{x_1\leftarrow t_1,\ldots,x_n\leftarrow t_n})u$, ce qui (à des questions d'$\alpha$-conversion près) est une antéréduction~\emnote{anti-réduction}: le second terme se réduit en le premier terme.

Ainsi la validité de cette règle a besoin d'un élément supplémentaire qui peut être satisfait par la condition suivante~: il faut qu'à chaque $\varphi$, si $t\reduc u$ et $u\reali \varphi$ alors $t\reali \varphi$. En fait, on peut montrer qu'il suffit pour cela d'assurer cette condition sur les $\rho(X)$. Donnons donc un peu de vocabulaire.

\begin{defi}[Partie saturée]
  Soit $S\subseteq\Lambda$, on dit que $S$ est saturée si la propriété suivante est vérifiée~:
  \[\forall t,u\in\Lambda, u \in S \land t \reduc u \implies t \in S\]

  On note
  \[\SAT \defeq \{S\subseteq \Lambda\mid S\;\text{est saturée}\}\]
\end{defi}

\begin{lem}
  $(\SAT,\subseteq,\Lambda,\varnothing,\bigcap,\bigcup)$ est un treillis complet.
\end{lem}

\begin{proof}
  Il est évident que $S\in \SAT \implies \varnothing\subseteq S \subseteq \Lambda$ et que ces deux parties sont stables par anté-réduction. Supposons que $\{S_i\}_{i\in I}$ est une famille de parties saturées. Si $t\reduc u$ et $u \in \bigcap S_i$, alors pour tout $i \in I$, $t \in S_i$ par saturation de $S_i$, donc $t\in \bigcap S_i$, donc $\bigcap S_i$ est saturée. De même si $u \in \bigcup S_i$, alors on trouve $i$ tel que $u \in S_i$ donc $t\in S_i$ par saturation. Donc $\bigcup S_i$ est saturée. Ainsi $(\SAT,\subseteq,\Lambda,\varnothing, \bigcap,\bigcup)$ est un treillis complet.
\end{proof}

On décide maintenant de modifier l'interprétation au-dessus en disant juste que $\rho(X) : \bN^n \to \SAT$, et de même que l'intersection pour le $\forall X, \varphi$ est définie sur $S : \bN^n \to \SAT$. On peut alors montrer le lemme de saturation.

\begin{lem}[Saturation]
  Pour toute formule $\varphi$, $\trad\varphi_\rho^\sigma\in \SAT$.
\end{lem}

\begin{proof}
  On procède par induction sur $\varphi$~:
  \begin{itemize}
  \item si $\varphi = X(t_1,\ldots,t_n)$ alors $\trad\varphi_\rho^\sigma=\rho(X)(t_1^\sigma,\ldots,t_n^\sigma)\in \SAT$.
  \item si $\varphi = \psi \to \chi$, supposons que $t\reali_\rho^\sigma \psi \to \chi$, $t'\reduc t$ et $u\reali_\rho^\sigma\psi$. Par hypothèse, $t\;u\reali_\rho^\sigma \chi$, et comme par hypothèse d'induction $\trad\chi_\rho^\sigma\in\SAT$ et $t'\;u\reduc t\;u$, on en déduit que $t'\;u\reali_\rho^\sigma \chi$. Ainsi pour tout $u\reali_\rho^\sigma\psi$, $t'\;u\reali_\rho^\sigma\chi$, ce qui signifie que $t'\reali_\rho^\sigma\psi\to\chi$.
  \item les deux interprétations données par des intersections donnent des ensembles saturés car $\SAT$ est un treillis complet.
  \end{itemize}
  Donc par induction $\trad\varphi_\rho^\sigma\in\SAT$.
\end{proof}
\emnote{attention, tu utilises la réduction sans l'avoir définie, et ce que tu énonces n'est pas toujours vrai (par exemple pour une réduction cbv droite-gauche ça ne marcherait pas), donc il y a besoin de fixer le cadre avant !}

On peut alors déduire le lemme d'adéquation (on le montrera plus tard pour une version enrichie du $\lambda$-calcul).

\subsection{Conclusion sur la réalisabilité}

Notre situation de base pour la réalisabilité est donc de pouvoir construire des théories cohérentes sur l'arithmétique du second ordre, en se basant sur un $\lambda$-calcul. On verra plus tard que les effets qu'on ajoute à notre $\lambda$-calcul peuvent grandement changer la théorie engendrée.

\section{Mathématiques à rebours}

L'autre part importante de ce stage se trouve dans les mathématiques à rebours. Par mathématiques à rebours, on désigne un domaine, lié à la calculabilité, dont l'objectif est de quantifier la force logique des théorèmes usuels des mathématiques. Nous allons donc présenter les éléments importants de ce domaine qui sont utilisés dans le stage.

\subsection{Présentation globale}

La première question à se poser, pour organiser les théorèmes suivant leur force logique, est \textit{comment peut-on décider si un théorème est plus fort qu'un autre ?} Il est assez clair qu'on s'attend à un pré-ordre tout sauf total (mais à un pré-ordre quand même). L'idée la plus simple est de dire que $\varphi$ est un théorème plus fort que $\psi$ si $\varphi \vdash \psi$, donc si $\varphi$ est une proposition plus faible pour l'ordre de prouvabilité que $\psi$ (c'est assez logique~: être un théorème très fort, c'est être un théorème dont la preuve implique un maximum de choses). Le souci, maintenant, est que si on considère pour $\vdash$ la relation de prouvabilité dans $\ZF$, alors la plupart des résultats sont vrais\emnote{j'imagine que tu veux dire ``dans un contexte vide'', ce qui induit que tout s'écrase}. On conviendra que le pré-ordre $(\{*\},=)$ n'est pas le plus intéressant, il nous faut donc quelque chose de plus précis.

Si un théorème fort est un théorème qui entraine plus de résultats avec sa vérité, et comme une théorie est un ensemble de résultats pris pour axiomes, on peut donc s'attendre à ce qu'une théorie plus faible prouve moins de résultats directement, et laisse donc plus de marge pour séparer des résultats. Cependant, la théorie doit être suffisamment forte pour rester expressive, et prouver ce qu'on considère comme le plus élémentaire. Par exemple dire que l'addition est commutative dans $\bN$ n'est pas un résultat qu'on souhaite placer dans notre ordre.

Le choix se porte alors sur l'arithmétique du second ordre, plus précisément sur un sous-système appelé RCA$_0$. Ce sous-système contient suffisamment pour parler des réels et des fonctions continues (même des fonctions mesurables). C'est donc un système très expressif, relativement à sa faible capacité à prouver des résultats. \emnote{peut-être dire qu'il y a plusieurs sous-système possibles ?}

\subsection{Le cas intuitionniste et l'indépendance relative}

Comme on l'a dit, plus la théorie de base est faible, plus on est précis dans le pré-ordre intuitif de la force logique. Malheureusement, ça n'est pas toujours ce que l'on souhaite. Par exemple, l'une des questions de théorie des ensembles les plus importantes de l'histoire a porté sur un résultat indépendant de ZFC, l'hypothèse du continu. Dans ce cas, prouver que le résultat est indépendant de ZFC est un meilleur résultat que celui que l'hypothèse du continu est indépendant de ZF.

Syntaxiquement, cela se voit par le fait qu'une preuve de l'hypothèse du continu dans ZFC pourrait se formuler dans ZF\emnote{l'inverse non?}, donc l'un implique l'autre. Sémantiquement, cela signifie qu'on peut trouver un modèle validant HC et un validant $\lnot$HC dans une classe de modèles encore plus restreinte que celle des modèles de ZF~: celle des modèles de ZFC. Dans les deux cas, on comprend qu'une théorie plus forte donne un meilleur résultat d'indépendance.

Dans le cas intuitionniste, cette volonté de prouver des cas d'indépendance est récurrente, et il est donc bon de se placer dans un cadre plus fort. En particulier, la version intuitionniste de ZF (pas de ZFC, car le tiers exclu est vrai dans ZFC) nous donne de meilleurs résultats d'indépendance.

\subsection{Les formes faibles de l'axiome du choix}

L'axiome du choix, connu pour être un axiome indépendant de ZF et pour avoir été débattu dans le choix de l'accepter ou non (en raison par exemple de son caractère non constructif), peut se décliner en beaucoup de version faibles. Nous allons en présenter plusieurs, dans l'ordre croissant de leur force logique en logique classique~:
\begin{itemize}
\item \textit{Fan Theorem} (FT)~: supposons qu'on ait un prédicat $P$ sur $\bN^*$ qui est clos par extension, c'est-à-dire que si $u\in P$ et $u \preceq v$ alors $v\in P$. Supposons que pour tout chemin infini $\alpha\in\bN^\bN$ il existe un nombre $n$ tel que $\alpha_{0,\ldots,n}\in P$, alors il existe $n \in\bN$ tel que pour tout $\alpha \in \bN^\bN$, $\alpha_{0,\ldots,n}\in P$.
\item \textit{Weak König's Lemma} (WKL)~: tout arbre binaire infini possède une branche infinie.
\item \textit{König's Lemma} (KL)~: tout arbre infini mais finiment branchant (pour tout n\oe ud il existe un nombre fini de n\oe uds qui en sont voisins) possède une branche infinie.
\item \textit{Axiome du choix dénombrable} (ACN)~: pour toute famille $(X_i)_{i\in \bN}$ d'ensembles non vides, il existe une fonction $\alpha : \bN \to \bigcup X_i$ telle que $\alpha(i)\in X_i$.
\item \textit{Axiome du choix dépendant} (DC)~: soit un ensemble $X$ et une relation binaire $R$ sur $X$ telle que pour tout $x\in X$, il existe $y\in X$ tel que $R(x,y)$, alors il existe une suite $\alpha : \bN \to X$ telle que $\forall i\in \bN, R(\alpha(i),\alpha(i+1))$.
\item \textit{Axiome du choix} (AC)~: toute surjection admet une section.\emnote{un peu rude pour les non-spécialistes =)}
\end{itemize}

On sait que dans le cas classique, cette hiérarchie est strictement croissante \emnote{sauf pour Fan, non  ?}. Dans le cas intuitionniste, plus faible, la hiérarchie est donc encore stricte. Par contre, elle n'est pas croissante, puisqu'on peut par exemple avoir DC sans avoir KL.

En fait, on peut distinguer deux principes importants dans les premières versions~: l'omniscience et le choix. Dans le cas de DC, on peut construire notre suite simplement en considérant la suite jusqu'à un rang $n$ et en appliquant un oracle permettant de trouver un élément en relation avec le dernier construit. On passe donc simplement d'une fonction locale à une fonction globale par itération. Dans le cas de KL, il nous faut traiter l'infinité de données des chemins potentiels pour trouver le sommet suivant dans le chemin, en pouvant savoir à l'avance de quel côté se trouve un chemin infini. En logique classique, l'omniscience est toujours vérifiée, mais pas en intuitionniste, d'où cette cassure de monotonie au passage de KL à ACN.

\section{Plus de généralité pour la réalisabilité}

Nous avons abordé pour l'instant la réalisabilité sous un angle assez empirique~: étant donné un $\lambda$-calcul, on peut essayer en vérifiant des conditions assez intuitives de construire un modèle de réalisabilité, et de voir la théorie en résultant. Prendre une approche plus systématique et abstraire le système de calcul en lui-même va nous permettre d'aller plus loin que cet empirisme.

\subsection{Les effets sont logiques}

Une branche importante de la réalisabilité, la réalisabilité à le Krivine (ou réalisabilité classique), naît de l'observation que l'instruction call/cc, qui est une instruction de programmation permettant par exemple d'utiliser des exceptions, peut être typée par la loi de Peirce, principe équivalent au tiers exclu. Comme les exceptions permettent en quelque sorte de revenir en arrière dans l'exécution d'un programme, ce typage donne alors à penser que la capacité de revenir en arrière permet de passer de la logique intuitionniste à la logique classique. On peut aussi voir ça en terme de jeux en considérant que la logique intuitionniste demande à un défendant de gagner face à une série de contradictions en étant toujours cohérent avec lui-même, là où la logique classique premet au défendant de changer d'avis sur un argument en redéfendant cet argument du début.

Cela illustre un principe central de la réalisabilité~: ajouter des effets au langage de programmation donne dans le modèle de réalisabilité des formules logiques qui peuvent devenir vraies. Jean-Louis Krivine a alors pu utiliser ce typage pour construire une version de la réalisabilité dans laquelle la théorie $\mathcal T_\reali$ est une théorie classique.

On peut donc naturellement se demander comment classer les effets selon les théorèmes qui en découlent. Pour cela, il devient important de trouver un cadre plus uniforme pour aborder la réalisabilité, puisqu'il est difficile de traiter d'abstraction sans le faire dans un cadre défini. Le cadre trouvé dans le cas de la réalisabilité est celui des catégories.

\subsection{Tripos de réalisabilité}

Les catégories nous permettent d'étudier la logique, en particulier avec la notion de catégorie fibrée. Celle-ci permet de définir la notion d'avoir deux catégories où l'une exprime un langage parlant de l'autre catégorie, ou de construire une famille de catégories indicée par une autre catégorie.

Dans le cas de la pur logique propositionnelle (intuitionniste), la structure algébrique associée est celle des pré-algèbres de Heyting, qui correspond catégoriquement aux catégories ordonnées cartésiennes fermées. Pour en faire de la logique parlant plus explicitement d'une certaine catégorie (disons des ensembles), on considère une fibration au-dessus de cette catégorie, dont les fibres sont des pré-algèbres de Heyting.

Nous allons donc voir les définitions importantes pour manipuler des catégories fibrées.

\begin{defi}[Flèche cartésienne]
  Soient $\bE,\bB$ deux catégories et $p : \bE \to \bB$ un foncteur. On dit qu'une flèche $u : X \to Y$ dans $\bE$ est cartésienne si la propriété suivante est vérifiée~: pour toute flèche $v : Z \to Y$ et $k : pZ \to pX$ il existe une unique flèche $w : Z \to X$ telle que $v = u\circ w$. Cela se résume par le diagramme suivant~:
\end{defi}

\begin{defi}[Catégorie fibrée]
  Une catégorie fibrée est la donnée de deux catégories $\bE,\bB$ et d'un foncteur $p : \bE \to \bB$ satisfaisant la propriété suivante~: pour toute
\end{defi}

\section{Boîte à outils réalisable}

\subsection{Un modèle de HA2 amélioré}

On commence par donner une présentation d'une version enrichie de l'arithmétique de Heyting du second ordre (HA2) avec un modèle de réalisabilité basé sur le lambda-calcul.

\subsubsection{Langage de programmation}

Le $\lambda$-calcul est connu pour permettre de représenter n'importe quelle fonction calculable. A ce titre, on pourrait considérer le $\lambda$-calcul pur non typé pour réaliser des propositions. Cependant, pour la même raison, nous allons adopter un $\lambda$-calcul possédant plus de constructeurs~: tant qu'à pouvoir coder tout ce que l'on souhaite, autant se donner directement des primitives pour manipuler les éléments qui nous importent.

\begin{defi}[$\Lambda$]
  On se donne un ensemble $\mathcal X_\Lambda$ de variables de termes. On définit l'ensemble des $\lambda$-termes, $\Lambda$, par la grammaire suivante~:
  \[t,u ::= x\mid\lambda x. t\mid t\;u\mid \langle t,u\rangle\mid \pi_1\;t\mid \pi_2\;t\mid 0\mid S\;t\mid \rec_\bN\;t\;u\;u\mid \btt\mid\bff\mid\rec_\bB\;t\;u\;v\mid [\;]\mid t:: u\mid \rec_\bL\;t\;u\;v\]
\end{defi}

\begin{defi}[Contexte]
  On définit l'ensemble $\Lambda_{\bnnbr{\;}}$ des contextes, définit récursivement par~:
  \begin{multline*}
    E,F::= \bnnbr{\ }\mid \lambda x.E\mid E\;t\mid t\;E\mid \langle E,t\rangle\mid \langle t,E\rangle\mid \pi_1\;E\mid\pi_2\;E\mid\bfS\;\mid \rec_\bN\;E\;t\;u\mid \rec_\bN\;t\;E\;u\mid\rec_\bN\;t\;u\;E\\\mid\rec_\bB\;E\;t\;u\mid\rec_\bB\;t\;E\;u\mid\rec_\bB\;t\;u\;E\mid E::t\mid t::E\mid \rec_\bL\;E\;t\;u\mid \rec_\bL\;t\;E\;u\mid\rec_\bL\;t\;u\;E
  \end{multline*}

  Si $C$ est un contexte et $t$ un $\lambda$-terme, on note $C\bnnbr{t}$ le terme obtenu en substituant $\bnnbr{}$ par $t$ dans l'écriture de $C$.
\end{defi}

On définit de façon usuelle la substitution ainsi que la relation d'$\alpha$-équivalence, modulo laquelle on travaillera à partir de maintenant.

\begin{defi}[Réduction]
  On définit la relation $\mapsto\subseteq \Lambda\times\Lambda$ par les règles suivantes~:
  \begin{center}
    \begin{prooftree}
      \infer0{(\lambda x.t)u\mapsto t[u/x]}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \infer0{\forall i\in\{1,2\}, \pi_1\;\langle t_1,t_2\rangle \mapsto t_i}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \infer0{\rec_\bN\;t\;u\;0\mapsto t}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \infer0{\rec_\bN\;t\;u\;(S\;v)\mapsto u\;v\;(\rec_\bN\;t\;u\;v)}
    \end{prooftree}

    \vspace{0.5cm}
    
    \begin{prooftree}
      \infer0{\rec_\bB\;t\;u\;\btt\mapsto t}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \infer0{\rec_\bB\;t\;u\;\bff\mapsto u}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \infer0{\rec_\bL\;t\;u\;[\;]\mapsto t}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \infer0{\rec_\bL\;t\;u\;(v::w)\mapsto u\:v\:w\:(\rec_\bL\:t\:u\:w)}
    \end{prooftree}
  \end{center}

  On définit alors la relation $\reduc$ par
  \[t\reduc u \defeq \exists C \in\Lambda_{\bnnbr{\;}},\exists t'\;u'\in\Lambda, t = C\bnnbr{t'}\land u = C\bnnbr{u'}\land t' \mapsto u'\]
\end{defi}

On introduit la notion d'ensemble saturé, qui peut se considérer comme une partie de $\Lambda$ qui est calculatoirement pertinente.

\begin{defi}[Partie saturée]
  Soit $S\subseteq\Lambda$, on dit que $S$ est saturée si la propriété suivante est vérifiée~:
  \[\forall t,u\in\Lambda, u \in S \land t \reduc u \implies t \in S\]

  On note
  \[\SAT \defeq \{S\subseteq \Lambda\mid S\;\text{est saturée}\}\]
\end{defi}
\emnote{Attention, implicitement ``may''-réduction.}
\subsubsection{Partie logique}

On définit maintenant la partie logique de notre modèle de réalisabilité. Celui-ci est un modèle multi-sorté, dont les sortes sont données par la grammaire suivante~:
\[S,S' ::= \Nat\mid\Bool\mid\List\mid S \to S'\]
représentant respectivement les entiers, les booléens, les listes d'entiers et les fonctions d'une sorte à une autre. On notera $\Sort$ l'ensemble des sortes. On définit maintenant les termes du premier ordre (qui sont donc typés).

\begin{defi}[Termes du premier ordre]
  On se donne un ensemble dénombrable $\mathcal X_1$ de variables du premier ordre. Un contexte de typage du premier ordre $\Gamma$ est une liste de paires $(x,\tau)\in \mathcal X_1\times \Sort$. On définit l'ensemble des termes du premier ordre bien typés par les règles suivantes~:
  \begin{center}
    \begin{prooftree}
      \infer0{\Gamma, x : S \vdash x : S}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \infer0{\Gamma\vdash \bZ : \Nat}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash \bt : \Nat}
      \infer1{\Gamma\vdash \bfS\;\bt : \Nat}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash \bt : S}
      \hypo{\Gamma\vdash \bu : \Nat \to S \to S}
      \hypo{\Gamma\vdash \bv : \Nat}
      \infer3{\Gamma\vdash \rec_{\Nat}\;\bt\;\bu\;\bv : S}
    \end{prooftree}

    \vspace{0.5cm}

    \begin{prooftree}
      \infer0{\Gamma\vdash \bbtt : \Bool}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \infer0{\Gamma\vdash \bbff : \Bool}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash \bt : S}
      \hypo{\Gamma\vdash \bu : S}
      \hypo{\Gamma\vdash \bv : \Bool}
      \infer3{\Gamma\vdash \rec_{\Bool}\;\bt\;\bu\;\bv : S}
    \end{prooftree}

    \vspace{0.5cm}
    
    \begin{prooftree}
      \infer0{\Gamma\vdash \bnil : \List}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash \bt : \Nat}
      \hypo{\Gamma\vdash \bu : \List}
      \infer2{\Gamma\vdash \bt \bcons \bu : \List}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash \bt : S}
      \hypo{\Gamma\vdash \bt : \Nat \to \List \to S \to S}
      \hypo{\Gamma\vdash \bv : \List}
      \infer3{\Gamma\vdash \rec_{\List}\;\bt\;\bu\;\bv : S}
    \end{prooftree}

    \vspace{0.5cm}

    \begin{prooftree}
      \hypo{\Gamma, \bx : S \vdash \bt : S'}
      \infer1{\Gamma\vdash \blam \bx.\bt : S \to S'}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash \bt : S \to S'}
      \hypo{\Gamma\vdash \bu : S}
      \infer2{\Gamma\vdash \bt(\bu) : S'}
    \end{prooftree}
    \emnote{pas fan de ce parenthésage}
  \end{center}
\end{defi}

On peut maintenant définit les formules de notre HA2 enrichi.

\begin{defi}[Propositions]
  On se donne un ensemble $\mathcal X_2$ de variables du second ordre. Un contexte de typage du second ordre $\Delta$ est une liste de paires $(X,\alpha) \in \mathcal X_2\times \List(\{\bN,\bB,\bL\})$. Une proposition du second ordre est un objet bien typé par les règles suivantes~:
  \begin{center}
    \begin{prooftree}
      \hypo{(X : A_1,A_2,\ldots,A_n)\in\Delta}
      \hypo{\forall i \in \{1,\ldots,n\}, \Gamma\vdash \bt_i : A_i}
      \infer2{\Gamma\mid\Delta\vdash X(\bt_1,\ldots,\bt_n) : \Propo}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\mid\Delta\vdash \varphi : \Propo}
      \hypo{\Gamma\mid\Delta\vdash \psi : \Propo}
      \infer2{\Gamma\mid\Delta\vdash \varphi \land \psi : \Propo}
    \end{prooftree}

    \vspace{0.5cm}
    
    \begin{prooftree}
      \hypo{\Gamma\mid\Delta\vdash \varphi : \Propo}
      \hypo{\Gamma\mid\Delta\vdash \psi : \Propo}
      \infer2{\Gamma\mid\Delta\vdash \varphi \to \psi : \Propo}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma, x : A\mid \Delta\vdash \varphi : \Propo}
      \infer1{\Gamma\mid\Delta\vdash\forall x^A, \varphi : \Propo}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\mid \Delta, X : A_1,\ldots,A_n\vdash \varphi : \Propo}
      \infer1{\Gamma\mid\Delta\vdash \forall X^{A_1,\ldots,A_n}, \varphi : \Propo}
    \end{prooftree}

    \vspace{0.5cm}
    
    \begin{prooftree}
      \hypo{\Gamma\vdash \bt : S}
      \infer1{\Gamma\mid\Delta\vdash S(\bt) : \Propo}
    \end{prooftree}
  \end{center}
\end{defi}

L'idée des prédicats de la forme $S(\bt)$ avec une sorte $S$ est de donner un témoin du fait que le terme est un terme standard.

\begin{nota}
  Pour rendre les propositions plus lisibles, on adoptera la convention que, plutôt que d'écrire $\forall x^A, A(x) \to \varphi$, on notera directement $\forall x^{\{A\}}, \varphi$. Ce procédé de conditionner par $A(x)$ sera appelé ici relativisation. On définit de même la version relativisée de $\exists$.
\end{nota}

\subsubsection{Relation de réalisabilité}

On définit par induction sur la structure des sortes une interprétation ensembliste $\trad S$ (aussi notée $\bS$) pour chaque $S$, et pour chaque $x \in \bS$ un ensemble (possiblement vide) $\ceil x\in \SAT$ de codes de $x$~:
\begin{itemize}
\item $\trad{\Nat} = \bN$, et $\ceil n = \{t\in\Lambda\mid t \reduc^* S^n\;0\}$.
\item $\trad{\Bool} = \{0,1\}$, $\ceil{0} = \{t\in\Lambda\mid t \reduc^*\bff\}$ et $\ceil 1 = \{t\in\Lambda\mid t \reduc^*\btt\}$.
\item $\trad{\List} = \bN^*$, $\ceil{\varepsilon} = \{t\in\Lambda\mid t \reduc^*[\:]\}$ et $\ceil{u\star a} = \{t \in \Lambda\mid \exists t',t'' \in \Lambda, t' \in \ceil a, t'' \in \ceil u, t \reduc^* t' :: t''\}$
\item $\trad{S \to S'} = \{f : \trad S \to \trad{S'}\}$ et $\ceil f = \{t\in \Lambda\mid \forall s \in \trad S, \forall e \in \ceil s, t\;e \in \ceil{f(s)}\}$
\end{itemize}

\begin{lem}
  Pour toute sorte $S$ et $s\in \bS$, $\ceil s \in \SAT$.
\end{lem}

\begin{proof}
  Par induction sur $S$~:
  \begin{itemize}
  \item pour le cas de $\Nat,\Bool,\List$ le résultat est directement dû au fait que la relation $\reduc^* t$ pour $t\in \Lambda$ est close par antéréduction.
  \item si $t\reduc u$ et $u\in \ceil f$, alors pour tout $s\in \bS, e \in \ceil s$ on sait que $t\;e\reduc u\;e \in \ceil{f(s)}$, donc par induction (comme tout ensemble de codes d'élément de $S'$ est saturé) on en déduit que $t\;e \in \ceil{f(s)}$, donc que $t\in \ceil f$, ainsi $\ceil f \in \SAT$.
  \end{itemize}
  D'où la preuve par induction.
\end{proof}

On définit aussi les notions de valuations du premier et du second ordre.

\begin{defi}[Valuation]
  Une valuation du premier ordre est une fonction $\sigma : \mathcal X_1 \to \bigcup_{S \in \Sort}\bS$ partielle. On dit que $\sigma$ est adéquate pour un terme $\bt$ si pour toute variable libre $\bx\in \varlib{\bt}$, le type de $\bx$ correspond au type de $\sigma(\bx)$. Si $\sigma$ est adéquate pour $\bt$ alors on note $\bt^\sigma \in \bigcup_{S\in\Sort}\bS$ la valeur obtenue en substituant les variables libres par les valeurs données par $\sigma$. On considère dans la suite que toutes les valuations du premier ordre considérées sont adéquates. De même $\sigma$ est adéquate pour une proposition $\varphi$ si elle est adéquate pour tout terme apparaissant dans $\varphi$.

  Une valuation du second ordre est une fonction $\rho : \mathcal X_2\to \SAT^{\List(\bigcup \bS)}$. On dit que $\rho$ est adéquate pour une proposition $\varphi$ si pour toute variable libre $X\in\varlib{\varphi}$, l'arité de $\rho(X)$ correspond à celle de $X$.
\end{defi}

\begin{rmk}
  Pour éviter d'écrire trop de définitions, nous laissons celle de $\bt^\sigma$ implicite. Néanmoins, nous devons donner un point important~: la signification de $\rec\;\bt\;\bu\;\bv$. Les fonctions de la forme $\rec_{\mathbb A}\;\bt\;\bu$ sont définies par la propriété universelle de la structure considérée (les entiers naturels, les booléens et les listes). On peut donc considérer qu'on a les axiomes du type $\rec_\bN\;\bt\;\bu\;\bZ = \bt$ puisque les interprétations des deux termes sont égales.
\end{rmk}

On peut maintenant définir la fonction $\trad{-}_\rho^\sigma : \Propo \to \mathcal P(\Lambda)$.

\begin{defi}[Interprétation]
  On définit la fonction par induction, en considérant $\rho$ et $\sigma$ adéquates~:
  \begin{itemize}
  \item $\trad{X(\bt_1,\ldots,\bt_n)}_\rho^\sigma \defeq \rho(X)(t_1^\sigma,\ldots,t_n^\sigma)$
  \item $\trad{\varphi\to\psi}_\rho^\sigma\defeq \{t\in\Lambda\mid \forall u \in \trad{\varphi}_\rho^\sigma, t\;u\in\trad{\psi}_\rho^\sigma\}$
  \item $\trad{\varphi\land \psi}_\rho^\sigma \defeq \{t\in \Lambda\mid \pi_1\;t\in\trad{\varphi}_\rho^\sigma, \pi_2\;t\in\trad{\psi}_\rho^\sigma\}$
  \item $\displaystyle\trad{\forall x^S, \varphi}_\rho^\sigma \defeq \bigcap_{v \in \bS}\trad{\varphi}_\rho^{\sigma[x \leftarrow v]}$
  \item $\displaystyle\trad{\forall X^{S_1,\ldots,S_n}, \varphi}_\rho^\sigma\defeq\bigcap_{F : \prod_i \bS_i \to \SAT}\trad{\varphi}_{\rho[X \leftarrow F]}^\sigma$
  \item $\trad{S(\bt)}_\rho^\sigma \defeq \ceil{\bt^\sigma}$
  \end{itemize}
\end{defi}

\begin{nota}
  On notera $t\reali_\rho^\sigma \varphi$ pour $t\in\trad\varphi_\rho^\sigma$, et $t\reali\varphi$ pour $t\in\trad\varphi_\varnothing^\varnothing$. On appellera alors $t$ un réaliseur de $\varphi$.
\end{nota}

On peut alors voir la différence fondamentale entre $\forall x^A, \varphi$ et $\forall x^{\{A\}}, \varphi$~: le premier signifie simplement qu'il existe un réaliseur de $\varphi$ qui est uniforme pour $x$, c'est-à-dire que ce même réaliseur fonctionne pour toutes les valeurs possibles de $x$ ; le second, lui, indique que l'on possède une façon de calculer $\overline \bt$ en un réaliseur de $\varphi$\emnote{pas sûr de comprendre ce que tu veux dire, plutôt \emph{calculer à partir de $\bar \bt$ un réalisateur de $\varphi(t)$} ?}. Par exemple $\forall x^\bN, x = x$ signifie qu'il existe $t$ qui réalise chaque $n = n$ pour $n\in \bN$, quand $\forall x^{\{\bN\}}, x = x$ signifie qu'il existe une fonction qui à $\overline n$ associe une preuve de $n = n$. Créer cette distinction permet de contrôler le plus finement possible ce qui est du ressort du calcul et ce qui est du ressort de la vérité logique. En particulier, on voit qu'un réaliseur d'une formule de la forme $\forall x^{\{A\}}, \exists y^{\{B\}}, \varphi(x,y)$ calcule directement une fonction $A \to B$ ainsi qu'une preuve de $\varphi(x,f(x))$, là où sans cette relativisation le $y$ dépendant de $x$ n'a pas besoin d'être calculable et peut simplement exister dans la méta-théorie.

Un des lemmes principaux est celui de saturation, qui assure que tous les ensembles ainsi définis restent saturés.

On commence par montrer que $\SAT$ est un treillis complet.

\begin{lem}
  $(\SAT,\subseteq,\Lambda,\varnothing,\bigcap,\bigcup)$ est un treillis complet.
\end{lem}

\begin{proof}
  Il est évident que $S\in \SAT \implies \varnothing\subseteq S \subseteq \Lambda$ et que ces deux parties sont stables par anté-réduction. Supposons que $\{S_i\}_{i\in I}$ est une famille de parties saturées. Si $t\reduc u$ et $u \in \bigcap S_i$, alors pour tout $i \in I$, $t \in S_i$ par saturation de $S_i$, donc $t\in \bigcap S_i$, donc $\bigcap S_i$ est saturée. De même si $u \in \bigcup S_i$, alors on trouve $i$ tel que $u \in S_i$ donc $t\in S_i$ par saturation. Donc $\bigcup S_i$ est saturée. Ainsi $(\SAT,\subseteq,\Lambda,\varnothing, \bigcap,\bigcup)$ est un treillis complet.
\end{proof}

\begin{lem}[Saturation]
  Pour toute proposition $\varphi \in \Propo$ et valuations $\sigma,\rho$ adéquates, $\trad\varphi_\rho^\sigma\in\SAT$.
\end{lem}

\begin{proof}
  On procède par induction sur $\varphi$~:\emnote{rq: noter les cas !}  
  \begin{itemize}
  \item dans le cas de $X(\bt_1,\ldots,\bt_n)$ la définition même de $\trad-_\rho^\sigma$ nous donne un ensemble saturé.
  \item supposons que $\trad{\varphi}_\rho^\sigma\in\SAT$ et $\trad{\psi}_\rho^\sigma\in \SAT$. Soient alors $t,u\in\Lambda$ tels que $t\reduc u$ et $u \reali_\rho^\sigma \varphi \to \psi$. Soit $v\reali_\rho^\sigma\varphi$, par définition on en déduit que $u\;v\reali_\rho^\sigma \psi$ donc par hypothèse d'induction, et comme $t\;v\reduc u\;v$, $t\;v\reali_\rho^\sigma\psi$ donc $t\reali_\rho^\sigma\psi$, d'où $\trad{\varphi\to\psi}_\rho^\sigma \in\SAT$. \emnote{rq: besoin de réduction leftmost}
  \item supposons que $\trad{\varphi_1}_\rho^\sigma$ et $\trad{\varphi_2}_\rho^\sigma$ sont des ensembles saturés, soient alors $t, u \in \Lambda$ tels que $t\reduc u$ et $u \in \trad{\varphi_1\land \varphi_2}_\rho^\sigma$. On sait donc que pour tout $i\in\{1,2\}$, $\pi_i\;u \reali_\rho^\sigma \varphi_i$ et $\pi_i\;t\reduc\pi_i\;u$, donc par hypothèse d'induction et saturation $\pi_i\;t\reali_\rho^\sigma\varphi_i$. Donc $\trad{\varphi_1\land\varphi_2}_\rho^\sigma\in\SAT$.\emnote{rq: avec la réduction des paires en profondeurs, cela repose sur l'aspect ``may''. Et sinon, normalement c'est bon puisqu'entre $t$ et $u$ pas de paires donc la réduction des $\pi$ est bloquée}
  \item si tous les $\trad\varphi_\rho^{\sigma[x\leftarrow v]}$ sont saturés, comme $\SAT$ est un treillis complet, on en déduit directement que $\bigcap \trad\varphi_\rho^{\sigma[x\leftarrow v]}$ est saturé.
  \item de même que précédemment, le fait que $\SAT$ est un treillis complet assure que ce cas passe à l'induction.
  \item un lemme précédent nous dit que $\ceil{t^\sigma}$ est bien saturé.
  \end{itemize}

  Ainsi, par induction, si $t\reduc u$ et $u\reali_\rho^\sigma \varphi$ alors $t\reali_\rho^\sigma\varphi$.
\end{proof}

\subsubsection{Système de types}

On définit un système de typage pour $\Lambda$, dont les types sont des propositions.

\begin{defi}[Typage]
  Nos contextes de typage de termes sont des listes de couples $(x : \varphi)\in\mathcal X_\Lambda\times \Propo$. On définit la relation de typage par les règles suivantes, où les deux premiers contextes servent à typer les variables du premier et second ordre (en particulier on considère que tous les séquents sont bien typés)~:
  \begin{center}
    \begin{prooftree}
      \infer0[Ax]{\Gamma\mid\Delta\mid\Xi,x : \varphi\vdash x : \varphi}
    \end{prooftree}

    \vspace{0.5cm}
    
    \begin{prooftree}
      \hypo{\Gamma\mid\Delta\mid\Xi \vdash t : \varphi}
      \infer1[Aff$_1$]{\Gamma, \bx : A\mid\Delta\mid\Xi \vdash t : \varphi}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\mid\Delta\mid\Xi\vdash t : \varphi}
      \infer1[Aff$_2$]{\Gamma\mid\Delta, X : A_1,\ldots,A_n\mid\Xi\vdash t : \varphi}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\mid\Delta\mid\Xi\vdash t : \varphi}
      \infer1[Aff$_\Lambda$]{\Gamma\mid\Delta\mid\Xi, x : \psi\vdash t : \varphi}
    \end{prooftree}

    \vspace{0.5cm}
    
    \begin{prooftree}
      \hypo{\Gamma\mid\Delta\mid\Xi, x : \varphi \vdash t : \psi}
      \infer1[$\to_\mathrm i$]{\Gamma\mid\Delta\mid\Xi\vdash \lambda x.t : \varphi \to \psi}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\mid\Delta\mid\Xi\vdash t : \varphi \to \psi}
      \hypo{\Gamma\mid\Delta\mid\Xi\vdash u : \varphi}
      \infer2[$\to_\mathrm e$]{\Gamma\mid\Delta\mid\Xi\vdash t\:u : \psi}
    \end{prooftree}

    \vspace{0.5cm}
    
    \begin{prooftree}
      \hypo{\Gamma\mid\Delta\mid\Xi\vdash t : \varphi}
      \hypo{\Gamma\mid\Delta\mid\Xi\vdash u : \psi}
      \infer2[$\land_\mathrm i$]{\Gamma\mid\Delta\mid\Xi\vdash \langle t,u\rangle : \varphi\land \psi}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\mid\Delta\mid\Xi\vdash t : \varphi_1\land\varphi_2}
      \infer1[$\land_\mathrm e^i$]{\Gamma\mid\Delta\mid\Xi\vdash \pi_i\;t : \varphi_i}
    \end{prooftree}

    \vspace{0.5cm}

    \begin{prooftree}
      \hypo{\Gamma, \bx : A\mid\Delta\mid\Xi\vdash t : \varphi}
      \infer1[$\forall^1_\mathrm i$]{\Gamma\mid\Delta\mid\Xi\vdash t : \forall \bx^A, \varphi}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\mid\Delta\mid\Xi\vdash t : \forall \bx^A, \varphi}
      \hypo{\Gamma\vdash \bt : A}
      \infer2[$\forall^1_\mathrm e$]{\Gamma\mid\Delta\mid\Xi\vdash t : \varphi[\bt/\bx]}
    \end{prooftree}
    \emnote{peut-être que le séquent de jugement pour $\bt$ pourrait être annoté pour bien montrer que c'est un jugement du système du 1er ordre}

    \vspace{0.5cm}
    
    \begin{prooftree}
      \hypo{\Gamma\mid\Delta, X^{A_1,\ldots,A_n}\mid\Xi\vdash t : \varphi}
      \infer1[$\forall^2_\mathrm i$]{\Gamma\mid\Delta\mid\Xi\vdash t : \forall X^{A_1,\ldots,A_n}, \varphi}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\mid\Delta\mid\Xi\vdash t : \forall X^{A_1,\ldots,A_n}, \varphi}
      \hypo{\Gamma, \bx_1 : A_1,\ldots,\bx_n : A_n\mid\Delta\vdash \psi : \Propo}
      \infer2[$\forall^2_\mathrm e$]{\Gamma\mid\Delta\mid\Xi\vdash t : \varphi[\psi(\bx_1,\ldots,\bx_n)/X]}
    \end{prooftree}

    \vspace{0.5cm}

    \begin{prooftree}
      \infer0[$\bN_\mathrm i^0$]{\Gamma\mid\Delta\mid\Xi\vdash 0 : \Nat(\bZ)}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\mid\Delta\mid\Xi\vdash t : \Nat(\bt)}
      \infer1[$\bN_\mathrm i^S$]{\Gamma\mid\Delta\mid\Xi\vdash S\;t : \Nat(\bfS\:\bt)}
    \end{prooftree}

    \vspace{0.5cm}
    
    \begin{prooftree}
      \hypo{\Gamma\mid\Delta\mid\Xi\vdash t : S(\bt)}
      \hypo{\Gamma\mid\Delta\mid\Xi\vdash u : (\Nat\to S \to S)(\bu)}
      \hypo{\Gamma\mid\Delta\mid\Xi\vdash v : \Nat(\bv)}
      \infer3[$\bN_\mathrm e$]{\Gamma\mid\Delta\mid\Xi\vdash \rec_\bN\;t\;u\;v : S(\rec_{\Nat}\;\bt\;\bu\;\bv)}
    \end{prooftree}

    \vspace{0.5cm}
    
    \begin{prooftree}
      \infer0[$\bB_\mathrm i ^{\btt}$]{\Gamma\mid\Delta\mid\Xi\vdash \btt : \Bool(\bbtt)}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \infer0[$\bB_\mathrm i^{\bff}$]{\Gamma\mid\Delta\mid\Xi\vdash \bff : \Bool(\bbff)}
    \end{prooftree}

    \vspace{0.5cm}
    
    \begin{prooftree}
      \hypo{\Gamma\mid\Delta\mid\Xi\vdash t : \Bool(\bt)}
      \hypo{\Gamma\mid\Delta\mid\Xi\vdash u : S(\bu)}
      \hypo{\Gamma\mid\Delta\mid\Xi\vdash v : S(\bv)}
      \infer3[$\bB_\mathrm e$]{\Gamma\mid\Delta\mid\Xi\vdash \rec_\bB\;t\;u\;v : S(\rec_{\Bool}\;\bt\;\bu\;\bv)}
    \end{prooftree}

    \vspace{0.5cm}
    
    \begin{prooftree}
      \infer0[$\bL_\mathrm i^{[\;]}$]{\Gamma\mid\Delta\mid\Xi\vdash [\:] : \List(\bnil)}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\mid\Delta\mid\Xi\vdash t : \Nat(\bt)}
      \hypo{\Gamma\mid\Delta\mid\Xi\vdash u : \List(\bu)}
      \infer2[$\bL_\mathrm i^{::}$]{\Gamma\mid\Delta\mid\Xi\vdash t :: u : \List(\bt \bcons \bu)}
    \end{prooftree}

    \vspace{0.5cm}
    
    \begin{prooftree}
      \hypo{\Gamma\mid\Delta\mid\Xi\vdash t : S(\bt)}
      \hypo{\Gamma\mid\Delta\mid\Xi\vdash u : (\Nat\to \List \to S \to S)(\bu)}
      \hypo{\Gamma\mid\Delta\mid\Xi\vdash v : \List(\bv)}
      \infer3[$\bL_\mathrm e$]{\Gamma\mid\Delta\mid\Xi\vdash \rec_\bL\;t\;u\;v : S(\rec_{\List}\;\bt\;\bu\;\bv)}
    \end{prooftree}

    \vspace{0.5cm}

    \begin{prooftree}
      \hypo{\Gamma, \bx : S\mid\Delta\mid\Xi, x : S(\bx)\vdash t : S'(\bt)}
      \infer1[$\blam_\mathrm i$]{\Gamma\mid\Delta\mid\Xi\vdash \lambda x.t : (S\to S')(\blam \bx. \bt)}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\mid\Delta\mid\Xi\vdash t : (S\to S')(\bt)}
      \hypo{\Gamma\mid\Delta\mid\Xi\vdash u : S(\bu)}
      \infer2[$\blam_\mathrm e$]{\Gamma\mid\Delta\mid\Xi\vdash t\;u : S'(\bt(\bu))}
    \end{prooftree}
  \end{center}
\end{defi}
\emnote{pourquoi restreindre le type des récurseurs à des sortes ? On peut aussi présenter les récurseurs non appliqués directement avec leur type au 2nd-ordre je pense :
$\rec_\bN : \forall Z.(Z(0) \to \forall x^{\{\bN\}}.(Z(x)\to Z(x+1)) \to
\forall x^{\{\bN\}}.Z(x)$. Sinon j'ai peur qu'on n'ait pas de principe d'induction au niveau logique. De façon générale, je ne sais pas si on a besoin d'inclure les prédicats $...\vdash t:S(\bt)$ dans le système de type (ce qu'on discutait au tableau), je pense que des règles $...\vdash t:S$ suffisent, ce qui simplifie pas mal (et correspondrait plus ou moins à une sorte de règle ``d'injection'' (à mieux penser sûrement) :
}
\textcolor{blue}{
\begin{prooftree}
 \hypo{\Gamma \vdash t :S}
 \infer1{\Gamma \mid \Delta \mid \Xi \vdash t:S}
\end{prooftree}
}

On définit une notion de valuation adaptée au lemme d'adéquation.
\emnote{substitution (au lieu de valuation), interne au langage de termes}
\begin{defi}[Valuation de réalisabilité]
  Soit un contexte de typage du premire ordre $\Gamma$, un contexte de typage du second ordre $\Delta$ et un contexte de typage de termes $\Xi = (x_i : \varphi_i)_{i\in I}$. On dit que des valuations $\sigma, \rho, \nu$ où $\nu : \mathcal X_\Lambda \to \Lambda$ sont adéquates pour $\Gamma,\Delta,\Xi$, ce que l'on note $\nu\reali_\rho^\sigma \Gamma,\Delta,\Xi$, si $\sigma$ est adéquate pour $\Gamma$, $\rho$ est adéquate pour $\Delta$ et si
  \[\forall i \in I, \text{\sout{$\sigma(x_i)$}\emnote{$\nu(x_i)$} }\reali_\rho^\sigma\varphi_i\]
\end{defi}

On définit aussi la substitution simultanée $t^\nu$ en remplaçant directement chaque variable libre $x$ de $t$ par $\nu(x)$ dans $t$.

On a besoin de deux lemmes de substitutions, pour le premier et le second ordre.

\begin{lem}
  Pour tous $\sigma,\rho$, $\bt : A$, $t\in\Lambda$, $\varphi : \Propo$, on a
  \[t\reali_\rho^{\sigma[\bx \leftarrow \bt^\sigma]} \varphi\iff t\reali_\rho^\sigma \varphi[\bt/\bx]\]
\end{lem}

\begin{proof}
  Tout d'abord, par une induction sur $\bu$ (que nous ne détaillerons pas), on peut quitte à renommer nos variables considérer que $\forall \bu, \bu^{\sigma[\bx\leftarrow \bt^\sigma]} = (\bu[\bt/\bx])^\sigma$. On raisonne maintenant par induction sur $\varphi$~:
  \begin{itemize}
  \item si $\varphi = X(\bt_1,\ldots,\bt_n)$ alors
    \begin{align*}
      \trad\varphi_\rho^{\sigma[\bx\leftarrow \bt^\sigma]} &= \rho(X)(\bt_1^{\sigma[\bx\leftarrow \bt^\sigma]},\ldots,\bt_n^{\sigma[\bx\leftarrow \bt^\sigma]})\\
      &= \rho(X)((\bt_1[\bt/\bx])^\sigma,\ldots,(\bt_n[\bt/\bx])^\sigma)\\
      &= \trad{\varphi[\bt/\bx]}_\rho^\sigma
    \end{align*}
  \item si $\varphi = \psi \to \chi$ alors
    \begin{align*}
      \trad{\psi\to\chi}_\rho^{\sigma[\bx\leftarrow\bt^\sigma]} &= \{t\in\Lambda\mid \forall u\in\trad\psi_\rho^{\sigma[\bx\leftarrow\bt^\sigma]}, t\;u \in \trad\chi_\rho^{\sigma[\bx\leftarrow\bt^\sigma]}\}\\
      &= \{t\in\Lambda\mid\forall u\in \trad{\psi[\bt/\bx]}_\rho^\sigma, t\;u\in\trad{\chi[\bt/\bx]}_\rho^\sigma\}\\
      &= \trad{\psi[\bt/\bx]\to\chi[\bt/\bx]}_\rho^\sigma\\
      &= \trad{(\psi\to\chi)[\bt/\bx]}_\rho^\sigma
    \end{align*}
  \item si $\varphi = \psi_1\land \psi_2$ alors
    \begin{align*}
      \trad{\psi_1\land\psi_2}_\rho^{\sigma[\bx\leftarrow\bt^\sigma]} &= \{t\in\Lambda\mid \forall i \in\{1,2\}, \pi_i\;t\in \trad{\psi_i}_\rho^{\sigma[\bt\leftarrow\bt^\sigma]}\}\\
      &= \{t\in\Lambda\mid\forall i \in \{1,2\}, \pi_i\;t\in\trad{\psi_i[\bt/\bx]}_\rho^\sigma\}\\
      &= \trad{(\psi\land\chi)[\bt/\bx]}_\rho^\sigma
    \end{align*}
  \item si $\varphi = \forall \by^S, \psi$ alors quitte à renommer $\by$ pour s'assurer que la variable n'appartienne pas aux variables libres de $\bt$~:
    \begin{align*}
      \trad{\forall \by^S,\psi}_\rho^{\sigma[\bx\leftarrow\bt^\sigma]} &= \bigcap_{s\in \bS}\trad{\psi}_\rho^{\sigma[\bx\leftarrow\bt^\sigma,\by\leftarrow s]}\\
      &= \bigcap_{s\in\bS}\trad\psi_\rho^{\sigma[\by\leftarrow s, \bx\leftarrow \bt^\sigma]}\\
      &= \bigcap_{s\in\bS}\trad{\psi[\bt/\bx]}_\rho^{\sigma[\by\leftarrow s]}\\
      &= \trad{\forall \bx, \psi[\bt/\bx]}_\rho^\sigma\\
      &= \trad{(\forall \bx, \psi)[\bt/\bx]}_\rho^\sigma
    \end{align*}
  \item si $\varphi = \forall X^{S_1,\ldots,S_n}, \psi$ alors
    \begin{align*}
      \trad{\forall X^{S_1,\ldots,S_n}, \psi}_\rho^{\sigma[\bx\leftarrow \bt^\sigma]} &= \bigcap_{F : \bS_1\times\cdots\times \bS_n\to \SAT}\trad{\psi}_{\rho[X\leftarrow F]}^{\sigma[\bx\leftarrow \bt^\sigma]}\\
      &= \bigcap_{F : \bS_1\times\cdots\times \bS_n \to \SAT}\trad{\psi[\bt/\bx]}_{\rho[X\leftarrow F]}^\sigma\\
      &= \trad{(\forall X^{S_1,\ldots,S_n}, \psi)[\bt/\bx]}_\rho^\sigma
    \end{align*}
  \item si $\varphi = S(\bu)$, alors $\trad{S(\bu)}_\rho^{\sigma[\bx\leftarrow \bt^\sigma]} = \ceil{\bu^{\sigma[\bx\leftarrow\bt^\sigma]}} = \ceil{(\bu[\bt/\bx])^\sigma}$
  \end{itemize}
  Donc, par induction, $\trad\varphi_\rho^{\sigma[\bx\leftarrow\bt]} = \trad{\varphi[\bt/\bx]}_\rho^\sigma$, d'où le résultat.
\end{proof}

\begin{lem}
  Pour tous $\Gamma,\Delta$, si $\Gamma,\bx_1 : S_1,\ldots,\bx_n : S_n\mid\Delta\vdash \psi : \Propo$ alors pour tous $\rho,\sigma$ bien choisis \emnote{cad ?} on a, en notant $S : (a_1,\ldots,a_n) \mapsto \trad\psi_\rho^{\sigma[\bx_1\leftarrow a_1,\ldots,\bx_n\leftarrow a_n]}$~:
  \[t\reali_{\rho[X\leftarrow S]}^\sigma \varphi \iff t\reali_\rho^\sigma \varphi[\psi(\bx_1,\ldots,\bx_n)/X]\]
\end{lem}

\begin{proof}
  Par induction sur $\varphi$~:
  \begin{itemize}
  \item si $\varphi = X(\bt_1,\ldots,\bt_n)$, alors
    \begin{align*}
      \trad\varphi_{\rho[X\leftarrow S]}^\sigma &= S(\bt_1^\sigma,\ldots,\bt_n^\sigma) \\
      &= \trad\psi_\rho^{\sigma[\bx_1\leftarrow \bt_1^\sigma,\ldots,\bx_n\leftarrow \bt_n^\sigma]} \\
      &= \trad{\psi[\bt_1/\bx_1,\ldots,\bt_n/\bx_n]}_\rho^\sigma\\
      &= \trad{X(\bt_1,\ldots,\bt_n)[\psi(\bx_1,\ldots,\bx_n)/X]}_\rho^\sigma
    \end{align*}
  \item si $\varphi = Y(\bt_1,\ldots,\bt_p)$ où $X\neq Y$ alors $\trad{\varphi}_{\rho[X\leftarrow S]}^\sigma = \trad\varphi_\rho^\sigma = \trad{\varphi[\psi(\bx_1,\ldots,\bx_n)/X]}_\rho^\sigma$.
  \item si $\varphi = \chi \to \chi'$ alors
    \begin{align*}
      \trad{\chi\to\chi'}_{\rho[X\leftarrow S]}^\sigma &= \{t\in\Lambda\mid \forall u \in \trad\chi_{\rho[X\leftarrow S]}^\sigma, t\;u\in \trad{\chi'}_{\rho[X\leftarrow S]}^\sigma\}\\
      &= \{t\in\Lambda\mid \forall u \in \trad{\chi[\psi(\bx_1,\ldots,\bx_n)/X]}_\rho^\sigma, t\;u\in\trad{\chi'[\psi(\bx_1,\ldots,\bx_n)/X]}_\rho^\sigma\}\\
      &= \trad{\chi[\psi(\bx_1,\ldots,\bx_n)/X] \to \chi'[\psi(\bx_1,\ldots,\bx_n)/X]}_\rho^\sigma\\
      &= \trad{(\chi\to\chi')[\psi(\bx_1,\ldots,\bx_n)/X]}_\rho^\sigma
    \end{align*}
  \item si $\varphi = \chi_1\land\chi_2$ alors
    \begin{align*}
      \trad{\chi\land\chi'}_{\rho[X\leftarrow S]}^\sigma &= \{t\in\Lambda\mid \forall i \in \trad\{1,2\}, \pi_i\;t\in \trad{\chi_i}_{\rho[X\leftarrow S]}^\sigma\}\\
      &= \{t\in\Lambda\mid \forall i \in \{1,2\}, \pi_i\;t\in\trad{\chi_i[\psi(\bx_1,\ldots,\bx_n)/X]}_\rho^\sigma\}\\
      &= \trad{\chi_1[\psi(\bx_1,\ldots,\bx_n)/X] \land \chi_2[\psi(\bx_1,\ldots,\bx_n)/X]}_\rho^\sigma\\
      &= \trad{(\chi_1\to\chi_2)[\psi(\bx_1,\ldots,\bx_n)/X]}_\rho^\sigma
    \end{align*}
  \item si $\varphi = \forall \bx^S, \chi$ alors
    \begin{align*}
      \trad{\forall \bx^S, \chi}_{\rho[X\leftarrow S]}^\sigma &= \bigcap_{s\in\bS}\trad\chi_{\rho[X\leftarrow S]}^{\sigma[\bx\leftarrow s]}\\
      &= \bigcap_{s\in\bS}\trad{\chi/[\psi(\bx_1,\ldots,\bx_n)/X]}_\rho^{\sigma[\bx\leftarrow s]}\\
      &= \trad{\forall \bx^S, \chi[\psi(\bx_1,\ldots,\bx_n)/S]}_\rho^\sigma\\
      &= \trad{(\forall \bx^S, \chi)[\psi(\bx_1,\ldots,\bx_n)/S]}_\rho^\sigma
    \end{align*}
  \item si $\varphi = \forall Y^{S_1,\ldots,S_p},\chi$ alors
    \begin{align*}
      \trad{\forall Y^{S_1,\ldots,S_p}, \chi}_{\rho[X\leftarrow S]}^\sigma &= \bigcap_{F : \bS_1\times\cdots\times\bS_p \to \SAT}\trad\chi_{\rho[X\leftarrow S, Y\leftarrow F]}^\sigma\\
      &= \bigcap_{F : \bS_1\times\cdots\times\bS_p}\trad\chi_{\rho[Y\leftarrow F, X\leftarrow S]}^\sigma\\
      &= \bigcap_{F : \bS_1\times\cdots\times\bS_p}\trad{\chi[\psi(\bx_1,\ldots,\bx_n)/X]}_{\rho[Y\leftarrow F]}^\sigma\\
      &= \trad{\forall Y^{S_1,\ldots,S_n}, \chi[\psi(\bx_1,\ldots,\bx_n)/X]}_\rho^\sigma\\
      &= \trad{(\forall Y^{S_1,\ldots,S_n}, \chi)[\psi(\bx_1,\ldots,\bx_n)/X]}_\rho^\sigma
    \end{align*}
  \item si $\varphi = S(\bt)$ alors $\varphi[\psi(\bx_1,\ldots,\bx_n)/X] = \varphi$ d'où le résultat.
  \end{itemize}
  Donc, par induction, $\trad\varphi_{\rho[X\leftarrow S]}^\sigma = \trad{\varphi[\psi(\bx_1,\ldots,\bx_n)/X]}_\rho^\sigma$.
\end{proof}

\begin{lem}[Adéquation]
  Soient des contextes $\Gamma,\Delta,\Xi$ et des valuations $\nu\reali_\rho^\sigma \Gamma,\Delta,\Xi$. Soit $t\in\Lambda$ et $\varphi\in\Propo$ tels que $\Gamma\mid\Delta\mid\Xi\vdash t : \varphi$. Alors $t^\nu \reali_\rho^\sigma \varphi$.
\end{lem}

\begin{proof}
  La preuve se fait par induction sur la relation de typage $\vdash$~:\emnote{rq: utiliser les noms des règles de typage pour référencer chaque cas}
  \begin{itemize}
  \item Pour le cas d'une variable, il est clair que si $\varphi = \varphi_i$ et $x = x_i$ alors $t^\nu = \nu(x_i)\reali_\rho^\sigma \varphi_i$.
  \item Supposons que pour tout $\nu \reali_\rho^\sigma \Gamma,\Delta,(\Xi, x : \varphi)$, on a $t^\nu \reali_\rho^\sigma \psi$. Soit $\nu \reali_\rho^\sigma \Gamma,\Delta,\Xi$, montrons que $(\lambda x.t)^\nu\reali_\rho^\sigma \varphi \to \psi$. Soit $u \reali_\rho^\sigma \varphi$, alors $t^{\nu[x \leftarrow u]}\reali_\rho^\sigma \psi$ puisque $\nu[x\leftarrow u]\reali_\rho^\sigma \Gamma,\Delta,(\Xi, x : \psi)$. De plus, quitte à renommer, on a $t^{\nu[x\leftarrow u]} = t^\nu[u/x]$ donc $t^\nu[u/x]\reali_\rho^\sigma \psi$, donc par saturation $(\lambda x.t^\nu) u \reali_\rho^\sigma \psi$, donc \emnote{$(\lambda x.t)^\nu)=_\alpha$}$\lambda x.t^\nu\reali_\rho^\sigma \varphi\to\psi$. \emnote{caché : may-réduction}
  \item Supposons que $t^\nu\reali_\rho^\sigma \varphi \to \psi$ et $u^\nu\reali_\rho^\sigma \varphi$, alors par définition $\emnote{t^\nu\;u^\nu=}(t\;u)^\nu\reali_\rho^\sigma \psi$.
  \item Supposons que $t^\nu\reali_\rho^\sigma \varphi$ et $u^\nu\reali_\rho^\sigma \psi$, alors $\pi_1\;\langle t,u\rangle^\nu \reduc t^\nu \reali_\rho^\sigma \varphi$ et $\pi_2\;\langle t,u\rangle^\nu\reduc u^\nu \reali_\rho^\sigma \psi$, donc $\langle t,u\rangle^\nu \reali_\rho^\sigma \varphi\land\psi$.\emnote{caché : may-réduction}
  \item Si $t^\nu \reali_\rho^\sigma \varphi_1\land\varphi_2$ alors par définition, $\pi_i\;t^\nu\reali_\rho^\sigma \varphi_i$.
  \item On suppose que pour tout $\nu\reali_\rho^\sigma (\Gamma, \bx : A), \Delta,\Xi$, $t^\nu\reali_\rho^\sigma \varphi$. Soit alors des valuations $\nu\reali_\rho^\sigma \Gamma,\Delta,\Xi$, alors par hypothèse pour tout $a \in \mathbb A$, $\nu\reali_\rho^{\sigma[\bx\leftarrow a]}(\Gamma, \bx : A), \Delta, \Xi$ donc $t^\nu\reali_\rho^{\sigma[\bx \leftarrow a]}\varphi$ pour tout $a\in \mathbb A$, ce qui signifie que $t^\nu \reali_\rho^\sigma \forall \bx^A, \varphi$.
  \item On suppose que $t^\nu\reali_\rho^\sigma \forall \bx, \varphi$, donc pour tout $\emnote{\Gamma \vdash ?}\bt : A$, $\bt^\sigma \in \mathbb A$ donc \emnote{inverser $\rho/\sigma$ ?}$t^\nu\reali_{\rho[\bx\leftarrow \bt^\sigma]}^\sigma \varphi$, c'est-à-dire $t^\nu\reali_\rho^\sigma \varphi[\bt/\bx]$ par le lemme de substitution du premier ordre.
  \item On suppose que pour tout $\nu\reali_\rho^\sigma \Gamma, (\Delta, X : A_1,\ldots,A_n), \Xi$, $t^\nu \reali_\rho^\sigma \varphi$. Soit alors des valuations $\nu\reali_\rho^\sigma\Gamma, \Delta,\Xi$. On voit que pour tout $S : A_1\times\ldots\times A_n \to \SAT$, $\nu\reali_{\rho[X\leftarrow S]}^\sigma \Gamma, (\Delta, X : A_1,\ldots,A_n),\Xi$ donc $t^\nu\reali_{\rho[X\leftarrow S]}^\sigma \varphi$. Comme cela tient pour tout $S : A_1\times\ldots\times A_n \to \SAT$, on en déduit que $t^\nu\reali_\rho^\sigma \forall X, \varphi$. 
  \item Si $t^\nu\reali_\rho^\sigma \forall X, \varphi$ et $\Gamma,x_1 : A_1,\ldots,x_n : A_n\mid\Delta\vdash \psi : \Propo$, alors en particulier $S : (a_1,\ldots,a_n) \mapsto \trad\psi_\rho^{\sigma[x_1\leftarrow a_1,\ldots, x_n \leftarrow a_n]}$ définit une fonction $A_1\times\ldots\times A_n \to\SAT$\emnote{ref lemme sur valeur de vérité dans SAT}, donc $t^\nu\reali_{\rho[X\leftarrow S]}^\sigma \varphi$, c'est-à-dire $t^\nu\reali_\rho^\sigma \varphi[\psi(\bx_1,\ldots,\bx_n)/X]$ par le lemme de substitution du second ordre.\emnote{ici je pense que si on veut être précis précis, comme c'est la fonction $x_1,...x_n\mapsto\varphi(x_1,...x_n)$ qui substitue $X$ (dans la règle de typage), soit il faudrait écrire  quelque chose comme $\varphi(\bt_1,...\bt_n)/X(\bt_1,...,\bt_n)$, soit $\varphi/X$ en voyant $\varphi$ comme la fonction n-aire}
  \item Comme $0\reduc^* 0$ et $0\in\ceil\bZ$, on en déduit le résultat.
  \item Si $t\in\ceil{\bt^\sigma}$ où $\bt^\sigma \in \bN$, alors $t\reduc^* \overline{\bt^\sigma}$ donc $S\;t\reduc^*\overline{\bfS\;\bt^\sigma}$, donc $S\;t\in\ceil{\bfS\;\bt^\sigma}$.
  \item Supposons que $t^\nu\in\ceil{\bt^\sigma}$, $u^\nu\in\ceil{\bu^\sigma}$ et $v^\nu\in\ceil{\bv^\sigma}$. Comme $\bv^\sigma\in\bN$, on raisonne par induction sur $\bv^\sigma$~:
    \begin{itemize}
    \item si $\bv^\sigma = 0$, alors comme la fonction $\rec_\bN\;\bt\;\bu$ vaut $\bt$ en $0$, $(\rec_\bN\;\bt\;\bu\;\bv)^\sigma = \bt^\sigma$ et $v^\nu \reduc^* 0$, donc \emnote{je ne suis pas sûr de comprendre la prémisse, tu as donné une réduction sur $\rec_\bN$ qui ne dépend pas de valeur de fonctions méta dans $\bN$, non ? en fait je crois que j'ai compris ce que tu veux dire, cela a à voir avec ma remarque sur le typage des récurseurs.} $(\rec_\bN\;t\;u\;v)^\nu \reduc^* t^\nu \in \ceil{\bt^\sigma}$, donc $\rec_\bN\;t\;u\;v \in \ceil{(\rec_\bN\;\bt\;\bu\;\bv)^\sigma}$. \emnote{may-réduction cachée (par ex, $u=\Omega$)}
    \item si $\bv^\sigma = n + 1$, comme on sait que la fonction $\rec_\bN\;\bt\;\bu$ vaut $\bu(n)(\rec_\bN\;\bt\;\bu\;n)$ en $n+1$, on en déduit \emnote{même remarque} que $(\rec_\bN\;\bt\;\bu\;\bv)^\sigma = \bu^\sigma\;n\;(\rec_\bN\;\bt^\sigma\;\bu^\sigma\;n)$. De plus, $v^\nu \reduc^* S\;n$, donc
      \begin{align*}
        (\rec_\bN\;t\;u\;v)^\sigma &\reduc^* \rec_\bN\;t^\nu\;u^\nu\;(S\;n)\\
        &\reduc u^\nu\;n\;(\rec_\bN\;t^\nu\;u^\nu\;n)
      \end{align*}
      or on sait que $\rec_\bN\;t^\nu\;u^\nu\;n\in\ceil{\rec_\bN\;\bt^\sigma\;\bu^\sigma\;\overline n}$ par hypothèse d'induction. Comme $u^\nu \in \ceil{\bu^\sigma}$ et en considérant le type de $\bu^\sigma$, on en déduit que $(\rec_\bN\;t\;u\;(S\;n))^\nu \in \ceil{\rec_\bN\;\bt^\sigma\;\bu^\sigma\;(\overline{n+1})}$, donc par saturation, $\rec_\bN\;t\;u\;v\in\ceil{\rec_\bN\;\bt^\sigma\;\bu^\sigma\;\overline{n+1}}$.\emnote{may-réduction cachée, paire critique sur $\rec_\bN\;\_\;\_\;S v$}
    \end{itemize}
    Ainsi, par induction, $\rec_\bN\;t^\nu\;u^\nu\;v^\nu \in \ceil{\rec_\bN\;\bt^\sigma\;\bu^\sigma\;\bv^\sigma}$.
  \item les cas de $\bB$ et $\bL$ sont analogues.
  \item supposons que $t^\nu\in\ceil{S'(\bt^\sigma)}$ pour $\nu\reali_\rho^\sigma (\Gamma, \bx : S), \Delta,(\Xi, x : S(\bt))$. Soit alors $s\in\bS$ et $e\in \ceil s$. On sait que $t^{\nu[x\leftarrow e]}\in\ceil{S'(\bt^{\sigma[\bx\leftarrow s]})}$, donc $(\lambda x.t)^\nu e \in \ceil{S'(\bt^{\sigma[x\leftarrow s]})}$ par saturation, donc $\lambda x.t\in\ceil{(S\to S')(\blam x.\bt)}$.
  \item supposons que $t^\nu \in\ceil{(S\to S')(\bt^\sigma)}$ et $u^\nu \in\ceil{S(\bu^\sigma)}$, alors par définition de $\trad{S\to S'}$ on en déduit que $(t\;u)^\nu\in\ceil{S'(\bt(\bu))^\sigma}$.
  \end{itemize}

  On en déduit par induction le résultat.
\end{proof}

En particulier, si $\vdash t : \varphi$ alors $t\reali \varphi$.

\subsection{Traiter les arbres}

On peut désormais étudier les notions liées aux arbres. On donne d'abord deux fonctions utilitaires liées aux listes et aux fonctions~:

\begin{defi}[Longueur, préfixe]
  On définit les deux fonctions suivantes au premier ordre, où $\ell : \List$ et $f : \Nat\to\Nat$~:
  \begin{align*}
    |\ell| &\defeq \rec_{\List}\;\bZ\;(\blam \bx.\blam\by.\blam\bz.\bfS\;\bz)\;\ell\\
    f_n &\defeq \rec_{\Nat}\;\bnil\;(\blam \bn.\blam \bx.(f(\bn))\bcons \bx)\;n
  \end{align*}
\end{defi}

\begin{rmk}
  En utilisant l'adéquation, on peut directement voir que ces deux fonctions possèdent un code. On définit $\Gamma_0\mid\Xi_0 = \ell : \List\mid\varnothing\mid l : \List(\ell)$ et $\Gamma_1\mid\Xi_1 = \ell : \List, \bx : \Nat, \by : \List, \bz : \Nat\mid \varnothing\mid l : \List(\ell), x : \Nat(\bx), y : \Nat(\by), z : \Nat(\bz)$~:
  \begin{center}
    \begin{prooftree}
      \infer0[$\bN_\mathrm i^0$]{\Gamma_0\mid\Xi_0\vdash 0 : \Nat(\bZ)}
      \infer0[Ax]{\Gamma_1\mid\Xi_1\vdash \Nat(\bz)}
      \infer1[$\bN_\mathrm i^S$]{\Gamma_1\mid\Xi_1\vdash S\;z : \Nat(\bfS\;\bz)}
      \infer1[$(\blam_\mathrm i)^3$]{\Gamma_0\mid\Xi_0\vdash \lambda x\; y\;z.S\;z : (\Nat\to \List\to\Nat\to\Nat)(\blam \bx\;\by\;\bz.\bfS\;\bz)}
      \infer0[Ax]{\Gamma_0\mid\Xi_0\vdash \List(\ell)}
      \infer3{\Gamma_0\mid\Xi_0\vdash \rec_\bL\;0\;(\lambda x.\lambda y.\lambda z.S\;z)\;l : \Nat(|\ell|)}
      \infer1[$\blam_\mathrm i$]{\vdash \lambda l.\rec_\bL\;0\;(\lambda x\;y\;z.S\;z)\;l : (\List\to\Nat)(\lambda \ell.|\ell|)}
    \end{prooftree}
  \end{center}
\end{rmk}

Avec ces termes, on peut déjà définir les deux théorèmes que l'on veut étudier~:
\begin{equation}
  \KL \defeq \forall T^{\List}, (\forall n^{\{\Nat\}}, \exists \ell^{\{\List\}}, |\ell| = n \land T(\ell)) \implies \exists \alpha^{\{\Nat\to\Nat\}}, \forall n^{\{\Nat\}}, T(\alpha_n)
\end{equation}
\begin{equation}
  \FT \defeq \forall T^{\List}, (\exists f^{\{(\Nat\to\Nat)\to\Nat\}}, \forall \alpha^{\{\Nat \to \Nat\}}, T(\alpha_{f(\alpha)})) \implies \exists n^{\{\Nat\}}, \forall \alpha^{\{\Nat \to \Nat\}}, T(\alpha_n)
\end{equation}

\end{document}
