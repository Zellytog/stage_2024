\documentclass{article}
\usepackage{prelude}

\title{Choix, réalisabilité et evidenced frames}

\author{Titouan Leclercq}

\date{April 15$^{\mathrm{th}}$ -- July 15$^{\mathrm{th}}$}

\begin{document}

\maketitle

\tableofcontents

\section{Introduction}

Les mathématiques usuelles se déroulent dans $\ZFC$. Dans cette pratique des mathématiques, l'objectif est de démontrer un maximum de résultats, sans considération pour les moyens logiques employés (sous réserve que ceux-ci soient cohérents). Le cadre de ce rapport, celui des mathématiques à rebours, cherche au contraire à étudier la force logique de résultats déjà démontrés~: quels principes doit-on admettre pour les démontrer ?

Cette question est renouvelée dans le cadre de la logique intuitionniste et des mathématiques constructivistes (qui n'acceptent que les procédés démonstratifs explicites), puisque l'on peut étudier le caractère constructif des principes en plus de leur force logique. Dans cette étude, une famille de formules a un rôle privilégié : les principes de choix. Leur forme générale est
\[\forall x^A, \exists y^B, R(x,y) \implies \exists f^{A\to B}, \forall x^A, R(x,f(x))\]
Où $R$ est une relation binaire entre $A$ et $B$. Suivant le choix de $A,B$ et des formes que peut prendre $R$, on obtient de nombreux principes de choix. On peut, de plus, étudier une variante de ces principes en en prenant la contraposée~: en logique intuitionniste, ces ``principes de co-choix'' sont plus faibles, souvent strictement.

Dans ce rapport, nous étudierons l'un des principes les plus faibles~: le lemme de König et sa contraposée, le Fan theorem. Sans rentrer dans les détails, en considérant un arbre comme une partie de $\bN^*$ close par préfixe (son complémentaire est donc une partie de $\bN^*$ close par extension), on peut énoncer le lemme de König et le Fan theorem de la façon suivante~:
\[\KL \defeq (\forall n^{\bN}, \exists u^{\bN^*}, |p| = n \land p \in T) \implies \exists \alpha^{\bN\to\bN}, \forall n^{\bN}, \alpha_0\ldots\alpha_{n-1}\in T\]
\[\FT \defeq (\forall \alpha^{\bN\to\bN}, \exists n^\bN, \alpha_0\ldots \alpha_{n-1}\in C)\implies \exists n^\bN, \forall \alpha^{\bN\to\bN}\]
L'écriture $\exists p^{\bN^*},|p| = n \land p \in T$ est équivalente à $\exists \alpha^{\bN\to\bN}, \alpha_0\ldots\alpha_{n-1}\in T$, mais plus naturelle à considérer. On suppose ici que $T$ est clos par préfixe et $C$ par extension, nous donnant donc que $\FT$ est la contraposée de $\KL$.

Mentionnons que $\KL$ comme $\FT$ sont toujours vrais dans $\ZFC$, et même dans $\ZF$~: on travaille donc dans une théorie plus faible dans laquelle on peut étudier l'ajout de $\FT$ ou $\KL$ comme non trivial. La théorie habituellement utilisée en mathématiques à rebours est un fragment faible de l'arithmétique du second ordre nommée $\RCAO$. Cette théorie est très expressive car elle permet de décrire les réels ainsi que les fonctions continues voire mesurables. Cependant, cette expressivité se fait au coût d'une grande quantité de codages, qui nous intéressent peu. Nous faisons donc le choix de travailler dans une version de l'arithmétique dans laquelle on peut parler de façon interne de fonctions, de paires et d'autres outils primitifs en plus des entiers, le tout avec une logique du second ordre.

Maintenant que le contexte est plus clair, nous pouvons décrire l'objectif du stage présenté dans ce rapport~: il s'agit de mesurer le contenu logique de $\FT$, en le séparant entre autre de $\KL$. Pour ce faire, nous utilisons la réalisabilité, qui est un outil reliant les langages de programmation et la logique.

Partant d'un langage simple (le $\lambda$-calcul), nous allons construire un modèle de réalisabilité vérifiant Fan theorem mais invalidant le lemme de König (sous une forme faible). La preuve que ce modèle vérifie $\FT$ nous servira ensuite à chercher une présentation plus générale de l'argument qu'une forme de continuité des fonctions calculables implique $\FT$.

\section{Construire un modèle de réalisabilité}

\subsection{L'interprétation BHK}

L'interprétation de Brouwer-Heying-Kolmogorov (BHK) désigne une interprétation de la sémantique d'une propositions, non en terme de valeurs de vérité, mais en terme d'ensemble de ses preuves.

La volonté derrière l'interprétation BHK est de définir une logique constructiviste, en ce qu'une preuve d'une proposition doit explicitement construire les outils auxquels elle fait appel (en particulier, on cherche à avoir la propriété du témoin, qui dit que si $\vdash \exists x, \varphi$ alors il existe $t$ tel que $\vdash \varphi[t/x]$). Comme le principe du tiers exclu est non constructif, on se place dans le cadre de la logique intuitionniste.

On suppose qu'on a un ensemble de propositions $\Phi$ et un ensemble de témoins, $\bX$. L'interprétation BHK donne une façon de penser une relation signifiant \textit{être une preuve de}, qu'on notera $x\reali \varphi$~:
\begin{itemize}
\item on suppose qu'on a défini ce que signifie $x\reali\varphi$ pour $\varphi$ atomique
\item on n'a jamais $x\reali \bot$
\item si $\varphi$ est de la forme $\psi\to \chi$, alors $x\reali\psi \to\chi$ signifie que $x$ est une fonction (en un sens intuitif pour le moment) qui à $y\in \bX$ tel que $y\reali \psi$, associe $x(y)\in \bX$ tel que $x(y)\reali \chi$.
\item si $\varphi$ est de la forme $\psi\land\chi$, alors $x\reali \psi\land \chi$ signifie que $x$ est une paire de deux éléments $y\reali \psi$ et $z\reali \chi$~: prouver la conjonction de deux formules signifie avoir une preuve de chaque formule.
\item si $\varphi$ est de la forme $\psi\lor\chi$, alors $x\reali\psi\lor \chi$ signifie que $x$ s'écrit $(i,y)$ où, si $i = 1$, $y\reali \psi$ et si $i = 2$, $y\reali \chi$. Cela correspond à une union disjointe (en particulier, une preuve d'une disjonction garde la trace de laquelle des deux propositions est prouvée).
\item si $\varphi$ est de la forme $\forall a, \psi$ alors $x\reali\forall a, \psi$ signifie que $x$ représente une fonction qui, pour tout $v$ du domaine de discours, associe $x(a)\reali \psi[v/a]$. Une preuve d'une quantification universelle est ainsi une fonction qui, au terme auquel on veut appliquer la proposition, renvoie une preuve de cette proposition appliquée au terme.
\item si $\varphi$ est de la forme $\exists a, \psi$ alors $x\reali \exists a, \psi$ signifie que $x$ s'écrit comme une paire $(t,y)$ où $t$ est un terme et $y\reali \psi[t/a]$. Ainsi, donner une preuve d'une proposition existentielle signifie donner un témoin du caractère existentiel (quel terme est désigné par le $\exists$) et une preuve que ce témoin vérifie effectivement la proposition.
\end{itemize}

Dans cette interprétation, dire qu'une formule $\varphi$ est vraie signifie
\[\exists x \in \bX, x \reali \varphi\]
On souhaite donc que cette notion de vérité définisse une théorie logique, c'est-à-dire que la propriété ``être vraie'' soit stable par déduction naturelle. On appellera adéquation la propriété correspondant à cette stabilité par déduction naturelle.

\subsection{Sur l'adéquation}

Rappelons les règles de déduction naturelle, et profitons-en pour clarifier les choix syntaxiques que nous faisons. Tout d'abord, on considère une logique multi-sortée, d'où une définition un peu plus chargée d'une signature~:
\begin{defi}[Signature]
    Une signature $\Sigma$ est la donnée~:
    \begin{itemize}
    \item d'un ensemble $\mathcal S$ de sortes.
    \item d'un ensemble $\mathcal F$ de symboles de fonctions, et d'une fonction $\alpha : \mathcal F\to \mathcal S^* \times \mathcal S$ donnant à chaque symbole son arité.
    \item d'un ensemble $\mathcal R$ de symboles de fonctions, et d'une fonction $\alpha : \mathcal R \to \mathcal S^*$ donnant à chaque symbole son arité.
    \end{itemize}
\end{defi}

On se fixe donc une signature $\Sigma$ pour le reste de cette présentation de la syntaxe utilisée. On fixe de plus un ensemble $\mathcal X_1$, dénombrable, de variables du premier ordre, qu'on notera $\bx,\by,\ldots$ (on gardera les lettres non en gras pour désigner d'autres variables). L'ensemble des termes typés est défini par les règles suivantes, où $S,S_1,\ldots$ désignent des éléments de $\mathcal S$ et $\Gamma$ désigne un contexte de typage du premier ordre (une liste de couples $(x,S)$ où $x \in \mathcal X_1$ et $S\in\mathcal S$)~:
\begin{center}
    \begin{prooftree}
        \infer0{\Gamma, \bx : S \vdash \bx : S}
    \end{prooftree}
    \qquad
    \begin{prooftree}
        \hypo{\Gamma\vdash \bt_1 : S_1}
        \hypo{\cdots}
        \hypo{\Gamma\vdash \bt_n : S_n}
        \hypo{\alpha(f) = (S_1,\ldots,S_n,S)}
        \infer4{\Gamma\vdash f(\bt_1,\ldots,\bt_n) : S}
    \end{prooftree}
\end{center}

On définit maintenant les propositions, mais comme celles-ci contiennent des constructions d'ordre $2$, on doit d'abord introduire des variables d'ordre $2$. On notera $\mathcal X_2$ l'ensemble des variables d'ordre $2$ (ces variables seront notées en majuscules), et on appellera contexte de typage d'ordre deux une liste de couples $(X,S)$ où $X \in \mathcal X_2$ et $S \in \mathcal S^*$.

\begin{defi}[Propositions]
    On définit les propositions (bien formées) par les règles de typage suivantes~:
    \begin{center}
        \begin{prooftree}
            \hypo{\Gamma\vdash \bt_1 : S_1}
            \hypo{\cdots}
            \hypo{\Gamma\vdash \bt_n : S_n}
            \infer3{\Gamma\mid\Delta, X : (S_1,\ldots,S_n) \vdash X(\bt_1,\ldots,\bt_n) : \Propo}
        \end{prooftree}
        \quad
        \begin{prooftree}
            \hypo{\Gamma\vdash \bt_1 : S_1}
            \hypo{\cdots}
            \hypo{\Gamma\vdash \bt_n : S_n}
            \hypo{\alpha(r) = (S_1,\ldots,S_n)}
            \infer4{\Gamma\mid\Delta\vdash r(\bt_1,\ldots,\bt_n) : \Propo}
        \end{prooftree}

        \vspace{0.5cm}
        
        \begin{prooftree}
            \hypo{\Gamma\mid\Delta\vdash\varphi : \Propo}
            \hypo{\Gamma\mid\Delta\vdash \psi : \Propo}
            \infer2{\Gamma\mid\Delta\vdash \varphi\to\psi : \Propo}
        \end{prooftree}
        \quad
        \begin{prooftree}
            \hypo{\Gamma, \bx : S\mid \Delta\vdash \varphi : \Propo}
            \infer1{\Gamma\mid\Delta\vdash \forall \bx^S, \varphi : \Propo}
        \end{prooftree}
        \quad
        \begin{prooftree}
            \hypo{\Gamma\mid \Delta, X : (S_1,\ldots,S_n)\vdash \varphi : \Propo}
            \infer1{\Gamma\mid\Delta\vdash \forall X^{S_1,\ldots,S_n}, \varphi : \Propo}
        \end{prooftree}
    \end{center}
\end{defi}

On définit de façon évidente les notions de variables libres dans les termes et les propositions, on notera $\varlib{\bt}$ (respectivement $\varlib{\varphi}$) pour cela. Les variables libres d'une proposition contiennent potentiellement des éléments de $\mathcal X_1$ et de $\mathcal X_2$.

On définit aussi la substitution d'ordre $1$ et d'ordre $2$.

\begin{defi}[Substitution du premier ordre]
    Soit $\Gamma, \bx : S\vdash \bt : S'$ et $\Gamma\vdash \bu : S$, on définit alors $\Gamma\vdash \bt[\bu/\bx] : S'$ par induction sur $\bt$~:
    \begin{itemize}
        \item si $\bt = \bx$ alors $\bt[\bu/\bx] = \bu$.
        \item si $\bt = \by$, où $\by \neq \bx$, alors $\bt[\bu/\bx] = \bt$.
        \item si $\bt = f(\bt_1,\ldots,\bt_n)$ alors $\bt[\bu/\bx] = f(\bt_1[\bu/\bx],\ldots,\bt_n[\bu/\bx]$.
    \end{itemize}
    On peut maintenant, pour $\Gamma, \bx : S\mid\Delta\vdash \varphi : \Propo$ et $\Gamma\vdash \bt : S$, définir $\Gamma\mid\Delta\vdash\varphi[\bt/\bx] : \Propo$~:
    \begin{itemize}
        \item si $\varphi = X(\bt_1,\ldots,\bt_n)$ alors $\varphi[\bt/\bx] = X(\bt_1[\bt/\bx],\ldots,\bt_n[\bt/\bx])$.
        \item si $\varphi = r(\bt_1,\ldots,\bt_n)$ alors $\varphi[\bt/\bx] = r(\bt_1[\bt/\bx],\ldots,\bt_n[\bt/\bx])$.
        \item si $\varphi = \psi \to \chi$ alors $\varphi[\bt/\bx] = \psi[\bt/\bx] \to \chi[\bt/\bx]$.
        \item si $\varphi = \forall \by^{S'}, \psi$ et que $\by\notin \varlib{\bt}$, alors $\varphi[\bt/\bx] = \forall \by^{S'}, \psi[\bt/\bx]$.
        \item si $\varphi = \forall X^{S_1,\ldots,S_n}, \psi$ alors $\varphi[\bt/\bx] = \forall X^{S_1,\ldots,S_n}, \psi[\bt/\bx]$.
    \end{itemize}
\end{defi}

Par une induction sur les termes et les propositions, on peut montrer que la substitution préserve le typage, c'est-à-dire qu'on a bien la relation de typage décrite dans la définition~: $\Gamma\vdash \bt[\bu/\bx] : S'$ et $\Gamma\mid\Delta\vdash \varphi[\bt/\bx] : \Propo$.

\begin{defi}[Substitution du second ordre]
    Soit $\Gamma\mid \Delta, X : S_1,\ldots,S_n\vdash \varphi : \Propo$ et $\Gamma, \bx_1 : S_1,\ldots, \bx_n : S_n\mid\Delta\vdash \psi : \Propo$, alors on définit $\Gamma\mid\Delta\vdash\varphi[X/\psi(\bx_1,\ldots,\bx_n)]$ par induction sur $\varphi$~:
    \begin{itemize}
        \item si $\varphi = X(\bt_1,\ldots,\bt_n)$ alors $\varphi[X/\psi(\bx_1,\ldots,\bx_n)] = \psi[\bt_1/\bx_1,\ldots,\bt_n/\bx_n]$.
        \item si $\varphi = Y(\bt_1,\ldots,\bt_n)$
        \item si $\varphi = r(\bt_1,\ldots,\bt_n$ alors $\varphi[X/\psi(\bx_1,\ldots,\bx_n)] = \varphi$.
        \item si $\varphi = \chi \to \theta$ alors $\varphi[X/\psi(\bx_1,\ldots,\bx_n)] = \chi[X/\psi(\bt_1,\ldots,\bt_n)] \to \theta[X/\psi(\bt_1,\ldots,\bt_n)]$.
        \item si $\varphi = \forall \bx^S, \chi$ et $\bx \notin \varlib{\psi}$, alors $\varphi[X/\psi(\bx_1,\ldots,\bx_n)] = \forall \bx^S, \chi[X/\psi(\bt_1,\ldots,\bt_n)]$.
        \item si $\varphi = \forall Y^{S'_1,\ldots,S'_n}, \chi$ et $Y\notin\varlib{\psi}$, alors $\varphi[X/\psi(\bx_1,\ldots,\bx_n)] = \forall Y^{S'_1,\ldots,S'_n}, \chi[X/\psi(\bx_1,\ldots,\bx_n)]$.
    \end{itemize}
\end{defi}

Là encore, une induction nous permet de vérifier que cette substitution interagit correctement avec le typage.

Cela nous permet enfin d'introduire la déduction naturelle. Les règles que nous présentons sont particulièrement chargées, puisqu'elles utilisent trois contexte (un contexte de typage du premier ordre, un du second ordre et un contexte logique), mais elles sont globalement les mêmes que les règles habituelles de la déduction naturelle.

\begin{defi}[Déduction naturelle]
    On définit la relation $\Gamma\mid\Delta\mid\Xi\vdash\varphi$, où $\Gamma$ est un contexte de typage du premier ordre, $\Delta$ du second ordre, $\Xi$ un contexte logique, c'est-à-dire ici une liste de propositions, et $\varphi$ une proposition, par les règles suivantes~:
    \begin{center}
        \begin{prooftree}
            \infer0[Ax]{\Gamma\mid\Delta\mid\Xi, \varphi\vdash \varphi}
        \end{prooftree}
        \qquad
        \begin{prooftree}
            \hypo{\Gamma\mid\Delta\mid\Xi, \varphi\vdash \psi}
            \infer1[$\to_\mathrm i$]{\Gamma\mid\Delta\mid\Xi\vdash \varphi \to \psi}
        \end{prooftree}
        \qquad
        \begin{prooftree}
            \hypo{\Gamma\mid\Delta\mid\Xi\vdash \varphi\to \psi}
            \hypo{\Gamma\mid\Delta\mid\Xi\vdash \varphi}
            \infer2[$\to_\mathrm e$]{\Gamma\mid\Delta\mid\Xi\vdash \psi}
        \end{prooftree}

        \vspace{0.5cm}

        \begin{prooftree}
            \hypo{\Gamma, \bx : S\mid\Delta\mid\Xi\vdash\varphi}
            \infer1[$\forall_\mathrm i^1$]{\Gamma\mid\Delta\mid\Xi\vdash \forall \bx^S, \varphi}
        \end{prooftree}
        \qquad
        \begin{prooftree}
            \hypo{\Gamma\mid\Delta\mid\Xi\vdash \forall x^S, \varphi}
            \hypo{\Gamma\vdash \bt : S}
            \infer2[$\forall_\mathrm e^1$]{\Gamma\mid\Delta\mid\Xi\vdash \varphi[\bt/\bx]}
        \end{prooftree}

        \vspace{0.5cm}

        \begin{prooftree}
            \hypo{\Gamma\mid\Delta, X : S_1,\ldots,S_n\mid\Xi\vdash \varphi}
            \infer1[$\forall_\mathrm i^2$]{\Gamma\mid\Delta\mid\Xi\vdash \forall X^{S_1,\ldots,S_n}, \varphi}
        \end{prooftree}
        \qquad
        \begin{prooftree}
            \hypo{\Gamma\mid\Delta\mid\Xi\vdash \forall X^{S_1,\ldots,S_n}, \varphi}
            \hypo{\Gamma, \bx_1 : S_1,\ldots, \bx_n : S_n\mid \Delta\vdash \psi : \Propo}
            \infer2[$\forall_\mathrm e^2$]{\Gamma\mid\Delta\mid\Xi\vdash \varphi[X/\psi(\bx_1,\ldots,\bx_n)]}
        \end{prooftree}
    \end{center}
\end{defi}

Dire que notre interprétation est adéquate signifie alors que $\reali$ est stable par les mêmes règles. Cependant, il n'est pas clair comment interpréter au niveau de la relation $\reali$ le séquent $\Xi\vdash \varphi$ (le contexte de typage, lui, peu être considéré comme extérieur à l'étude logique). Pour l'interpréter, on va considérer que nos éléments $x$ de $\bX$ s'interprètent comme des fonctions $f_x$, $n$-aires (où $\Xi = \varphi_1,\ldots,\varphi_n$), telles que
\[\forall x_1,\ldots,x_n, (\forall i \in \{1,\ldots,n\}, x_i \reali \varphi_i)\implies f_x(x_1,\ldots,x_n)\reali\varphi\]

Dans ce contexte, l'adéquation signifie que notre langage a besoin des constructions suivantes~:
\begin{itemize}
    \item il doit y avoir un moyen d'abstraire une variable et d'en instancier une, permettant de transformer un élément $x\in\bX$ interprété comme une fonction $(x_1,\ldots,x_n) \mapsto f_x(x_1,\ldots,x_n)$ en une fonction $(x_1,\ldots,x_{n-1})\mapsto (y\mapsto f_x(x_1,\ldots,x_{n-1},y))$ et une fonction $(x_1,\ldots,x_n)\mapsto f_x(x_1,\ldots,x_n)$ avec un objet $y\in\bX$ en une fonction $(x_1,\ldots,x_{n-1})\mapsto f_x(x_1,\ldots,x_{n-1},y)$.
    \item les règles sur $\forall^1$ suggèrent de pouvoir représenter nos termes à l'intérieur même de notre ensemble $\bX$~: on veut par exemple avoir un objet de $\bX$ pour chaque entier $n\in\bN$.
    \item nous reviendrons plus tard sur les quantification du second ordre, qui seront interprétées par des contextes supplémentaires.
\end{itemize}

Un élément important de cette façon de faire est la modularité~: dans ce qui est prescrit ici, on ne donne aucune condition sur la forme globale de $\bX$ mais seulement sur des constructions qui doivent exister, ce qui signifie que l'on pourra modifier $\bX$ en gardant la propriété d'adéquation tant que nos constructions restent présentes. Comme on veut considérer $\bX$ comme un langage de programmation, ce point est essentiel, puisqu'on a très souvent envie d'ajouter des constructions ou des primitives à un langages en souhaitant préserver les propriétés déjà établies.

\subsection{A propos du second ordre}

Le choix de la logique du second ordre nous a permis d'alléger considérablement le nombre de constructeurs introduits pour construire des formules. En effet, la combinaison de $\to$ et $\forall^2$ permet de récupérer les constantes $\top$ et $\bot$ ainsi que $\lor$ et $\land$, mais aussi les quantifications existentielles.

Donnons donc ces différents encodages~:
\begin{align*}
    \top &\defeq \forall X, X \to X\\
    \bot &\defeq \forall X, X\\
    \varphi \land \psi &\defeq \forall X, (\varphi \to \psi \to X) \to X\\
    \varphi \lor \psi &\defeq \forall X, (\varphi \to X) \to (\psi \to X) \to X\\
    \exists x^S, \varphi &\defeq \forall X, (\forall x^S, (\varphi \to X)) \to X\\
    \exists X^{S_1,\ldots,S_n}, \varphi &\defeq \forall Y, (\forall X^{S_1,\ldots,S_n}, (\varphi \to Y))\to Y
\end{align*}

\begin{rmk}
    On n'écrit pas d'arité sur les $X$ et $Y$ dans ces définitions car ceux-ci sont d'arité nulle~: ils sont remplacés par des formules closes.
\end{rmk}

Nous ne donnons pas ici les règles liées à ces différents constructeurs, mais on peut prouver que les règles logique usuelles sont dérivables à partir de notre encodage et des règles déjà introduites. Donnons au moins un exemple avec $\exists_\mathrm e^1$~:

\begin{center}
    \begin{prooftree}
        \hypo{\Gamma\mid\Delta\mid\Xi\vdash \exists x^S, \varphi}
        \infer0{\Gamma\mid\Delta\vdash \psi : \Propo}
        \infer2[$\forall_\mathrm e^2$]{\Gamma\mid\Delta\mid\Xi\vdash (\forall x^S, \varphi \to \psi)\to \psi}
        \hypo{\Gamma, x : S\mid\Delta\mid\Xi, \varphi\vdash \psi}
        \infer1[$\to_\mathrm i$]{\Gamma, x : S\mid\Delta\mid\Xi\vdash \varphi\to\psi}
        \infer1[$\forall_\mathrm i^1$]{\Gamma\mid\Delta\mid\Xi\vdash \forall x^S, \varphi\to\psi}
        \infer2[$\to_\mathrm e$]{\Gamma\mid\Delta\mid\Xi\vdash \psi}
    \end{prooftree}
\end{center}

Passons maintenant à comment interpréter le second ordre dans notre modèle de réalisabilité. Une variable du second ordre représente une partie de l'univers de discours. Par exemple, $X^\bN$ représente une partie de $\mathbb N$. Les parties d'un ensemble $X$, en logique classique, donc exactement les éléments de $2^X$~: cela est dû au fait que le sens d'une proposition est toujours un élément de $2$. Comme, dans notre cas, le sens d'une proposition doit être l'ensemble de ses témoins, on remplace $2$ par $\mathcal P(\bX)$. On obtient donc comme interprétation d'une variable $X^\bN$ une fonction $\mathbb N \to \mathcal P (\bX)$ Plus généralement, en ayant défini une interprétation $\bS$ de chaque sorte $S$, une variable $X^{S_1,\ldots,S_n}$ sera interprétée par une fonction $\bS_1\times\cdots\times\bS_n \to \mathcal P(\bX)$.

Pour permettre à notre relation de réalisabilité de prendre en compte le second ordre, nous allons paramétrer cette relation par une assignation (partielle) des variables du second ordre à des interprétations comme décrites plus haut.

\'Etant donnée une interprétation des variables du second ordre, qu'on notera $\rho$, on notera $x\reali_\rho \varphi$ pour dire que $x$ est une preuve de $\varphi$ dans laquelle on interprète les variables libres du second ordre de $\varphi$ par $\rho$. Cela nous permet alors de définir \[x\reali_\rho \forall X^{S_1,\ldots,S_n}, \varphi \defeq x\in \bigcap_{A : \bS_1\times\cdots\times \bS_n \to \mathcal P(\bX)} \{x \in \bX\mid x\reali_{\rho[X\leftarrow A]} \varphi\}\]

Cette interprétation ne fonctionne que dans le cas où $\varphi$ est close pour les variables du premier ordre, mais on peut adapter l'idée précédente au premier ordre~: on ajoute une assignation $\sigma$ de $\mathcal X_1$ vers l'ensembles des $\bS$ pour $S\in\mathcal S$. Ce faisant, on peut définir $x\reali_\rho^\sigma \varphi$. Cependant, le sens de $x\reali \forall x^S, \varphi$ change alors~: cela signifie que $x$ est une preuve de tous les $\varphi[\bt/\bx]$, et non une fonction associant $s\in \bS$ à une preuve de $\varphi[s/\bx]$. On a donc deux possibilités d'interprétation du $\forall^1$~: comme une réalisation uniforme et comme une réalisation fonctionnelle. Nous verrons plus tard qu'on peut en fait exprimer la seconde à travers la première.

\subsection{Le langage de programmation}

Maintenant que nous avons donné les idées principales à propos de la relation de réalisabilité, précisons l'ensemble $\bX$ que nous étudions. Comme nous avons parlé d'interpréter nos éléments comme des fonctions, il semble naturel de considérer le $\lambda$-calcul, dans lequel tout terme peut se voir comme une fonction. On présentera succinctement le $\lambda$-calcul, principalement pour s'accorder sur les conventions et les notations.

\begin{defi}[$\lambda$-termes]
    On se fixe un ensemble dénombrable $\mathcal X_\Lambda$ de $\lambda$-variables, qu'on notera $x,y,\ldots$ On définit l'ensemble $\Lambda$ des $\lambda$-termes comme l'ensemble engendré par la grammaire suivante~:
    \[t,u ::= x \mid \lambda x. t\mid t\;u\]
\end{defi}

Pour le parenthésage, on prendra la convention de minimiser le nombre de parenthèses en les écrivant seulement quand nécessaire, et en prenant l'application (la construction $t\;u$) comme associative à droite, c'est-à-dire que $t\;u\;v$ se lit $(t\;u)\;v$. On considère toujours les $\lambda$-termes à $\alpha$-conversion près, c'est-à-dire que changer le nom de variables liées par de nouvelles variables (dans le $\lambda$ qui lie cette variable y compris) ne change pas le terme. On définit la substitution simultanée et la simple substitution~:

\begin{defi}[Substitution de termes]
    Soit $\nu : \mathcal X_\Lambda \rightharpoonup \Lambda$ une substitution de termes et $t\in \Lambda$, on notera $\nu(t)$ le terme défini par induction sur $t$ par~:
    \begin{itemize}
        \item si $t = x$ et $x \in \domm(\nu)$ alors $\nu(t) = \nu(x)$.
        \item si $t = x$ et $x\notin\domm(\nu)$ alors $\nu(t) = x$.
        \item si $t = \lambda x.u$ et $x\notin\bigcup_{y\in\domm(\nu)}\varlib{\nu(y)}$ alors $\nu(t) = \lambda x.\nu(u)$.
        \item si $t = u\;v$ alors $\nu(t) = (\nu(u))\;(\nu(v))$.
    \end{itemize}

    Si $\nu$ est définie uniquement sur un élément $x$, alors pour $u = \nu(x)$ on notera $t[u/x] = \nu(t)$.
\end{defi}

On définit aussi la $\beta$-réduction.

\begin{defi}[$\beta$-réduction]
    On définit d'abord la relation $\mapsto\subseteq \Lambda\times\Lambda$ par la règle
    \[(\lambda x.t)u \mapsto t[u/x]\]
    La relation $\rhd\subseteq \Lambda\times\Lambda$ est alors la plus petite relation contenant $\mapsto$ et stable par les règles
    \begin{center}
        \begin{prooftree}
            \hypo{t \rhd u}
            \infer1{\lambda x.t \rhd \lambda x.u}
        \end{prooftree}
        \quad
        \begin{prooftree}
            \hypo{t\rhd t'}
            \infer1{t\;u\rhd t'\;u}
        \end{prooftree}
        \quad
        \begin{prooftree}
            \hypo{u \rhd u'}
            \infer1{t\;u\rhd t\;u'}
        \end{prooftree}
    \end{center}
\end{defi}

Commençons maintenant à présenter un premier modèle de réalisabilité, basé sur ce $\lambda$-calcul. Nous utiliserons les notions d'environnement définies plus tôt à la fois pour le premier et le second ordre. On se place dans le cas d'une signature sans symbole de relation, et on considère que $\Nat$ est l'unique sorte, pour simplifier notre modèle dans un premier temps. On possède les fonctions habituelles de l'arithmétique~: $S,+,\times$ et la constante $0$. On interprétera $\Nat$ par l'ensemble $\bN$.

Précisons d'abord les interprétations de variables qui nous intéressent. On dira qu'une interprétation $\sigma$ du premier ordre, c'est-à-dire une fonction partielle $\sigma : \mathcal X_1 \rightharpoonup \bN$, est adéquate vis à vis de $\varphi$ si $\domm(\sigma) \supseteq\varlib{\varphi}$ pour le premier ordre. Dans le cas de plusieurs sortes, on demande aussi que $\sigma(\bx)$ soit du même type que $\bx$ dans $\varphi$ (si par exemple $\bx : S \vdash \varphi : \Propo$ alors on doit avoir $\vdash \sigma(\bx) : S$). On notera $\sigma\models \varphi$ dans ce cas. Remarquons que si $\sigma \models \varphi$ et que $\bt$ apparaît dans $\varphi$, alors on peut naturellement lui associer un élément $\sigma(\bt) \in \bN$.


On fait de même pour le second ordre~: une interprétation $\rho$ du second ordre, qui à $X \in \mathcal X_2$ associe une fonction $\bN^n \to \mathcal P(\Lambda)$ est adéquate pour $\varphi$ si les variables libres du second ordre de $\varphi$ sont dans le domaine de $\rho$, et si l'arité de $\rho(X)$ correspond à l'arité de $X$ dans le typage de $\varphi$. On notera, là encore, $\rho\models \varphi$.

On peut maintenant définir la relation de réalisabilité.

\begin{defi}[Relation de réalisabilité]
    Soit une proposition $\varphi$, $\sigma\models \varphi$ et $\rho\models \varphi$. On définit $\trad\varphi_\rho^\sigma \in \mathcal P(\Lambda)$ par induction sur $\varphi$~:
    \begin{itemize}
        \item si $\varphi = X(\bt_1,\ldots,\bt_n)$ alors $\trad\varphi_\rho^\sigma = \rho(X)(\sigma(\bt_1),\ldots,\sigma(\bt_n))$.
        \item si $\varphi = \psi \to \chi$ alors $\trad{\varphi\to\chi}_\rho^\sigma = \{t \in \Lambda\mid \forall u \in \trad{\psi}_\rho^\sigma, t\;u \in \trad{\chi}_\rho^\sigma\}$.
        \item si $\varphi = \forall \bx^S, \psi$ alors $\displaystyle\trad{\forall \bx^S, \psi}_\rho^\sigma = \bigcap_{n \in \bS} \trad{\psi}_\rho^{\sigma[\bx\leftarrow n]}$
        \item si $\varphi = \forall X^{S_1,\ldots,S_n}, \psi$ alors $\displaystyle\trad{\forall X^{S_1,\ldots,S_n},\psi}_\rho^\sigma = \bigcap_{F : \bS_1\times\cdots\times\bS_n \to \mathcal P(\Lambda)} \trad\psi_{\rho[X\leftarrow F]}^\sigma$
    \end{itemize}

    On notera $t\reali_\rho^\sigma \varphi$ si $t\in\trad\varphi_\rho^\sigma$ et $t\reali\varphi$ pour $t\reali_\varnothing^\varnothing\varphi$, et on dira alors que $t$ réalise (ou est un réaliseur de) $\varphi$.
\end{defi}

On souhaite maintenant montrer l'adéquation. L'interprétation de $t\in \Lambda$ comme une fonction $n$-aire peut se faire en prenant $(t_1,\ldots,t_n) \mapsto t[t_1/x_1,\ldots,t_n/x_n]$ en considérant qu'on a ordonné nos variables. Avec cette définition, l'interprétation de $\varphi_1,\ldots,\varphi_n\vdash \varphi$ devient
\[\forall t_1,\ldots,t_n \in \Lambda, (\forall i \in \{1,\ldots,n\}, t_i \reali \varphi_i)\implies t[t_1/x_1,\ldots,t_n/x_n]\reali\varphi\]

On voit alors que la règle d'axiome est vérifiée directement en prenant le terme $x_i$, qui vaudra $t_i$ après substitution et par hypothèse $t_i\reali \varphi_i = \varphi$. Cette règle est exactement celle qu'on attend du typage des $\lambda$-termes, et la prouvabilité est une relation syntaxique~: il convient donc de travailler directement avec un système de type et de montrer que ce système de type est adéquat en ce qu'un terme typé est un réaliseur.

\begin{defi}[Système de type]
    On définit la relation $\Gamma\mid\Delta\mid\Xi\vdash t : \varphi$, où $\Gamma$ et $\Delta$ sont comme introduits plus tôt, et $\Xi$ est une liste de couples $(x,\varphi)$ où $x \in \mathcal X_\Lambda$ et $\varphi \in \Propo$, par les règles suivantes~:
    \begin{center}
        \begin{prooftree}
            \infer0[Ax]{\Gamma\mid\Delta\mid\Xi, x : \varphi\vdash x : \varphi}
        \end{prooftree}
        \qquad
        \begin{prooftree}
            \hypo{\Gamma\mid\Delta\mid\Xi, x : \varphi\vdash t : \psi}
            \infer1[$\to_\mathrm i$]{\Gamma\mid\Delta\mid\Xi\vdash \lambda x. t : \varphi \to \psi}
        \end{prooftree}
        \qquad
        \begin{prooftree}
            \hypo{\Gamma\mid\Delta\mid\Xi\vdash t : \varphi\to \psi}
            \hypo{\Gamma\mid\Delta\mid\Xi\vdash u : \varphi}
            \infer2[$\to_\mathrm e$]{\Gamma\mid\Delta\mid\Xi\vdash t\;u : \psi}
        \end{prooftree}

        \vspace{0.5cm}

        \begin{prooftree}
            \hypo{\Gamma, \bx : S\mid\Delta\mid\Xi\vdash t : \varphi}
            \infer1[$\forall_\mathrm i^1$]{\Gamma\mid\Delta\mid\Xi\vdash t : \forall \bx^S, \varphi}
        \end{prooftree}
        \qquad
        \begin{prooftree}
            \hypo{\Gamma\mid\Delta\mid\Xi\vdash t : \forall x^S, \varphi}
            \hypo{\Gamma\vdash \bt : S}
            \infer2[$\forall_\mathrm e^1$]{\Gamma\mid\Delta\mid\Xi\vdash t : \varphi[\bt/\bx]}
        \end{prooftree}

        \vspace{0.5cm}

        \begin{prooftree}
            \hypo{\Gamma\mid\Delta, X : S_1,\ldots,S_n\mid\Xi\vdash t : \varphi}
            \infer1[$\forall_\mathrm i^2$]{\Gamma\mid\Delta\mid\Xi\vdash t : \forall X^{S_1,\ldots,S_n}, \varphi}
        \end{prooftree}
        \qquad
        \begin{prooftree}
            \hypo{\Gamma\mid\Delta\mid\Xi\vdash t : \forall X^{S_1,\ldots,S_n}, \varphi}
            \hypo{\Gamma, \bx_1 : S_1,\ldots, \bx_n : S_n\mid \Delta\vdash \psi : \Propo}
            \infer2[$\forall_\mathrm e^2$]{\Gamma\mid\Delta\mid\Xi\vdash t : \varphi[X/\psi(\bx_1,\ldots,\bx_n)]}
        \end{prooftree}
    \end{center}
\end{defi}

Nous avons donc vu que la règle Ax est directement vérifiée par la forme du lemme d'adéquation que nous voulons montrer. Passons maintenant à la règle $\to_\mathrm i$. Pour celle-ci, supposons qu'on a $\nu$ tel que $t_i = \nu(x_i) \reali \varphi_i$, $\nu(x) \reali \varphi$ et $t[t_1/x_1,\ldots,t_n/x_n, \nu(x)/x]\reali \varphi$, on veut en déduire que $\lambda x.t[t_1/x_1,\ldots,t_n/x_n]\reali \varphi\to\psi$. Pour cela, si on a $u \reali \varphi$, alors on veut que $(\lambda x.\nu(t))\;u\reali \psi$~: il n'y a pas de raison que cela soit vrai. Cependant, en utilisant l'hypothèse d'induction avec $\nu(x_i) = t_i$ et $\nu(x) = u$, on obtient $t[t_i/x_i, t/x]$ et on voit que $(\lambda x.t[t_i/x_i])u \rhd t[t_i/x_i, t/x]$ (quitte à faire de l'$\alpha$-conversion), on souhaite donc que $\trad\varphi$ soit stable par antiréduction pour permettre la validité de cette règle.

Cette exigence peut sembler artificielle, mais elle vient simplement du fait que le $\lambda$-calcul est un système syntaxique, et il contient donc plusieurs termes qui peuvent avoir le même sens~: en l'occurrence, ici, $(\lambda x.t)u$ et $t[u/x]$ ont tous les deux le sens de $f(x)$ dans un cadre plus sémantique. On pourrait donc décider de travailler dans $\Lambda/=_\beta$, mais l'analyse donnée plus tôt nous montre que cette exigence n'est en fait pas nécessaire, puisqu'il suffit de conserver la stabilité par antiréduction. Une partie stable par antiréduction est donc, en un sens, une partie ayant un contenu sémantique. Remarquons quand même que ce choix dépend largement du formalisme utilisé (ici le $\lambda$-calcul) mais aussi de la réduction considérée~: on aurait pu considérer la réduction call-by-value, cherchant à évaluer les arguments avant de faire une substitution, et les parties saturées auraient dû être définies différemment en obligeant l'évaluation de l'argument.

\begin{defi}[Partie saturée]
    On dit qu'une partie $S\subseteq \Lambda$ est saturée si la condition suivante est vérifiée~:
    \[\forall t,u\in \Lambda, t \rhd u \land u \in S \implies t \in S\]

    On note $\SAT$ l'ensemble des parties saturées.
\end{defi}

On peut montrer que pour avoir $\trad\varphi \in \SAT$, il suffit de l'imposer sur les interprétations des variables du second ordre (et des symboles de relations, mais nous n'en avons pas ici). Pour cela, on montre d'abord que les opérations ensemblistes usuelles préservent la stabilité.

\begin{lem}
  $(\SAT,\subseteq,\Lambda,\varnothing,\bigcap,\bigcup)$ est un treillis complet.
\end{lem}

\begin{proof}
  Il est évident que $S\in \SAT \implies \varnothing\subseteq S \subseteq \Lambda$ et que ces deux parties sont stables par antiréduction. Supposons que $\{S_i\}_{i\in I}$ est une famille de parties saturées. Si $t\reduc u$ et $u \in \bigcap S_i$, alors pour tout $i \in I$, $t \in S_i$ par saturation de $S_i$, donc $t\in \bigcap S_i$, donc $\bigcap S_i$ est saturée. De même si $u \in \bigcup S_i$, alors on trouve $i$ tel que $u \in S_i$ donc $t\in S_i$ par saturation. Donc $\bigcup S_i$ est saturée. Ainsi $(\SAT,\subseteq,\Lambda,\varnothing, \bigcap,\bigcup)$ est un treillis complet.
\end{proof}

On décide maintenant de modifier l'interprétation au-dessus en disant juste que $\rho(X) : \bN^n \to \SAT$, et de même que l'intersection pour le $\forall X, \varphi$ est définie sur $S : \bN^n \to \SAT$. On peut alors montrer le lemme de saturation.

\begin{lem}[Saturation]
  Pour toute formule $\varphi$, $\trad\varphi_\rho^\sigma\in \SAT$.
\end{lem}

\begin{proof}
  On procède par induction sur $\varphi$~:
  \begin{itemize}
  \item si $\varphi = X(t_1,\ldots,t_n)$ alors $\trad\varphi_\rho^\sigma=\rho(X)(t_1^\sigma,\ldots,t_n^\sigma)\in \SAT$.
  \item si $\varphi = \psi \to \chi$, supposons que $t\reali_\rho^\sigma \psi \to \chi$, $t'\reduc t$ et $u\reali_\rho^\sigma\psi$. Par hypothèse, $t\;u\reali_\rho^\sigma \chi$, et comme par hypothèse d'induction $\trad\chi_\rho^\sigma\in\SAT$ et $t'\;u\reduc t\;u$, on en déduit que $t'\;u\reali_\rho^\sigma \chi$. Ainsi pour tout $u\reali_\rho^\sigma\psi$, $t'\;u\reali_\rho^\sigma\chi$, ce qui signifie que $t'\reali_\rho^\sigma\psi\to\chi$.
  \item les deux interprétations données par des intersections donnent des ensembles saturés car $\SAT$ est un treillis complet.
  \end{itemize}
  Donc par induction $\trad\varphi_\rho^\sigma\in\SAT$.
\end{proof}

On a donc traité les règles Ax et $\to_\mathrm i$ pour l'adéquation. La règle $\to_\mathrm e$ est automatique~: la définition même de $\trad{\psi\to\chi}$ nous assure que la règle est vérifiée. Pour les deux quantificateurs, par contre, il nous faut introduire des lemmes de substitution, pour prouver qu'enrichir l'environnement $\sigma$ (respectivement $\rho$) revient à faire une substitution dans le terme (respectivement la proposition) qu'on interprète.

\begin{lem}\label{lem.subst.1}
  Pour tous $\varphi : \Propo$, $\sigma\models\varphi,\rho\models\varphi$, $\bt : S$, $t\in\Lambda$, on a
  \[t\reali_\rho^{\sigma[\bx \leftarrow \bt^\sigma]} \varphi\iff t\reali_\rho^\sigma \varphi[\bt/\bx]\]
\end{lem}

\begin{proof}
  Tout d'abord, par une induction sur $\bu$ (que nous ne détaillerons pas), on peut quitte à renommer nos variables considérer que $\forall \bu, \bu^{\sigma[\bx\leftarrow \bt^\sigma]} = (\bu[\bt/\bx])^\sigma$. On raisonne maintenant par induction sur $\varphi$~:
  \begin{itemize}
  \item si $\varphi = X(\bt_1,\ldots,\bt_n)$ alors
    \begin{align*}
      \trad\varphi_\rho^{\sigma[\bx\leftarrow \bt^\sigma]} &= \rho(X)(\bt_1^{\sigma[\bx\leftarrow \bt^\sigma]},\ldots,\bt_n^{\sigma[\bx\leftarrow \bt^\sigma]})\\
      &= \rho(X)((\bt_1[\bt/\bx])^\sigma,\ldots,(\bt_n[\bt/\bx])^\sigma)\\
      &= \trad{\varphi[\bt/\bx]}_\rho^\sigma
    \end{align*}
  \item si $\varphi = \psi \to \chi$ alors
    \begin{align*}
      \trad{\psi\to\chi}_\rho^{\sigma[\bx\leftarrow\bt^\sigma]} &= \{t\in\Lambda\mid \forall u\in\trad\psi_\rho^{\sigma[\bx\leftarrow\bt^\sigma]}, t\;u \in \trad\chi_\rho^{\sigma[\bx\leftarrow\bt^\sigma]}\}\\
      &= \{t\in\Lambda\mid\forall u\in \trad{\psi[\bt/\bx]}_\rho^\sigma, t\;u\in\trad{\chi[\bt/\bx]}_\rho^\sigma\}\\
      &= \trad{\psi[\bt/\bx]\to\chi[\bt/\bx]}_\rho^\sigma\\
      &= \trad{(\psi\to\chi)[\bt/\bx]}_\rho^\sigma
    \end{align*}
  \item si $\varphi = \psi_1\land \psi_2$ alors
    \begin{align*}
      \trad{\psi_1\land\psi_2}_\rho^{\sigma[\bx\leftarrow\bt^\sigma]} &= \{t\in\Lambda\mid \forall i \in\{1,2\}, \pi_i\;t\in \trad{\psi_i}_\rho^{\sigma[\bt\leftarrow\bt^\sigma]}\}\\
      &= \{t\in\Lambda\mid\forall i \in \{1,2\}, \pi_i\;t\in\trad{\psi_i[\bt/\bx]}_\rho^\sigma\}\\
      &= \trad{(\psi\land\chi)[\bt/\bx]}_\rho^\sigma
    \end{align*}
  \item si $\varphi = \forall \by^S, \psi$ alors quitte à renommer $\by$ pour s'assurer que la variable n'appartienne pas aux variables libres de $\bt$~:
    \begin{align*}
      \trad{\forall \by^S,\psi}_\rho^{\sigma[\bx\leftarrow\bt^\sigma]} &= \bigcap_{s\in \bS}\trad{\psi}_\rho^{\sigma[\bx\leftarrow\bt^\sigma,\by\leftarrow s]}\\
      &= \bigcap_{s\in\bS}\trad\psi_\rho^{\sigma[\by\leftarrow s, \bx\leftarrow \bt^\sigma]}\\
      &= \bigcap_{s\in\bS}\trad{\psi[\bt/\bx]}_\rho^{\sigma[\by\leftarrow s]}\\
      &= \trad{\forall \bx, \psi[\bt/\bx]}_\rho^\sigma\\
      &= \trad{(\forall \bx, \psi)[\bt/\bx]}_\rho^\sigma
    \end{align*}
  \item si $\varphi = \forall X^{S_1,\ldots,S_n}, \psi$ alors
    \begin{align*}
      \trad{\forall X^{S_1,\ldots,S_n}, \psi}_\rho^{\sigma[\bx\leftarrow \bt^\sigma]} &= \bigcap_{F : \bS_1\times\cdots\times \bS_n\to \SAT}\trad{\psi}_{\rho[X\leftarrow F]}^{\sigma[\bx\leftarrow \bt^\sigma]}\\
      &= \bigcap_{F : \bS_1\times\cdots\times \bS_n \to \SAT}\trad{\psi[\bt/\bx]}_{\rho[X\leftarrow F]}^\sigma\\
      &= \trad{(\forall X^{S_1,\ldots,S_n}, \psi)[\bt/\bx]}_\rho^\sigma
    \end{align*}
  \item si $\varphi = S(\bu)$, alors $\trad{S(\bu)}_\rho^{\sigma[\bx\leftarrow \bt^\sigma]} = \ceil{\bu^{\sigma[\bx\leftarrow\bt^\sigma]}} = \ceil{(\bu[\bt/\bx])^\sigma}$
  \end{itemize}
  Donc, par induction, $\trad\varphi_\rho^{\sigma[\bx\leftarrow\bt]} = \trad{\varphi[\bt/\bx]}_\rho^\sigma$, d'où le résultat.
\end{proof}

\begin{lem}\label{lem.subst.2}
  Pour tous $\Gamma,\Delta$, si $\Gamma,\bx_1 : S_1,\ldots,\bx_n : S_n\mid\Delta\vdash \psi : \Propo$ alors pour tous $\rho,\sigma$ bien choisis on a, en notant $F : (s_1,\ldots,s_n) \mapsto \trad\psi_\rho^{\sigma[\bx_1\leftarrow s_1,\ldots,\bx_n\leftarrow s_n]}$~:
  \[t\reali_{\rho[X\leftarrow F]}^\sigma \varphi \iff t\reali_\rho^\sigma \varphi[\psi(\bx_1,\ldots,\bx_n)/X]\]
\end{lem}

\begin{proof}
  Par induction sur $\varphi$~:
  \begin{itemize}
  \item si $\varphi = X(\bt_1,\ldots,\bt_n)$, alors
    \begin{align*}
      \trad\varphi_{\rho[X\leftarrow F]}^\sigma &= S(\bt_1^\sigma,\ldots,\bt_n^\sigma) \\
      &= \trad\psi_\rho^{\sigma[\bx_1\leftarrow \bt_1^\sigma,\ldots,\bx_n\leftarrow \bt_n^\sigma]} \\
      &= \trad{\psi[\bt_1/\bx_1,\ldots,\bt_n/\bx_n]}_\rho^\sigma\\
      &= \trad{X(\bt_1,\ldots,\bt_n)[\psi(\bx_1,\ldots,\bx_n)/X]}_\rho^\sigma
    \end{align*}
  \item si $\varphi = Y(\bt_1,\ldots,\bt_p)$ où $X\neq Y$ alors $\trad{\varphi}_{\rho[X\leftarrow F]}^\sigma = \trad\varphi_\rho^\sigma = \trad{\varphi[\psi(\bx_1,\ldots,\bx_n)/X]}_\rho^\sigma$.
  \item si $\varphi = \chi \to \chi'$ alors
    \begin{align*}
      \trad{\chi\to\chi'}_{\rho[X\leftarrow F]}^\sigma &= \{t\in\Lambda\mid \forall u \in \trad\chi_{\rho[X\leftarrow F]}^\sigma, t\;u\in \trad{\chi'}_{\rho[X\leftarrow F]}^\sigma\}\\
      &= \{t\in\Lambda\mid \forall u \in \trad{\chi[\psi(\bx_1,\ldots,\bx_n)/X]}_\rho^\sigma, t\;u\in\trad{\chi'[\psi(\bx_1,\ldots,\bx_n)/X]}_\rho^\sigma\}\\
      &= \trad{\chi[\psi(\bx_1,\ldots,\bx_n)/X] \to \chi'[\psi(\bx_1,\ldots,\bx_n)/X]}_\rho^\sigma\\
      &= \trad{(\chi\to\chi')[\psi(\bx_1,\ldots,\bx_n)/X]}_\rho^\sigma
    \end{align*}
  \item si $\varphi = \forall \bx^S, \chi$ alors
    \begin{align*}
      \trad{\forall \bx^S, \chi}_{\rho[X\leftarrow F]}^\sigma &= \bigcap_{s\in\bS}\trad\chi_{\rho[X\leftarrow F]}^{\sigma[\bx\leftarrow s]}\\
      &= \bigcap_{s\in\bS}\trad{\chi/[\psi(\bx_1,\ldots,\bx_n)/X]}_\rho^{\sigma[\bx\leftarrow s]}\\
      &= \trad{\forall \bx^S, \chi[\psi(\bx_1,\ldots,\bx_n)/S]}_\rho^\sigma\\
      &= \trad{(\forall \bx^S, \chi)[\psi(\bx_1,\ldots,\bx_n)/S]}_\rho^\sigma
    \end{align*}
  \item si $\varphi = \forall Y^{S_1,\ldots,S_p},\chi$ alors
    \begin{align*}
      \trad{\forall Y^{S_1,\ldots,S_p}, \chi}_{\rho[X\leftarrow F]}^\sigma &= \bigcap_{G : \bS_1\times\cdots\times\bS_p \to \SAT}\trad\chi_{\rho[X\leftarrow F, Y\leftarrow G]}^\sigma\\
      &= \bigcap_{G : \bS_1\times\cdots\times\bS_p\to\SAT}\trad\chi_{\rho[Y\leftarrow G, X\leftarrow F]}^\sigma\\
      &= \bigcap_{G : \bS_1\times\cdots\times\bS_p\to\SAT}\trad{\chi[\psi(\bx_1,\ldots,\bx_n)/X]}_{\rho[Y\leftarrow G]}^\sigma\\
      &= \trad{\forall Y^{S_1,\ldots,S_n}, \chi[\psi(\bx_1,\ldots,\bx_n)/X]}_\rho^\sigma\\
      &= \trad{(\forall Y^{S_1,\ldots,S_n}, \chi)[\psi(\bx_1,\ldots,\bx_n)/X]}_\rho^\sigma
    \end{align*}
  \end{itemize}
  Donc, par induction, $\trad\varphi_{\rho[X\leftarrow F]}^\sigma = \trad{\varphi[\psi(\bx_1,\ldots,\bx_n)/X]}_\rho^\sigma$.
\end{proof}

Il nous reste alors à montrer le lemme d'adéquation. Là encore, il nous faut rajouter des notations et de la syntaxe, pour pouvoir relier les différents contextes de typages et logique.

\begin{defi}[Valuation de réalisabilité]
  Soit un contexte de typage du premier ordre $\Gamma$, un contexte de typage du second ordre $\Delta$ et un contexte de typage de termes $\Xi = (x_i : \varphi_i)_{i\in I}$. On dit que des valuations $\sigma, \rho, \nu$ où $\nu : \mathcal X_\Lambda \to \Lambda$ sont adéquates pour $\Gamma,\Delta,\Xi$, ce que l'on note $\nu\reali_\rho^\sigma \Gamma,\Delta,\Xi$, si $\sigma$ est adéquate pour $\Gamma$, $\rho$ est adéquate pour $\Delta$ et si
  \[\forall i \in I, \nu(x_i)\reali_\rho^\sigma\varphi_i\]
\end{defi}

\begin{lem}[Adéquation]
  Soient des contextes $\Gamma,\Delta,\Xi$ et des valuations $\nu\reali_\rho^\sigma \Gamma,\Delta,\Xi$. Soit $t\in\Lambda$ et $\varphi\in\Propo$ tels que $\Gamma\mid\Delta\mid\Xi\vdash t : \varphi$. Alors $\nu(t) \reali_\rho^\sigma \varphi$.
\end{lem}

\begin{proof}
  La preuve se fait par induction sur la relation de typage $\vdash$~:
  \begin{itemize}
  \item Pour le cas d'une variable, il est clair que si $\varphi = \varphi_i$ et $x = x_i$ alors $\nu(t) = \nu(x_i)\reali_\rho^\sigma \varphi_i$.
  \item Supposons que pour tout $\nu \reali_\rho^\sigma \Gamma,\Delta,(\Xi, x : \varphi)$, on a $\nu(t) \reali_\rho^\sigma \psi$. Soit $\nu \reali_\rho^\sigma \Gamma,\Delta,\Xi$, montrons que $\nu(\lambda x.t)\reali_\rho^\sigma \varphi \to \psi$. Soit $u \reali_\rho^\sigma \varphi$, alors $\nu[x \leftarrow u](t)\reali_\rho^\sigma \psi$ puisque $\nu[x\leftarrow u]\reali_\rho^\sigma \Gamma,\Delta,(\Xi, x : \psi)$. De plus, quitte à renommer, on a $\nu[x\leftarrow u](t) = \nu(t)[u/x]$ donc $\nu(t)[u/x]\reali_\rho^\sigma \psi$, donc par saturation $(\lambda x.\nu(t)) u \reali_\rho^\sigma \psi$, donc $\lambda x.\nu(t)\reali_\rho^\sigma \varphi\to\psi$.
  \item Supposons que $\nu(t)\reali_\rho^\sigma \varphi \to \psi$ et $\nu(u)\reali_\rho^\sigma \varphi$, alors par définition $\nu(t\;u)\reali_\rho^\sigma \psi$.
  \item On suppose que pour tout $\nu\reali_\rho^\sigma (\Gamma, \bx : S),\Delta,\Xi$, $\nu(t)\reali_\rho^\sigma \varphi$. Soit alors $\nu\reali_\rho^\sigma \Gamma,\Delta\Xi$ et $s \in \bS$, comme $\nu\reali_\rho^{\sigma[\bx\leftarrow s]} (\Gamma,\bx : S), \Delta,\Xi$, on en déduit que $\nu(t)\reali_\rho^{\sigma[\bx\leftarrow s]} \varphi$, ce qui est valide pour tout $s\in \bS$, donc par définition $\nu(t)\reali_\rho^\sigma \forall \bx^S, \varphi$.
  \item On suppose que $\nu(t)\reali_\rho^\sigma \forall \bx, \varphi$, donc pour tout $\bt : S$, $\sigma(\bt) \in \mathbb S$ donc $\nu(t)\reali_{\rho[\bx\leftarrow \bt^\sigma]}^\sigma \varphi$, c'est-à-dire $\nu(t)\reali_\rho^\sigma \varphi[\bt/\bx]$ par le lemme de substitution du premier ordre.
  \item On suppose que pour tout $\nu\reali_\rho^\sigma \Gamma, (\Delta, X : S_1,\ldots,S_n), \Xi$, $\nu(t) \reali_\rho^\sigma \varphi$. Soit alors des valuations $\nu\reali_\rho^\sigma\Gamma, \Delta,\Xi$. On voit que pour tout $F : S_1\times\ldots\times S_n \to \SAT$, $\nu\reali_{\rho[X\leftarrow F]}^\sigma \Gamma, (\Delta, X : S_1,\ldots,S_n),\Xi$ donc $\nu(t)\reali_{\rho[X\leftarrow F]}^\sigma \varphi$. Comme cela tient pour tout $F : S_1\times\ldots\times S_n \to \SAT$, on en déduit que $\nu(t)\reali_\rho^\sigma \forall X^{S_1,\ldots,S_n}, \varphi$.
  \item Si $\nu(t)\reali_\rho^\sigma \forall X^{S_1,\ldots,S_n}, \varphi$ et $\Gamma,x_1 : S_1,\ldots,x_n : S_n\mid\Delta\vdash \psi : \Propo$, alors en particulier $F : (s_1,\ldots,s_n) \mapsto \trad\psi_\rho^{\sigma[x_1\leftarrow s_1,\ldots, x_n \leftarrow s_n]}$ définit une fonction $S_1\times\ldots\times S_n \to\SAT$, donc $\nu(t)\reali_{\rho[X\leftarrow F]}^\sigma \varphi$, c'est-à-dire $\nu(t)\reali_\rho^\sigma \varphi[\psi(\bx_1,\ldots,\bx_n)/X]$ par le lemme de substitution du second ordre.
  \end{itemize}

  On en déduit par induction le résultat.
\end{proof}

On a donc maintenant notre premier modèle de réalisabilité, qui définit effectivement une théorie
\[\mathcal T_\reali \defeq \{\varphi \in \Propo\mid \exists t \in \Lambda, t \reali \varphi\}\]
qui est stable par déduction naturelle. De plus, comme $\varnothing \in \SAT$, d'après notre interprétation, on en déduit que
\[\trad\bot = \bigcap_{S \in \SAT} S = \varnothing\]
donc $\bot$ n'est pas prouvable par $\mathcal T_\reali$~: c'est une théorie cohérente.

Malheureusement, notre modèle n'a pas beaucoup de principes logique intéressants. Pour une théorie parlant de $\bN$, on pourrait s'attendre à avoir le principe d'induction~: ça n'est pas le cas ici. La raison derrière est l'absence d'accès au contenu calculatoire des arguments. Réaliser une proposition de la forme $\forall x^{\Nat}, \psi(x)$ signifie que l'on peut montrer $\psi(x)$ indépendamment de $x$, donc que la preuve de $\psi$ ne changera pas selon $x$. Au contraire, dans la démonstration par récurrence, en considérant $x\reali \psi(0)$ et $s \reali \psi(n) \to \psi(n+1)$, la démonstration qui en est construite, pour $2$, est $s\;(s\;x)$, quand pour $1$ on a $s\;x$. On a donc besoin d'un moyen d'accéder au contenu calculatoire d'un terme. Pour cela, il nous faut encoder nos éléments par des $\lambda$-termes.

\subsection{Relativisation et codage}

Une façon habituelle de représenter les nombres entiers en $\lambda$-calcul est de considérer les entiers de Church, c'est-à-dire
\[\overline n \defeq \lambda f.\lambda x.f^n\;x\]
on peut montrer que toutes les fonctions calculables $\bN\to\bN$ peuvent se représenter (dans un sens fort) comme des $\lambda$-termes. On a donc une façon de représenter les objets du premier ordre (les entiers) au sein du langage.

Au niveau logique, on va ajouter un prédicat unaire, $\Nat$, qui exprime d'un terme qu'il représente un entier pris avec son contenu calculatoire, c'est-à-dire qu'on devra fournir à $\Nat(\bt)$ une preuve, qui est l'encodage de l'entier correspondant à $\bt$.

Par exemple, reprenons l'exemple de la récurrence. Supposons qu'on possède $x\reali \psi(0)$ et $s\reali \forall \bn, \Nat(\bn) \to \psi(\bn) \to \psi(\bn+1)$. Ici, notre $s$ a besoin de l'information d'à quel $\bn$ il s'applique. Cela signifie en particulier qu'on peut maintenant définir un programme qui, étant donné $\bn$ et son encodage $\overline{\bn}$, peut retourner une preuve de $\psi(\bn)$ en itérant $s$ et en l'appliquant à $x$. On a donc récupéré, depuis notre interprétation par intersection, une interprétation fonctionnelle de la quantification universelle. On peut se demander si la même chose est possible pour une quantification du second ordre~: c'est une idée que nous avons exploré dans ce stage pour avoir diverses version de $\KL$, mais la syntaxe que nous avons finalement adoptée nous permet de ne pas introduire ce genre de quantification supplémentaire. Un prédicat $\psi^{\Nat}$ qui serait possible à représenter par un code serait un prédicat décidable, et avoir des fonctions comme objets du premier ordre nous suffira alors (puisqu'il nous suffira donc de considérer une fonction $\Nat\to\Bool$).

On augmente donc la syntaxe des propositions avec la construction d'encodage de terme~:
\begin{center}
    \begin{prooftree}
        \hypo{\Gamma\vdash \bt : S}
        \infer1{\Gamma\mid\Delta\vdash S(\bt) : \Propo}
    \end{prooftree}
\end{center}

De plus, on définit la relation de réalisabilité pour ce constructeur~:
\[\trad{\Nat(\bt)}_\rho^\sigma \defeq \{t\in \Lambda\mid t \rhd^* \overline{\sigma(\bt)}\}\]
Remarquons que le choix est fait de considérer les termes se réduisant en un entier donné, plutôt que simplement l'entier. Cela se justifie par notre volonté d'avoir des ensembles saturés~: celui-ci est clairement saturé.

Ainsi, pour la preuve d'adéquation, tout fonctionne de la même manière~: il nous suffit de montrer que les lemmes de substitutions sont encore valides. En particulier, comme nous n'avons pas changé les règles de typage, la preuve d'adéquation elle-même ne change pas.

Supposons donc que $\varphi = \Nat(\bu)$ et prouvons le lemme \ref{lem.subst.1}~:
\begin{align*}
    \trad{\Nat(\bu)}_\rho^{\sigma[\bx\leftarrow \sigma(\bt)]} &= \{t \in \Lambda\mid t \rhd^* \overline{\sigma[\bx\leftarrow\sigma(\bt)](\bu)}\}\\
    &= \{ t \in \Lambda\mid t \rhd^* \overline{\sigma(\bu[\bt/\bx])}\}\\
    &= \trad{\Nat(\bu[\bt/\bx])}_\rho^\sigma
\end{align*}

Dans le cas du lemme \ref{lem.subst.2}, le résultat est direct étant donné que l'interprétation de $\Nat(\bt)$ ne dépend pas de l'interprétation du second ordre.

On souhaite en déduire une nouvelle théorie, qui considère la version dite relativisée des quantificateurs~: plutôt que de considérer $\forall \bx^S, \varphi$, on considère $\forall \bx^S, S(\bx) \to \varphi$, ce qu'on notera $\forall \bx^{\{S\}}, \varphi$ pour alléger la lecture. L'adéquation ne correspond alors par tout à fait au fait que la nouvelle théorie définie est close par déduction~: il faut de plus remarquer que si $\Gamma\vdash \bt : \Nat$ alors $\sigma(\bt) \in \bN$ et possède donc un code, ce qui signifie que l'on a bien $\Nat(\bt)$, permettant d'appliquer la règle $\forall_\mathrm e^1$ dans une version relativisée. Une version relativisée de la règle $\forall_\mathrm i^1$, par contre, est juste la concaténation de $\to_\mathrm i$ et $\forall_\mathrm i^1$ dans sa version uniforme.

On possède donc une nouvelle théorie. Pour une proposition du second ordre $\varphi$, on notera $\{\varphi\}$ sa version relativisée. On peut donc définir la nouvelle théorie comme~:
\[\{\mathcal T_\reali\} \defeq \{\varphi \in \Propo\mid \exists t \in \Lambda, t \reali\{\varphi\}\]

Cette théorie a un sens calculatoire bien plus profond~: par exemple, on peut vérifier que dans notre modèle actuel, l'axiome du choix est vérifié (dans un sens relativisé)~:
\[\reali \forall R^{\Nat,\Nat},(\forall \bx^{\{\Nat\}}, \exists \by^{\{\Nat\}}, R(\bx,\by))\to (\exists \bbf^{\{\Nat\to\Nat\}}, \forall \bx^{\{\Nat\}}, R(\bx,\bbf(\bx)))\]
Nous n'avons pas défini la sorte $\Nat\to\Nat$~: on verra plus tard comment parler des fonctions au premier ordre, mais on donne ici une idée informelle de la véracité de l'axiome du choix (dénombrable).

On fixe $R^{\Nat,\Nat}$ et on suppose qu'il existe $r\reali \forall \bx^{\{Nat\}}, \exists \by^{\{\Nat\}}, R(\bx,\by)$~: cela signifie que pour tout code $n$ de $\bx$, on peut trouver un code $m$ d'un certain $\by$ tel que $R(\bx,\by)$~: $r$ définit une fonction calculable qui aux codes de $\bx$ associe les codes de $\by$ correspondant. On considère les deux fonctions $\pi_1,\pi_2$ telles que pour $t \reali \exists \bx^{\{S\}}, \varphi$, $\pi_1\;t\reali S(\bx)$ et $\pi_2\;t\reali \varphi$, alors $\lambda x.\pi_1\;(r\;x)$ code une fonction $\Nat\to\Nat$ et $\lambda x.\pi_2\;(r\;x)$ montre que cette fonction réalise bien ce qu'on attend d'elle.


\begin{comment}
\section{Réalisabilité}

L'élément central de ce stage est la réalisabilité. Nous nous devons donc d'en donner une présentation un peu détaillée. Pour cela, nous allons faire un détour légèrement historique en présentant l'idée à l'origine de ce concept.

\subsection{L'interprétation BHK}



On voudrait alors que la relation $\reali\subseteq \bX \times \Phi$ ainsi construite définisse une théorie
\[\mathcal T_\reali \defeq \{\varphi\in\Phi\mid \exists x \in \bX, x\reali \varphi\}\]

En fait, on voit que cette théorie est close par les règles de la déduction naturelle intuitionnistes sous quelques hypothèses sur la structure de $\bX$~:
\begin{itemize}
\item si on suppose qu'on a des preuves de $\varphi_1,\ldots,\varphi_n$ alors on a évidemment une preuve de $\varphi_i$ pour chaque $i\in\{1,\ldots,n\}$, d'où \begin{center}\begin{prooftree}\hypo{\varphi\in\Gamma}\infer1{\Gamma\vdash \varphi}\end{prooftree}\end{center}
\item si dans le contexte $\varphi_1,\ldots,\varphi_n$ on a une preuve de $\bot$, comme il n'y a pas de preuve de $\bot$ alors par principe d'explosion on déduit qu'on a une preuve de n'importe quelle proposition
\item si dans le contexte où on a des preuves de $\Gamma,\varphi$ on peut construire une preuve de $\psi$, alors on peut considérer la fonction, dans le contexte $\Gamma$, qui à une preuve de $\varphi$ associe la preuve de $\psi$ construite, d'où \begin{center}\begin{prooftree}\hypo{\Gamma, \varphi\vdash \psi}\infer1{\Gamma\vdash\varphi\to\psi}\end{prooftree}\end{center}
\item si dans le contexte $\Gamma$ on a une preuve de $\varphi \to \psi$ et une preuve de $\varphi$, comme une preuve de $\varphi\to\psi$ est une fonction, on peut l'appliquer en la preuve de $\varphi$ pour obtenir une preuve de $\psi$, d'où \begin{center}\begin{prooftree}\hypo{\Gamma\vdash \varphi\to\psi}\hypo{\Gamma\vdash\varphi}\infer2{\Gamma\vdash \psi}\end{prooftree}\end{center}
\end{itemize}

On peut faire de même pour $\land$, $\lor$, $\forall$ et $\exists$. Pour pouvoir construire notre interprétation, il nous faut donc un ensemble $\bX$ muni de structure~:
\begin{itemize}
\item chaque $x\in \bX$ doit pouvoir se voir comme une forme de fonction dont on peut abstraire les entrées, au sens où si $t\in \bX$ possède des entrées $a_1,\ldots,a_n$ alors on veut avoir $\lambda a_i. t\in \bX$ qui est une entrée sur $a_1,\ldots,a_n \setminus a_i$ donnant la fonction $a_i \mapsto t$. En ayant cela, l'introduction et l'élimination de $\to$ devient automatique.
\item on doit être capable de créer des paires $\langle x,y\rangle \in \bX$ d'éléments de $\bX$, et des projections pour récupérer les informations sur chaque coordonnée. Dans ce cas l'introduction et l'élimination de $\land$ est directe.
\item on doit pouvoir représenter au moins un ensemble $\{0,1\}$ pour pouvoir définir l'introduction de $\lor$. Pour l'élimination de $\lor$, il nous faut une instruction if / then / else sur cet ensemble $\{0,1\}$ représenté pour faire une disjonction de cas sur la première coordonnée d'une preuve de $\varphi\lor \psi$.
\item on doit pouvoir représenter les termes du premier ordre au sein même de $\bX$, pour pouvoir considérer une fonction qui à un terme du premier ordre associe un élément de $\bX$ et avoir des paires dont une coordonnée est un terme du premier ordre.
\end{itemize}

Ces conditions donnent une estimation assez fidèle de ce que l'on va rechercher dans un modèle de réalisabilité. On va maintenant voir le premier exemple de réalisabilité, qui est une interprétation comme on l'a définie ici utilisant les fonctions calculables~: la réalisabilité de Kleene.

\subsection{Réalisabilité de Kleene}

Ce modèle de réalisabilité est en fait le point de départ même de la réalisabilité. Il se base sur le fait que pour raisonner sur les entiers, les fonctions calculables donnent une classe de fonctions largement raisonnable. Cependant, on a vu qu'il fallait, en plus de fonctions, pouvoir internaliser les termes du premier ordre. Heureusement, un théorème de calculabilité nous dit qu'il existe une fonction de codage $\varphi$ qui est surjective dans les fonctions calculables (et prend un entier en paramètre).

On considère donc $\bX = \bN$, et on peut vérifier que les conditions sont vérifiées~:
\begin{itemize}
\item si $e$ représente une fonction calculable sur $n + m$ entrées, on peut construire un code $e'$ représentant cette fonction calculable sur $n$ entrées dont les autres $m$ entrées ont été abstraites~: c'est le théorème $S_m^n$. De plus, on a une opération d'application d'une fonction calculable à un argument.
\item la bijection de Cantor nous donne une fonction $(n,m) \mapsto \langle n,m\rangle$ calculable dont les projections sont calculables.
\item l'ensemble $\{0,1\}$ est un sous-ensemble de $\bN$, il est donc évident qu'on peut l'utiliser. Pour la construction if / then / else, c'est une construction élémentaire en programmation, et on peut la combiner à l'égalité à $0$ pour obtenir l'élimination de $\lor$.
\item on peut représenter l'entier $n\in \bN$ dans $\bX$, par $n \in \bN$.
\end{itemize}

On peut donc construire une interprétation de $\bN$ comme un ensemble de preuves pour le langage de l'arithmétique. Il se trouve qu'on obtient alors un modèle de l'arithmétique de Heyting (qui est l'arithmétique de Peano mais dans la logique intuitionniste).

\subsection{Ordre supérieur}

Les conditions décrites plus haut peuvent se simplifier grandement dans le cas où l'on étudie une théorie non pas d'ordre $1$, mais d'ordre $2$ (ou d'ordre supérieur). En effet, il suffit d'introduire une construction $\forall$ portant sur les propositions pour, à partir de la logique minimale (ne contenant que $\to$ et ses règles en plus de la quantification précédemment mentionnée) de coder toute la logique propositionnelle, et les quantifications du premier ordre se limitent à la quantification universelle, la quantification existentielle pouvant s'encoder à partir de celle-ci. Présentons succinctement la syntaxe d'une théorie d'ordre $2$ (avec des termes d'ordre 1).

On ajoute un ensemble de variables du second ordre, qu'on notera par des lettres majuscules, chacune ayant une arité. Une variable $X$ d'arité $n$ représente un prédicat à $n$ variables libres quelconque. On construit alors nos propositions comme pour celle du premier ordre, mais en ajoutant la prise en compte (et la quantification) du deuxième ordre~:
\[\varphi,\psi ::= \cdots\mid X(t_1,\ldots,t_n)\mid \forall^2 X, \varphi\mid \exists^2 X, \varphi\]

Comme dit précédemment, il est possible de largement diminuer le nombre de constructeurs pour nos propositions en encodant par exemple $\land$ et $\lor$ à partir de $\to$ et de $\forall^2$. On définit donc simplement
\[\varphi,\psi ::= X(t_1,\ldots,t_n)\mid \varphi\to\psi\mid \forall^1 x, \varphi\mid \forall^2 X, \varphi\]

On donne l'ensemble des encodages qu'on peut alors faire~:
\begin{itemize}
\item $t = u \defeq \forall X, X(t) \to X(u)$
\item $\varphi\land \psi \defeq \forall X, (\varphi \to \psi \to X) \to X$
\item $\varphi \lor \psi \defeq \forall X, (\varphi \to X) \to (\psi \to X) \to X$
\item $\exists x, \varphi \defeq \forall X, (\forall x, \varphi \to X) \to X$
\item $\exists X, \varphi \defeq \forall Y, (\forall X, \varphi \to Y) \to Y$
\end{itemize}

Ainsi, si l'on arrive à avoir une interprétation de l'ordre $2$, il nous suffit seulement d'interpréter $\to$, $\forall^1$ et $\forall^2$.

\subsection{Intersection et relativisation}

On a vu qu'il était nécessaire de représenter les termes du premier ordre dans $\bX$, mais ceci peut être évité en changeant notre façon de considérer la quantification universelle. Plutôt que de voir $\forall x, \varphi$ comme une fonction, on peut aussi considérer cela comme une intersection. Dans ce cas, il nous faut pouvoir définir ce que signifie $t\reali\varphi$ avec $x\leftarrow n$ pour un certain $n$, ce qui nous pousse à généraliser notre relation $\reali$ avec des contextes. On notera $t\reali^\sigma \varphi$ pour dire que $t$ est une preuve de $\varphi$ dans le cas où $\sigma$ est utilisé pour interpréter $\varphi$. Dans ce cas, $\sigma$ va associer un entier à chaque variable libre dans $\varphi$. On introduit de même un contexte pour les variables du second ordre qui pourraient être libres dans $\varphi$, qu'on notera $\rho$, et où une variable $\bX$ d'arité $n$ est associée à une fonction $\bN^n \to \mathcal P(X)$. On notera alors $t\reali_\rho^\sigma \varphi$ pour noter nos deux contextes.

On arrive alors à une définition alternative de $\reali$ considérant les quantifications comme des preuves uniformes~:
\begin{itemize}
\item $t\reali^\sigma \forall x,\varphi \defeq \forall n \in \bN, (t\reali^{\sigma[x \leftarrow n]} \varphi)$
\item $t\reali_\rho \forall X, \varphi \defeq \forall S : \bN \to \mathcal P(X), (t\reali_{\rho[X\leftarrow S]}\varphi)$
\end{itemize}

C'est avant tout cette interprétation du second ordre qu'on considère, où réaliser une quantification signifie réaliser uniformément toutes les instances possibles.

On voit qu'alors il n'est plus nécessaire d'internaliser les notions du premier (ni même du second) ordre, et il nous suffit alors d'avoir un langage dans lequel on peut abstraire et appliquer des fonctions~: il est dur de ne pas penser au $\lambda$-calcul dans ce contexte, mais nous y reviendrons plus tard.

Il reste cependant souhaitable de pouvoir considérer le $\forall$ comme fonctionnel. En effet, il y a des cas dans lesquels realiser $\varphi(x)$ demande explicitement de savoir de quel $x$ on parle, et le réaliseur peut dépendre directement de ce $x$. L'avantage de l'interprétation uniforme est qu'elle permet de récupérer cela dans un deuxième temps~: il suffit de construire un prédicat dont l'utilité est de récupérer le $x$ que l'on veut. Dans le cas des entiers, on peut construire un prédicat $\bN(x)$ tel que $n\reali_\rho^\sigma\bN(x)$ exactement quand $x$ est interprété dans $\sigma$ par $n$. Dans ce cas, écrire $\forall x, \bN(x) \implies \varphi$ nous dit exactement que pour n'importe quel $n$, obtenir l'information de ce $n$ permet de prouver $\varphi$.

On a donc deux sortes de quantifications~: les quantifications uniformes et les quantification relativisées, l'une considérant l'intersection et l'autre l'espace fonctionnel.

En fait, on verra que la notion de relativisation peut se voir comme un cas particulier d'une construction catégorique plus générale (la construction \textit{tripos to topos} dans le cas d'un tripos de réalisabilité).

\subsection{Lambda-calcul et saturation}

On a mentionné plus tôt qu'il était naturel, dans le contexte donné, de considérer le $\lambda$-calcul comme candidat pour faire un modèle de réalisabilité. En reprenant ce qui a été dit plus tôt, en considérant comme langage l'arithmétique du second ordre, on définit donc une interprétation en utilisant pour $X$ un $\lambda$-calcul, qu'on prendra donc le plus simple possible pour l'instant.

\begin{defi}
  On se fixe un ensemble dénombrable $\mathcal X_\Lambda$ de variables de $\lambda$-termes. On définit l'ensemble $\Lambda$ des $\lambda$-termes par la grammaire~:
  \[t,u ::= x\mid \lambda x.t\mid t\;u\]
  où $x\in\mathcal X_\Lambda$. On définit de la façon habituelle l'$\alpha$-conversion, la substitution simultannée $t[t_1/x_1,\ldots,t_n/x_n]$ et la $\beta$-réduction.
\end{defi}

On donne donc une première tentative de définition de $t\reali_\rho^\sigma \varphi$ où $\rho$ associe toute variable libre du premier ordre de $\varphi$ à une fonction $\bN^n \to \mathcal P(\Lambda)$ et $\sigma$ associe toute variable libre du premier ordre de $\varphi$ à un entier $n \in \bN$. On utilisera la notation $\trad\varphi_\rho^\sigma \defeq \{t\in\Lambda\mid t \reali_\rho^\sigma \varphi\}$~:
\begin{itemize}
\item $\trad{X(t_1,\ldots,t_n)}_\rho^\sigma \defeq \rho(X)(t_1^\sigma,\ldots,t_n^\sigma)$
\item $\trad{\varphi \to \psi}_\rho^\sigma\defeq \{t\in \Lambda \mid \forall u\reali_\rho^\sigma \varphi, t\;u\reali_\rho^\sigma \psi\}$
\item $\trad{\forall x, \varphi}_\rho^\sigma \defeq \bigcap_{n \in \bN}\trad\varphi_\rho^{\sigma[x\leftarrow n]}$
\item $\trad{\forall X, \varphi}_\rho^\sigma \defeq \bigcap_{S : \bN^n \to \mathcal P(\Lambda)}\trad\varphi_{\rho[X\leftarrow S]}^\sigma$
\end{itemize}

Tout semble bien fonctionner, mais il y a en fait un problème~: notre interprétation précédente considérait $X$ comme un objet sémantique et non syntaxique. On entend par cela que dans $\Lambda$, il y a une différence entre $(\lambda x.t)u$ et $t[u/x]$ qui sont des termes différents, mais sont dans la même classe de $\Lambda/=_\beta$. On pourrait donc décider de ne considérer non pas $\Lambda$ mais $\Lambda/=_\beta$, mais il existe une façon plus élégante de régler ce problème.

L'endroit où cette distinction apparait est dans la preuve que les règles de la déduction naturelle intuitionniste sont vérifiées par notre interprétation de réalisabilité, plus précisément dans la règle d'introduction de $\to$. En considérant un système de types associant une proposition à un terme, de la forme $x_1 : \varphi_1,\ldots,x_n : \varphi_n \vdash t : \varphi$. Dans ce cas, la validité d'un tel séquent est
\[t_1\reali_\rho^\sigma \varphi_1,\ldots,t_n \reali_\rho^\sigma \varphi_n \implies t[t_1/x_1,\ldots, t_n/ x_n]\reali_\rho^\sigma \varphi\]
et on peut réécrire $\to_\mathrm i$ par la règle
\begin{center}
  \begin{prooftree}
    \hypo{\Gamma, x : \varphi\vdash t : \psi}
    \infer1{\Gamma\vdash \lambda x.t : \varphi \to \psi}
  \end{prooftree}
\end{center}

On veut donc unifier d'un côté $t[t_1/ x_1,\ldots,t_n/ x_n, u/ x]$ et de l'autre $(\lambda x.t[t_1/x_1,\ldots,t_n/x_n])u$, ce qui (à des questions d'$\alpha$-conversion près) est une anti-réduction~: le second terme se réduit en le premier terme.

Ainsi la validité de cette règle a besoin d'un élément supplémentaire qui peut être satisfait par la condition suivante~: il faut qu'à chaque $\varphi$, si $t\reduc u$ et $u\reali \varphi$ alors $t\reali \varphi$. En fait, on peut montrer qu'il suffit pour cela d'assurer cette condition sur les $\rho(X)$. Donnons donc un peu de vocabulaire.

\begin{defi}[Partie saturée]
  Soit $S\subseteq\Lambda$, on dit que $S$ est saturée si la propriété suivante est vérifiée~:
  \[\forall t,u\in\Lambda, u \in S \land t \reduc u \implies t \in S\]

  On note
  \[\SAT \defeq \{S\subseteq \Lambda\mid S\;\text{est saturée}\}\]
\end{defi}

Un lemme qui se prouve de façon très simple est utile ici, puisqu'il nous assure que toutes les constructions à base d'intersection et/ou d'union vont se comporter correctement vis à vis de la saturation.

\begin{lem}
  $(\SAT,\subseteq,\Lambda,\varnothing,\bigcap,\bigcup)$ est un treillis complet.
\end{lem}

\begin{proof}
  Il est évident que $S\in \SAT \implies \varnothing\subseteq S \subseteq \Lambda$ et que ces deux parties sont stables par anté-réduction. Supposons que $\{S_i\}_{i\in I}$ est une famille de parties saturées. Si $t\reduc u$ et $u \in \bigcap S_i$, alors pour tout $i \in I$, $t \in S_i$ par saturation de $S_i$, donc $t\in \bigcap S_i$, donc $\bigcap S_i$ est saturée. De même si $u \in \bigcup S_i$, alors on trouve $i$ tel que $u \in S_i$ donc $t\in S_i$ par saturation. Donc $\bigcup S_i$ est saturée. Ainsi $(\SAT,\subseteq,\Lambda,\varnothing, \bigcap,\bigcup)$ est un treillis complet.
\end{proof}

On décide maintenant de modifier l'interprétation au-dessus en disant juste que $\rho(X) : \bN^n \to \SAT$, et de même que l'intersection pour le $\forall X, \varphi$ est définie sur $S : \bN^n \to \SAT$. On peut alors montrer le lemme de saturation.

\begin{lem}[Saturation]
  Pour toute formule $\varphi$, $\trad\varphi_\rho^\sigma\in \SAT$.
\end{lem}

\begin{proof}
  On procède par induction sur $\varphi$~:
  \begin{itemize}
  \item si $\varphi = X(t_1,\ldots,t_n)$ alors $\trad\varphi_\rho^\sigma=\rho(X)(t_1^\sigma,\ldots,t_n^\sigma)\in \SAT$.
  \item si $\varphi = \psi \to \chi$, supposons que $t\reali_\rho^\sigma \psi \to \chi$, $t'\reduc t$ et $u\reali_\rho^\sigma\psi$. Par hypothèse, $t\;u\reali_\rho^\sigma \chi$, et comme par hypothèse d'induction $\trad\chi_\rho^\sigma\in\SAT$ et $t'\;u\reduc t\;u$, on en déduit que $t'\;u\reali_\rho^\sigma \chi$. Ainsi pour tout $u\reali_\rho^\sigma\psi$, $t'\;u\reali_\rho^\sigma\chi$, ce qui signifie que $t'\reali_\rho^\sigma\psi\to\chi$.
  \item les deux interprétations données par des intersections donnent des ensembles saturés car $\SAT$ est un treillis complet.
  \end{itemize}
  Donc par induction $\trad\varphi_\rho^\sigma\in\SAT$.
\end{proof}

Ici, on voit que notre lemme dépend assez directement de la réduction qui est utilisée. C'est en effet selon le choix de notre réduction que notre choix de quelles parties sont sémantiquement significatives doit se faire. Dans ce modèle simple avec le $\lambda$-calcul et la réduction cbn, les définitions sont simples, mais le passage sur la saturation peut être grandement modifié selon les changements qu'on apporte à la sémantique opérationnelle de notre langage.

\subsection{Le lemme d'adéquation}

En utilisant notre modèle simplifié basé sur le $\lambda$-calcul, nous allons montrer le lemme d'adéquation, qui a deux utilitées principales. La première est de montrer que la théorie $\mathcal T_\reali$ est bien une théorie au sens où elle est close par déduction logique (c'est-à-dire que les règles de la déduction naturelle sont valides dans cette théorie). La deuxième est de donner une façon syntaxique de construire des preuves de réalisabilité.

\subsection{Conclusion sur la réalisabilité}

On peut déjà construire un premier modèle de réalisabilité, simple, basé sur le $\lambda$-calcul.

\section{Mathématiques à rebours}

L'autre part importante de ce stage se trouve dans les mathématiques à rebours. Par mathématiques à rebours, on désigne un domaine, lié à la calculabilité, dont l'objectif est de quantifier la force logique des théorèmes usuels des mathématiques. Nous allons donc présenter les éléments importants de ce domaine qui sont utilisés dans le stage.

\subsection{Présentation globale}

La première question à se poser, pour organiser les théorèmes suivant leur force logique, est \textit{comment peut-on décider si un théorème est plus fort qu'un autre ?} Il est assez clair qu'on s'attend à un pré-ordre tout sauf total (mais à un pré-ordre quand même). L'idée la plus simple est de dire que $\varphi$ est un théorème plus fort que $\psi$ si $\varphi \vdash \psi$, donc si $\varphi$ est une proposition plus faible pour l'ordre de prouvabilité que $\psi$ (c'est assez logique~: être un théorème très fort, c'est être un théorème dont la preuve implique un maximum de choses). Le souci, maintenant, est que si on considère pour $\vdash$ la relation de prouvabilité dans $\ZF$, alors la plupart des résultats sont vrais. On conviendra que le pré-ordre $(\{*\},=)$ n'est pas le plus intéressant, il nous faut donc quelque chose de plus précis.

Si un théorème fort est un théorème qui entraine plus de résultats avec sa vérité, et comme une théorie est un ensemble de résultats pris pour axiomes, on peut donc s'attendre à ce qu'une théorie plus faible prouve moins de résultats directement, et laisse donc plus de marge pour séparer des résultats. Cependant, la théorie doit être suffisamment forte pour rester expressive, et prouver ce qu'on considère comme le plus élémentaire. Par exemple dire que l'addition est commutative dans $\bN$ n'est pas un résultat qu'on souhaite placer dans notre ordre.

Le choix se porte alors sur l'arithmétique du second ordre, plus précisément sur un sous-système appelé RCA$_0$. Ce sous-système contient suffisamment pour parler des réels et des fonctions continues (même des fonctions mesurables). C'est donc un système très expressif, relativement à sa faible capacité à prouver des résultats.

\subsection{Le cas intuitionniste et l'indépendance relative}

Comme on l'a dit, plus la théorie de base est faible, plus on est précis dans le pré-ordre intuitif de la force logique. Malheureusement, ça n'est pas toujours ce que l'on souhaite. Par exemple, l'une des questions de théorie des ensembles les plus importantes de l'histoire a porté sur un résultat indépendant de ZFC, l'hypothèse du continu. Dans ce cas, prouver que le résultat est indépendant de ZFC est un meilleur résultat que celui que l'hypothèse du continu est indépendant de ZF.

Syntaxiquement, cela se voit par le fait qu'une preuve de l'hypothèse du continu dans ZFC pourrait se formuler dans ZF, donc l'un implique l'autre. Sémantiquement, cela signifie qu'on peut trouver un modèle validant HC et un validant $\lnot$HC dans une classe de modèles encore plus restreinte que celle des modèles de ZF~: celle des modèles de ZFC. Dans les deux cas, on comprend qu'une théorie plus forte donne un meilleur résultat d'indépendance.

Dans le cas intuitionniste, cette volonté de prouver des cas d'indépendance est récurrente, et il est donc bon de se placer dans un cadre plus fort. En particulier, la version intuitionniste de ZF (pas de ZFC, car le tiers exclu est vrai dans ZFC) nous donne de meilleurs résultats d'indépendance.

\subsection{Les formes faibles de l'axiome du choix}

L'axiome du choix, connu pour être un axiome indépendant de ZF et pour avoir été débattu dans le choix de l'accepter ou non (en raison par exemple de son caractère non constructif), peut se décliner en beaucoup de version faibles. Nous allons en présenter plusieurs, dans l'ordre croissant de leur force logique en logique classique~:
\begin{itemize}
\item \textit{Fan Theorem} (FT)~: supposons qu'on ait un prédicat $P$ sur $\bN^*$ qui est clos par extension, c'est-à-dire que si $u\in P$ et $u \preceq v$ alors $v\in P$. Supposons que pour tout chemin infini $\alpha\in\bN^\bN$ il existe un nombre $n$ tel que $\alpha_{0,\ldots,n}\in P$, alors il existe $n \in\bN$ tel que pour tout $\alpha \in \bN^\bN$, $\alpha_{0,\ldots,n}\in P$.
\item \textit{Weak König's Lemma} (WKL)~: tout arbre binaire infini possède une branche infinie.
\item \textit{König's Lemma} (KL)~: tout arbre infini mais finiment branchant (pour tout n\oe ud il existe un nombre fini de n\oe uds qui en sont voisins) possède une branche infinie.
\item \textit{Axiome du choix dénombrable} (ACN)~: pour toute famille $(X_i)_{i\in \bN}$ d'ensembles non vides, il existe une fonction $\alpha : \bN \to \bigcup X_i$ telle que $\alpha(i)\in X_i$.
\item \textit{Axiome du choix dépendant} (DC)~: soit un ensemble $X$ et une relation binaire $R$ sur $X$ telle que pour tout $x\in X$, il existe $y\in X$ tel que $R(x,y)$, alors il existe une suite $\alpha : \bN \to X$ telle que $\forall i\in \bN, R(\alpha(i),\alpha(i+1))$.
\item \textit{Axiome du choix} (AC)~: toute surjection admet une section.
\end{itemize}

On sait que dans le cas classique, cette hiérarchie est strictement croissante, sauf pour FT qui est équivalent à WKL. Dans le cas intuitionniste, plus faible, la hiérarchie devient stricte aussi entre FT et WKL. Par contre, elle n'est pas croissante, puisqu'on peut par exemple avoir DC sans avoir KL.

En fait, on peut distinguer deux principes importants dans les premières versions~: l'omniscience et le choix. Dans le cas de DC, on peut construire notre suite simplement en considérant la suite jusqu'à un rang $n$ et en appliquant un oracle permettant de trouver un élément en relation avec le dernier construit. On passe donc simplement d'une fonction locale à une fonction globale par itération. Dans le cas de KL, il nous faut traiter l'infinité de données des chemins potentiels pour trouver le sommet suivant dans le chemin, en pouvant savoir à l'avance de quel côté se trouve un chemin infini. En logique classique, l'omniscience est toujours vérifiée, mais pas en intuitionniste, d'où cette cassure de monotonie au passage de KL à ACN.

\section{Plus de généralité pour la réalisabilité}

Nous avons abordé pour l'instant la réalisabilité sous un angle assez empirique~: étant donné un $\lambda$-calcul, on peut essayer en vérifiant des conditions assez intuitives de construire un modèle de réalisabilité, et de voir la théorie en résultant. Prendre une approche plus systématique et abstraire le système de calcul en lui-même va nous permettre d'aller plus loin que cet empirisme.

\subsection{Les effets sont logiques}

Une branche importante de la réalisabilité, la réalisabilité à le Krivine (ou réalisabilité classique), naît de l'observation que l'instruction call/cc, qui est une instruction de programmation permettant par exemple d'utiliser des exceptions, peut être typée par la loi de Peirce, principe équivalent au tiers exclu. Comme les exceptions permettent en quelque sorte de revenir en arrière dans l'exécution d'un programme, ce typage donne alors à penser que la capacité de revenir en arrière permet de passer de la logique intuitionniste à la logique classique. On peut aussi voir ça en terme de jeux en considérant que la logique intuitionniste demande à un défendant de gagner face à une série de contradictions en étant toujours cohérent avec lui-même, là où la logique classique premet au défendant de changer d'avis sur un argument en redéfendant cet argument du début.

Cela illustre un principe central de la réalisabilité~: ajouter des effets au langage de programmation donne dans le modèle de réalisabilité des formules logiques qui peuvent devenir vraies. Jean-Louis Krivine a alors pu utiliser ce typage pour construire une version de la réalisabilité dans laquelle la théorie $\mathcal T_\reali$ est une théorie classique.

On peut donc naturellement se demander comment classer les effets selon les théorèmes qui en découlent. Pour cela, il devient important de trouver un cadre plus uniforme pour aborder la réalisabilité, puisqu'il est difficile de traiter d'abstraction sans le faire dans un cadre défini. Le cadre trouvé dans le cas de la réalisabilité est celui des catégories.

\subsection{Tripos de réalisabilité}

Les catégories nous permettent d'étudier la logique, en particulier avec la notion de catégorie fibrée. Celle-ci permet de définir la notion d'avoir deux catégories où l'une exprime un langage parlant de l'autre catégorie, ou de construire une famille de catégories indicée par une autre catégorie.

Dans le cas de la pur logique propositionnelle (intuitionniste), la structure algébrique associée est celle des pré-algèbres de Heyting, qui correspond catégoriquement aux catégories ordonnées cartésiennes fermées. Pour en faire de la logique parlant plus explicitement d'une certaine catégorie (disons des ensembles), on considère une fibration au-dessus de cette catégorie, dont les fibres sont des pré-algèbres de Heyting.

Nous allons donc voir les définitions importantes pour manipuler des catégories fibrées.

\begin{defi}[Flèche cartésienne]
  Soient $\bE,\bB$ deux catégories et $p : \bE \to \bB$ un foncteur. On dit qu'une flèche $u : X \to Y$ dans $\bE$ est cartésienne si la propriété suivante est vérifiée~: pour toute flèche $v : Z \to Y$ et $k : pZ \to pX$ il existe une unique flèche $w : Z \to X$ telle que $v = u\circ w$. Cela se résume par le diagramme suivant~:
\end{defi}

\begin{defi}[Catégorie fibrée]
  Une catégorie fibrée est la donnée de deux catégories $\bE,\bB$ et d'un foncteur $p : \bE \to \bB$ satisfaisant la propriété suivante~: pour toute
\end{defi}

\section{Boîte à outils réalisable}

\subsection{Un modèle de HA2 amélioré}

On commence par donner une présentation d'une version enrichie de l'arithmétique de Heyting du second ordre (HA2) avec un modèle de réalisabilité basé sur le lambda-calcul.

\subsubsection{Langage de programmation}

Le $\lambda$-calcul est connu pour permettre de représenter n'importe quelle fonction calculable. A ce titre, on pourrait considérer le $\lambda$-calcul pur non typé pour réaliser des propositions. Cependant, pour la même raison, nous allons adopter un $\lambda$-calcul possédant plus de constructeurs~: tant qu'à pouvoir coder tout ce que l'on souhaite, autant se donner directement des primitives pour manipuler les éléments qui nous importent.

\begin{defi}[$\Lambda$]
  On se donne un ensemble $\mathcal X_\Lambda$ de variables de termes. On définit l'ensemble des $\lambda$-termes, $\Lambda$, par la grammaire suivante~:
  \[t,u ::= x\mid\lambda x. t\mid t\;u\mid \langle t,u\rangle\mid \pi_1\;t\mid \pi_2\;t\mid Y \mid 0\mid S\mid \rec_\bN \mid \btt\mid\bff\mid\rec_\bB\mid [\;]\mid t:: u\mid \rec_\bL\]
\end{defi}

\begin{defi}[Contexte]
  On définit l'ensemble $\Lambda_{\bnnbr{\;}}$ des contextes, définit récursivement par~:
  \[E,F::= \bnnbr{\ }\mid \lambda x.E\mid E\;t\mid t\;E\mid \langle E,t\rangle\mid \langle t,E\rangle\]

  Si $C$ est un contexte et $t$ un $\lambda$-terme, on note $C\bnnbr{t}$ le terme obtenu en substituant $\bnnbr{}$ par $t$ dans l'écriture de $C$.
\end{defi}

On définit de façon usuelle la substitution ainsi que la relation d'$\alpha$-équivalence, modulo laquelle on travaillera à partir de maintenant.

\begin{defi}[Réduction]
  On définit la relation $\mapsto\subseteq \Lambda\times\Lambda$ par les règles suivantes~:
  \begin{center}
    \begin{prooftree}
      \infer0{(\lambda x.t)u\mapsto t[u/x]}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \infer0{\forall i\in\{1,2\}, \pi_1\;\langle t_1,t_2\rangle \mapsto t_i}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \infer0{Y\;t \mapsto t\;(Y\;t)}
    \end{prooftree}

    \vspace{0.5cm}
    
    \begin{prooftree}
      \infer0{\rec_\bN\;t\;u\;0\mapsto t}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \infer0{\rec_\bN\;t\;u\;(S\;v)\mapsto u\;v\;(\rec_\bN\;t\;u\;v)}
    \end{prooftree}

    \vspace{0.5cm}
    
    \begin{prooftree}
      \infer0{\rec_\bB\;t\;u\;\btt\mapsto t}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \infer0{\rec_\bB\;t\;u\;\bff\mapsto u}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \infer0{\rec_\bL\;t\;u\;[\;]\mapsto t}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \infer0{\rec_\bL\;t\;u\;(v::w)\mapsto u\:v\:w\:(\rec_\bL\:t\:u\:w)}
    \end{prooftree}
  \end{center}

  On définit alors la relation $\reduc$ par
  \[t\reduc u \defeq \exists C \in\Lambda_{\bnnbr{\;}},\exists t'\;u'\in\Lambda, t = C\bnnbr{t'}\land u = C\bnnbr{u'}\land t' \mapsto u'\]
\end{defi}

On introduit la notion d'ensemble saturé, qui peut se considérer comme une partie de $\Lambda$ qui est calculatoirement pertinente.

\begin{defi}[Partie saturée]
  Soit $S\subseteq\Lambda$, on dit que $S$ est saturée si la propriété suivante est vérifiée~:
  \[\forall t,u\in\Lambda, u \in S \land t \reduc u \implies t \in S\]

  On note
  \[\SAT \defeq \{S\subseteq \Lambda\mid S\;\text{est saturée}\}\]
\end{defi}

On définit de plus un sous-système de $\Lambda$, typé, qui est une variante de PCF.

\begin{defi}[$\LamP$]
  On définit l'ensemble des types de $\LamP$ par
  \[S,S' ::= \Nat\mid\Bool\mid\List\mid S \to S' \mid S \times S'\]
  Les règles de typage sont les suivantes~:
  \begin{center}
    \begin{prooftree}
      \infer0{\Gamma, x : S \vdash x : S}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma, x : S \vdash t : S'}
      \infer1{\Gamma\vdash \lambda x.t : S \to S'}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash t : S \to S'}
      \hypo{\Gamma\vdash u : S}
      \infer2{\Gamma\vdash t\;u : S'}
    \end{prooftree}

    \vspace{0.5cm}

    \begin{prooftree}
      \hypo{\Gamma\vdash t : S}
      \hypo{\Gamma\vdash u : S'}
      \infer2{\Gamma\vdash \langle t,u\rangle S \times S'}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash t : S_1\times S_2}
      \infer1{\Gamma\vdash \pi_i\;t : S_i}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash t : S \to S}
      \infer1{\Gamma\vdash Y\;t : S}
    \end{prooftree}

    \vspace{0.5cm}

    \begin{prooftree}
      \infer0{\Gamma\vdash 0 : \Nat}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash t : \Nat}
      \infer1{\Gamma\vdash S\;t : \Nat}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash t : S}
      \hypo{\Gamma\vdash u : \Nat \to S \to S}
      \hypo{\Gamma\vdash v : \Nat}
      \infer3{\Gamma\vdash \rec_\bN\;t\;u\;v}
    \end{prooftree}

    \vspace{0.5cm}

    \begin{prooftree}
      \infer0{\Gamma\vdash \btt : \Bool}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \infer0{\Gamma\vdash \bff : \Bool}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash t : S}
      \hypo{\Gamma\vdash u : S}
      \hypo{\Gamma\vdash v : \Bool}
      \infer3{\Gamma\vdash \rec_\bB\;t\;u\;v : S}
    \end{prooftree}

    \vspace{0.5cm}
    
    \begin{prooftree}
      \infer0{\Gamma\vdash [\:] : \List}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash t : \Nat}
      \hypo{\Gamma\vdash u : \List}
      \infer2{\Gamma\vdash t :: u : \List}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash t : S}
      \hypo{\Gamma\vdash u : \Nat \to \List \to S \to S}
      \hypo{\Gamma\vdash v : \List}
      \infer3{\Gamma\vdash \rec_\bL\;t\;u\;v : S}
    \end{prooftree}
  \end{center}

  Les règles de réduction sont celles induites par $\Lambda$. On définit aussi $\SATP$ comme l'ensemble des parties saturées de $\LamP$.

  Pour chaque type $S$, on définit
  \[\LamS{S}\defeq \{t\in\LamP\mid \vdash t : S\}\]
\end{defi}

\subsubsection{Partie logique}

On définit maintenant la partie logique de notre modèle de réalisabilité. Celui-ci est un modèle multi-sorté, dont les sortes sont données par les types de $\LamP$,
représentant respectivement les entiers, les booléens, les listes d'entiers et les fonctions d'une sorte à une autre. On notera $\Sort$ l'ensemble des sortes. On définit maintenant les termes du premier ordre (qui sont donc typés).

\begin{defi}[Termes du premier ordre]
  On se donne un ensemble dénombrable $\mathcal X_1$ de variables du premier ordre. Un contexte de typage du premier ordre $\Gamma$ est une liste de paires $(x,\tau)\in \mathcal X_1\times \Sort$. On définit l'ensemble des termes du premier ordre bien typés par les règles suivantes~:
  \begin{center}
    \begin{prooftree}
      \infer0{\Gamma, \bx : S \vdash \bx : S}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \infer0{\Gamma\vdash \bZ : \Nat}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash \bt : \Nat}
      \infer1{\Gamma\vdash \bfS\;\bt : \Nat}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash \bt : S}
      \hypo{\Gamma\vdash \bu : \Nat \to S \to S}
      \hypo{\Gamma\vdash \bv : \Nat}
      \infer3{\Gamma\vdash \rec_{\Nat}\;\bt\;\bu\;\bv : S}
    \end{prooftree}

    \vspace{0.5cm}

    \begin{prooftree}
      \infer0{\Gamma\vdash \bbtt : \Bool}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \infer0{\Gamma\vdash \bbff : \Bool}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash \bt : \Bool}
      \hypo{\Gamma\vdash \bu : S}
      \hypo{\Gamma\vdash \bv : S}
      \infer3{\Gamma\vdash \rec_{\Bool}\;\bt\;\bu\;\bv : S}
    \end{prooftree}

    \vspace{0.5cm}
    
    \begin{prooftree}
      \infer0{\Gamma\vdash \bnil : \List}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash \bt : \Nat}
      \hypo{\Gamma\vdash \bu : \List}
      \infer2{\Gamma\vdash \bt \bcons \bu : \List}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash \bt : S}
      \hypo{\Gamma\vdash \bt : \Nat \to \List \to S \to S}
      \hypo{\Gamma\vdash \bv : \List}
      \infer3{\Gamma\vdash \rec_{\List}\;\bt\;\bu\;\bv : S}
    \end{prooftree}

    \vspace{0.5cm}

    \begin{prooftree}
      \hypo{\Gamma, \bx : S \vdash \bt : S'}
      \infer1{\Gamma\vdash \blam \bx.\bt : S \to S'}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash \bt : S \to S'}
      \hypo{\Gamma\vdash \bu : S}
      \infer2{\Gamma\vdash \bt\;\bu : S'}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash \bt : S}
      \hypo{\Gamma\vdash \bu : S'}
      \infer2{\Gamma\vdash \langle \bt,\bu \rangle : S \times S'}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\vdash \bt : S_1 \times S_2}
      \infer1{\Gamma\vdash \bpi_i\;\bt : S_i}
    \end{prooftree}
  \end{center}
\end{defi}

\begin{rmk}
  Pour tout terme $\bt : S$ du premier ordre, il existe canoniquement un terme $\hat{\bt} : S$ dans $\LamP$ (l'ajout du constructeur $Y$ empêche d'avoir la réciproque).
\end{rmk}

On peut maintenant définit les formules de notre HA2 enrichi.

\begin{defi}[Propositions]
  On se donne un ensemble $\mathcal X_2$ de variables du second ordre. Un contexte de typage du second ordre $\Delta$ est une liste de paires $(X,\alpha) \in \mathcal X_2\times \List(\{\bN,\bB,\bL\})$. Une proposition du second ordre est un objet bien typé par les règles suivantes~:
  \begin{center}
    \begin{prooftree}
      \hypo{(X : A_1,A_2,\ldots,A_n)\in\Delta}
      \hypo{\forall i \in \{1,\ldots,n\}, \Gamma\vdash \bt_i : A_i}
      \infer2{\Gamma\mid\Delta\vdash X(\bt_1,\ldots,\bt_n) : \Propo}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\mid\Delta\vdash \varphi : \Propo}
      \hypo{\Gamma\mid\Delta\vdash \psi : \Propo}
      \infer2{\Gamma\mid\Delta\vdash \varphi \land \psi : \Propo}
    \end{prooftree}

    \vspace{0.5cm}
    
    \begin{prooftree}
      \hypo{\Gamma\mid\Delta\vdash \varphi : \Propo}
      \hypo{\Gamma\mid\Delta\vdash \psi : \Propo}
      \infer2{\Gamma\mid\Delta\vdash \varphi \to \psi : \Propo}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma, x : A\mid \Delta\vdash \varphi : \Propo}
      \infer1{\Gamma\mid\Delta\vdash\forall x^A, \varphi : \Propo}
    \end{prooftree}
    \quad
    \begin{prooftree}
      \hypo{\Gamma\mid \Delta, X : A_1,\ldots,A_n\vdash \varphi : \Propo}
      \infer1{\Gamma\mid\Delta\vdash \forall X^{A_1,\ldots,A_n}, \varphi : \Propo}
    \end{prooftree}

    \vspace{0.5cm}
    
    \begin{prooftree}
      \hypo{\Gamma\vdash \bt : S}
      \infer1{\Gamma\mid\Delta\vdash S(\bt) : \Propo}
    \end{prooftree}
  \end{center}
\end{defi}

L'idée des prédicats de la forme $S(\bt)$ avec une sorte $S$ est de donner un témoin du fait que le terme est un terme standard.

\begin{nota}
  Pour rendre les propositions plus lisibles, on adoptera la convention que, plutôt que d'écrire $\forall x^A, A(x) \to \varphi$, on notera directement $\forall x^{\{A\}}, \varphi$. Ce procédé de conditionner par $A(x)$ sera appelé ici relativisation. On définit de même la version relativisée de $\exists$.
\end{nota}

\subsubsection{Relation de réalisabilité}

On définit par induction sur la structure des sortes une interprétation ensembliste $\trad S$ (aussi notée $\bS$) pour chaque $S$, et pour chaque $x \in \bS$ un ensemble (possiblement vide) $\ceil x\in \SATP$ de codes de $x$~:
\begin{itemize}
\item $\trad{\Nat} = \bN$, et $\ceil n = \{t\in\LamS{\Nat}\mid t \reduc^* S^n\;0\}$.
\item $\trad{\Bool} = \{0,1\}$, $\ceil{0} = \{t\in\LamS{\Bool}\mid t \reduc^*\bff\}$ et $\ceil 1 = \{t\in\LamS{\Bool}\mid t \reduc^*\btt\}$.
\item $\trad{\List} = \bN^*$, $\ceil{\varepsilon} = \{t\in\LamS{\List}\mid \vdash t \reduc^*[\:]\}$ et $\ceil{u\star a} = \{t \in \LamS{\List}\mid \exists t',t'' \in \LamP, t' \in \ceil a, t'' \in \ceil u, t \reduc^* t' :: t''\}$
\item $\trad{S \to S'} = \{f : \trad S \to \trad{S'}\}$ et $\ceil f = \{t\in \LamS{S\to S'}\mid \forall s \in \trad S, \forall e \in \ceil s, t\;e \in \ceil{f(s)}\}$
\end{itemize}

\begin{lem}
  Pour toute sorte $S$ et $s\in \bS$, $\ceil s \in \SAT$.
\end{lem}

\begin{proof}
  Par induction sur $S$~:
  \begin{itemize}
  \item pour le cas de $\Nat,\Bool,\List$ le résultat est directement dû au fait que la relation $\reduc^* t$ pour $t\in \Lambda$ est close par antéréduction.
  \item si $t\reduc u$ et $u\in \ceil f$, alors pour tout $s\in \bS, e \in \ceil s$ on sait que $t\;e\reduc u\;e \in \ceil{f(s)}$, donc par induction (comme tout ensemble de codes d'élément de $S'$ est saturé) on en déduit que $t\;e \in \ceil{f(s)}$, donc que $t\in \ceil f$, ainsi $\ceil f \in \SAT$.
  \end{itemize}
  D'où la preuve par induction.
\end{proof}

On définit aussi les notions de valuations du premier et du second ordre.

\begin{defi}[Valuation]
  Une valuation du premier ordre est une fonction $\sigma : \mathcal X_1 \to \bigcup_{S \in \Sort}\bS$ partielle. On dit que $\sigma$ est adéquate pour un terme $\bt$ si pour toute variable libre $\bx\in \varlib{\bt}$, le type de $\bx$ correspond au type de $\sigma(\bx)$. Si $\sigma$ est adéquate pour $\bt$ alors on note $\bt^\sigma \in \bigcup_{S\in\Sort}\bS$ la valeur obtenue en substituant les variables libres par les valeurs données par $\sigma$. On considère dans la suite que toutes les valuations du premier ordre considérées sont adéquates. De même $\sigma$ est adéquate pour une proposition $\varphi$ si elle est adéquate pour tout terme apparaissant dans $\varphi$.

  Une valuation du second ordre est une fonction $\rho : \mathcal X_2\to \SAT^{\List(\bigcup \bS)}$. On dit que $\rho$ est adéquate pour une proposition $\varphi$ si pour toute variable libre $X\in\varlib{\varphi}$, l'arité de $\rho(X)$ correspond à celle de $X$.
\end{defi}

\begin{rmk}
  Pour éviter d'écrire trop de définitions, nous laissons celle de $\bt^\sigma$ implicite. Néanmoins, nous devons donner un point important~: la signification de $\rec\;\bt\;\bu\;\bv$. Les fonctions de la forme $\rec_{\mathbb A}\;\bt\;\bu$ sont définies par la propriété universelle de la structure considérée (les entiers naturels, les booléens et les listes). On peut donc considérer qu'on a les axiomes du type $\rec_\bN\;\bt\;\bu\;\bZ = \bt$ puisque les interprétations des deux termes sont égales.
\end{rmk}

On peut maintenant définir la fonction $\trad{-}_\rho^\sigma : \Propo \to \mathcal P(\Lambda)$.

\begin{defi}[Interprétation]
  On définit la fonction par induction, en considérant $\rho$ et $\sigma$ adéquates~:
  \begin{itemize}
  \item $\trad{X(\bt_1,\ldots,\bt_n)}_\rho^\sigma \defeq \rho(X)(\bt_1^\sigma,\ldots,\bt_n^\sigma)$
  \item $\trad{\varphi\to\psi}_\rho^\sigma\defeq \{t\in\Lambda\mid \forall u \in \trad{\varphi}_\rho^\sigma, t\;u\in\trad{\psi}_\rho^\sigma\}$
  \item $\trad{\varphi\land \psi}_\rho^\sigma \defeq \{t\in \Lambda\mid \pi_1\;t\in\trad{\varphi}_\rho^\sigma, \pi_2\;t\in\trad{\psi}_\rho^\sigma\}$
  \item $\displaystyle\trad{\forall x^S, \varphi}_\rho^\sigma \defeq \bigcap_{v \in \bS}\trad{\varphi}_\rho^{\sigma[x \leftarrow v]}$
  \item $\displaystyle\trad{\forall X^{S_1,\ldots,S_n}, \varphi}_\rho^\sigma\defeq\bigcap_{F : \prod_i \bS_i \to \SAT}\trad{\varphi}_{\rho[X \leftarrow F]}^\sigma$
  \item $\trad{S(\bt)}_\rho^\sigma \defeq \{t \in\Lambda\mid \exists u \in \ceil{\bt^\sigma}, t\reduc^* u\}$
  \end{itemize}
\end{defi}

\begin{nota}
  On notera $t\reali_\rho^\sigma \varphi$ pour $t\in\trad\varphi_\rho^\sigma$, et $t\reali\varphi$ pour $t\in\trad\varphi_\varnothing^\varnothing$. On appellera alors $t$ un réaliseur de $\varphi$.
\end{nota}

On peut alors voir la différence fondamentale entre $\forall x^A, \varphi$ et $\forall x^{\{A\}}, \varphi$~: le premier signifie simplement qu'il existe un réaliseur de $\varphi$ qui est uniforme pour $x$, c'est-à-dire que ce même réaliseur fonctionne pour toutes les valeurs possibles de $x$ ; le second, lui, indique que l'on possède une façon de calculer $\overline \bt$ en un réaliseur de $\varphi$. Par exemple $\forall x^\bN, x = x$ signifie qu'il existe $t$ qui réalise chaque $n = n$ pour $n\in \bN$, quand $\forall x^{\{\bN\}}, x = x$ signifie qu'il existe une fonction qui à $\overline n$ associe une preuve de $n = n$. Créer cette distinction permet de contrôler le plus finement possible ce qui est du ressort du calcul et ce qui est du ressort de la vérité logique. En particulier, on voit qu'un réaliseur d'une formule de la forme $\forall x^{\{A\}}, \exists y^{\{B\}}, \varphi(x,y)$ calcule directement une fonction $A \to B$ ainsi qu'une preuve de $\varphi(x,f(x))$, là où sans cette relativisation le $y$ dépendant de $x$ n'a pas besoin d'être calculable et peut simplement exister dans la méta-théorie.

Un des lemmes principaux est celui de saturation, qui assure que tous les ensembles ainsi définis restent saturés.

On commence par montrer que $\SAT$ est un treillis complet.

\begin{lem}
  $(\SAT,\subseteq,\Lambda,\varnothing,\bigcap,\bigcup)$ est un treillis complet.
\end{lem}

\begin{proof}
  Il est évident que $S\in \SAT \implies \varnothing\subseteq S \subseteq \Lambda$ et que ces deux parties sont stables par anté-réduction. Supposons que $\{S_i\}_{i\in I}$ est une famille de parties saturées. Si $t\reduc u$ et $u \in \bigcap S_i$, alors pour tout $i \in I$, $t \in S_i$ par saturation de $S_i$, donc $t\in \bigcap S_i$, donc $\bigcap S_i$ est saturée. De même si $u \in \bigcup S_i$, alors on trouve $i$ tel que $u \in S_i$ donc $t\in S_i$ par saturation. Donc $\bigcup S_i$ est saturée. Ainsi $(\SAT,\subseteq,\Lambda,\varnothing, \bigcap,\bigcup)$ est un treillis complet.
\end{proof}

\begin{lem}[Saturation]
  Pour toute proposition $\varphi \in \Propo$ et valuations $\sigma,\rho$ adéquates, $\trad\varphi_\rho^\sigma\in\SAT$.
\end{lem}

\begin{proof}
  On procède par induction sur $\varphi$~:
  \begin{itemize}
  \item dans le cas de $X(\bt_1,\ldots,\bt_n)$ la définition même de $\trad-_\rho^\sigma$ nous donne un ensemble saturé.
  \item supposons que $\trad{\varphi}_\rho^\sigma\in\SAT$ et $\trad{\psi}_\rho^\sigma\in \SAT$. Soient alors $t,u\in\Lambda$ tels que $t\reduc u$ et $u \reali_\rho^\sigma \varphi \to \psi$. Soit $v\reali_\rho^\sigma\varphi$, par définition on en déduit que $u\;v\reali_\rho^\sigma \psi$ donc par hypothèse d'induction, et comme $t\;v\reduc u\;v$, $t\;v\reali_\rho^\sigma\psi$ donc $t\reali_\rho^\sigma\psi$, d'où $\trad{\varphi\to\psi}_\rho^\sigma \in\SAT$.
  \item supposons que $\trad{\varphi_1}_\rho^\sigma$ et $\trad{\varphi_2}_\rho^\sigma$ sont des ensembles saturés, soient alors $t, u \in \Lambda$ tels que $t\reduc u$ et $u \in \trad{\varphi_1\land \varphi_2}_\rho^\sigma$. On sait donc que pour tout $i\in\{1,2\}$, $\pi_i\;u \reali_\rho^\sigma \varphi_i$ et $\pi_i\;t\reduc\pi_i\;u$, donc par hypothèse d'induction et saturation $\pi_i\;t\reali_\rho^\sigma\varphi_i$. Donc $\trad{\varphi_1\land\varphi_2}_\rho^\sigma\in\SAT$.
  \item si tous les $\trad\varphi_\rho^{\sigma[x\leftarrow v]}$ sont saturés, comme $\SAT$ est un treillis complet, on en déduit directement que $\bigcap \trad\varphi_\rho^{\sigma[x\leftarrow v]}$ est saturé.
  \item de même que précédemment, le fait que $\SAT$ est un treillis complet assure que ce cas passe à l'induction.
  \item la construction de $\trad{S(\bt)}$ nous donne directement la saturation.
  \end{itemize}

  Ainsi, par induction, si $t\reduc u$ et $u\reali_\rho^\sigma \varphi$ alors $t\reali_\rho^\sigma\varphi$.
\end{proof}

\subsubsection{Système de types}

On définit un système de typage pour $\Lambda$, dont les types sont des propositions.



En particulier, si $\vdash t : \varphi$ alors $t\reali \varphi$.

\subsection{Traiter les arbres}

On peut désormais étudier les notions liées aux arbres. On donne d'abord deux fonctions utilitaires liées aux listes et aux fonctions~:

\begin{defi}[Longueur, préfixe]
  On définit les deux fonctions suivantes au premier ordre, où $\ell : \List$ et $f : \Nat\to\Nat$~:
  \begin{align*}
    |\ell| &\defeq \rec_{\List}\;\bZ\;(\blam \bx.\blam\by.\blam\bz.\bfS\;\bz)\;\ell\\
    f_n &\defeq \rec_{\Nat}\;\bnil\;(\blam \bn.\blam \bx.(f(\bn))\bcons \bx)\;n
  \end{align*}
\end{defi}

\begin{rmk}
  En utilisant l'adéquation, on peut directement voir que ces deux fonctions possèdent un code. On définit $\Gamma_0\mid\Xi_0 = \ell : \List\mid\varnothing\mid l : \List(\ell)$ et $\Gamma_1\mid\Xi_1 = \ell : \List, \bx : \Nat, \by : \List, \bz : \Nat\mid \varnothing\mid l : \List(\ell), x : \Nat(\bx), y : \Nat(\by), z : \Nat(\bz)$~:
  \begin{center}
    \begin{prooftree}
      \infer0[$\bN_\mathrm i^0$]{\Gamma_0\mid\Xi_0\vdash 0 : \Nat(\bZ)}
      \infer0[Ax]{\Gamma_1\mid\Xi_1\vdash \Nat(\bz)}
      \infer1[$\bN_\mathrm i^S$]{\Gamma_1\mid\Xi_1\vdash S\;z : \Nat(\bfS\;\bz)}
      \infer1[$(\blam_\mathrm i)^3$]{\Gamma_0\mid\Xi_0\vdash \lambda x\; y\;z.S\;z : (\Nat\to \List\to\Nat\to\Nat)(\blam \bx\;\by\;\bz.\bfS\;\bz)}
      \infer0[Ax]{\Gamma_0\mid\Xi_0\vdash \List(\ell)}
      \infer3{\Gamma_0\mid\Xi_0\vdash \rec_\bL\;0\;(\lambda x.\lambda y.\lambda z.S\;z)\;l : \Nat(|\ell|)}
      \infer1[$\blam_\mathrm i$]{\vdash \lambda l.\rec_\bL\;0\;(\lambda x\;y\;z.S\;z)\;l : (\List\to\Nat)(\lambda \ell.|\ell|)}
    \end{prooftree}
  \end{center}
\end{rmk}

Pour définir un arbre, on considère directement un prédicat sur $\List$ qui est stable par préfixe. Pour cela, introduisons la notion de préfixe~:

\begin{defi}[Préfixe]
  On définit la relation $\preceq$ de préfixe sur $\List$ par la fonction suivante~:
  \[\ell\preceq\ell' \defeq (\rec_{\List}\;(\rec_{\List}\;\bbtt\;(\blam n.\blam l. \blam b.\bbff))\;(\blam n.\blam l.\blam b.\rec_{\List}\;\bbff\;(\blam n'.\blam l'.\blam b'.(n =? n') \land_\bB (b\;l)))\;\ell\;\ell') = \bbtt\]
\end{defi}

On peut vérifier que cette relation fait bien ce qu'on attend~:
\begin{itemize}
\item si $\ell = \bnil$, alors on a la fonction valant $\bbtt$ en $\bnil$ et $\bbff$ ailleurs.
\item si $\ell = h \bcons t$ et $\ell' = h' \bcons t'$ alors la fonction vaut $h = h' \land t = t'$, et si $\ell' = \bnil$ alors la fonction vaut $\bbff$.
\end{itemize}

\begin{defi}[Arbre, prédicat monotone]
  On définit le prédicat $\isTree(T)$ pour $T^{\List}$ par
  \[\isTree(T)\defeq \forall \ell^{\List}\;\ell'^{\List}, T(\ell') \land \ell \preceq \ell' \to T(\ell)\]
  On définit aussi le fait d'être monotone par
  \[\isMono(T)\defeq \forall \ell^{\List}\;\ell'^{\List}, T(\ell) \land \ell \preceq \ell' \to T(\ell')\]
\end{defi}

\begin{nota}
  On notera $\forall T^{\Tree}, \varphi$ et $\forall T^{\Mono}, \varphi$ pour considérer respectivement $\forall T^{\List}, \isTree(T) \to \varphi$ et $\forall T^{\List}, \isMono(T) \to \varphi$.
\end{nota}

On définit aussi $\BinTree$ par~:
\[\BinTree(T) \defeq \forall \ell, T(\ell) \implies \forall n, n \in \ell \implies n \leq 1\]

Avec ces termes, on peut déjà définir les deux théorèmes que l'on veut étudier~:
\begin{equation}
  \KL \defeq \forall T^{\Tree}, (\forall n^{\{\Nat\}}, \exists \ell^{\{\List\}}, |\ell| = n \land T(\ell)) \implies \exists \alpha^{\{\Nat\to\Nat\}}, \forall n^{\{\Nat\}}, T(\alpha_n)
\end{equation}
\begin{equation}
  \WKL \defeq \forall T^{\BinTree}, (\forall n^{\{\Nat\}}, \exists \ell^{\{\List\}}, |\ell| = n \land T(\ell)) \implies \exists \alpha^{\{\Nat\to\Nat\}}, \forall n^{\{\Nat\}}, T(\alpha_n)
\end{equation}
\begin{equation}
  \FT \defeq \forall T^{\Mono}, (\forall \alpha^{\{\Nat \to \Nat\}},\exists n^{\{\Nat\}}, T(\alpha_n)) \implies \exists n^{\{\Nat\}}, \forall \alpha^{\{\Nat \to \Nat\}}, T(\alpha_n)
\end{equation}
\begin{equation}
  \FT' \defeq \forall T^{\Mono}, (\exists f^{\{(\Nat\to\Nat)\to\Nat\}}, \forall \alpha^{\{\Nat \to \Nat\}}, T(\alpha_{f(\alpha)})) \implies \exists n^{\{\Nat\}}, \forall \alpha^{\{\Nat \to \Nat\}}, T(\alpha_n)
\end{equation}

\begin{them}[FAN]
  Il existe $t\in \Lambda$ tel que
  \[t\reali \FT\]
\end{them}

\begin{proof}
  On définit une interprétation des sortes et termes du premier ordre dans la catégorie $\Cpo$. Par induction~:
  \begin{itemize}
  \item $\trad{\Nat}_{\Cpo} \defeq \bN_\bot$
  \item $\trad{\Bool}_{\Cpo} \defeq \bB_\bot$
  \item $\trad{\List}_{\Cpo} \defeq (\List(\bN))_\bot$
  \item $\trad{S \to S'}_{\Cpo} \defeq \bS'^{\bS}$
  \item $\trad{S\times S'}_{\Cpo} \defeq \bS\times \bS'$
  \end{itemize}
  L'interprétation des termes du premier ordre est définie naturellement comme l'interprétation de système T dans $\Cpo$ (les paires, les abstraction et applications viennent directement de la structure de catégorie cartésienne fermée, les constructeurs de $\Nat$ d'un objet entier naturel \textit{etc}.)

  De plus, on peut interpréter chaque terme $t\in\LamS{S}$ par un élément $\hat t$ de $\bS$. On peut prouver que si $t\reali_\rho^\sigma S(\bt)$ alors $\hat t = \bt^\sigma$.

  Supposons alors qu'on a $r\reali \forall f^{\{\Nat \to \Nat\}}, \exists n^{\{\Nat\}}, f_n \in T$. L'argument classique montrant que l'axiome du choix avec des quantifications fortes est vrai en théorie des types fonctionne aussi ici (dans le cas de quantifications relativisées), on trouve donc
  \[t\reali \forall X^{\{S,S'\}}, (\forall x^{\{S\}}, \exists y^{\{S'\}}, X(x,y))\to \exists f^{\{S \to S'\}}, \forall x^{\{S\}}, X(x,f(x))\]
  d'où
  \[t\;r\reali \exists f^{\{(\Nat\to\Nat)\to\Nat\}}, \forall \alpha^{\{\Nat\to\Nat\}}, \alpha_{f(\alpha)}\in T\]
  On en déduit donc, par notre interprétation des termes du premier ordre, qu'il existe $f : (\bN_\bot\to\bN_\bot)\to\bN_\bot$ continue. Ainsi, si on a un chemin $\alpha$ on peut trouver un préfixe fini $n_\alpha$ tel que
  \[\forall \gamma, \gamma_{n_\alpha} = \alpha_{n_\alpha} \implies f(\gamma) = f(\alpha)\]

  On construit le prédicat décidable $C : \List \to \Bool$ dont l'idée est de parcourir tous les chemins les uns à la suite des autres en ajoutant chaque fois pour une liste $\ell$ la liste obtenue en appliquant $r$ à $\ell 0^\infty$. On a donc besoin d'une fonction d'énumération des listes~:
  \[\enumer \defeq A FAIRE\]

  On a aussi besoin d'une fonction transformant une liste $\ell$ en la fonction $\bN \to \bN$ définie par $\ell 0^\infty$~:
  \[\toinfty \defeq \rec_{\List}\;(\lambda n.0)\;(\lambda n.\lambda x.\lambda f.\rec_{\Nat}\;n\;(\lambda m.\lambda g.f\;m))\]

  La construction de $C$ est alors la suivante~:
  \[C \defeq \lambda b. Y(\lambda f. \lambda \ell. \rec_{Bool}\;(\ell << b)\;(\rec_{\Bool}\;(r\;(\toinfty\;\ell)\preceq b)\;\top\;(f\;(\enumer\;\ell)))\;(r\;(\toinfty\;\ell) \preceq b))\]

  Pour trouver le $n$ correspondant, il nous suffit alors de regarder pour chaque $n$ si toutes les listes de taille $n$ sont dans
  $C$~:
  \[fan\defeq Y (\lambda f\;i. \rec_{\Bool}\;(\forall \ell, |\ell| = i \implies C(\ell))\;i\;(f\;(i+1)))\;0\]

  Il nous suffit alors de montrer que cet algorithme finit par trouver une bar uniforme. Par l'asurde, supposons que ça n'est pas le cas. On trouve donc pour chaque $n$ une liste $\ell_n\notin C$ de taille $n$. Par le lemme de König (dans la méta-théorie), on en déduit l'existence d'un chemin $\alpha : \bN \to \bN$ tel que $\forall n, \alpha_n \notin C$. Ce chemin donne lieu à l'existence d'une fonction $\alpha_\bot : \bN_\bot \to \bN_\bot$ continue (en prenant la version stricte de $\alpha$, qui est continue puisque stricte entre deux domaines plats), donc d'après notre laïus précédent, on peut trouver un nombre $n$ tel que tout chemin coïncidant avec $\alpha$ sur ses $n$ premières valeurs est associé à la même valeur. Ainsi, $\alpha_n 0^\infty$ renvoie $r(\alpha)$. Mais comme $r(\alpha)$ est un préfixe de $\alpha$, on en déduit que $\alpha \in C$, ce qui est absurde. Donc $C$ est un algorithme qui termine.
\end{proof}

On montre maintenant que $\KL$ n'est pas vérifié, même sous sa forme faible $\WKL$.

\begin{them}
  Il n'existe pas de $t\in \Lambda$ tel que
  \[t\reali \WKL\]
\end{them}

\begin{proof}
  La démonstration repose sur l'argument classique de l'arbre de Kleene. Pour pouvoir l'utiliser, on commence par admettre le résultat habituel de l'existence d'une machine universelle, ce que l'on peut écrire par le résultat suivant~:
  \[\exists U^{\{\Nat\to\Nat\to\Nat\to\Nat\}}, \forall f^{\{\Nat\to\Nat\}}, \exists e^{\Nat}, \forall x^{\{\Nat\}}, \exists t^{\Nat},
  \left\{\begin{array}{c}
  \forall t' < t, U(e,x,t') = 0\\
  U(e,x,t) = S(f(x))\\
  \forall t' > t, U(e,x,t') = U(e,x,t)
  \end{array}\right.\]
  Cela repose principalement sur l'argument qu'une fonction $f^{\{\Nat\to\Nat\}}$ est encodée par un terme, et les termes sont dénombrables~: on peut donc simuler $f(x)$ par l'exécution de la réduction de $t(x)$ où $t$ est le terme qui représente $f$.

  On définit alors la fonction $d : \Nat\to\Nat\to\Nat$ qui représente la fonction diagonalement non récursive obtenue en tronquant le calcul à une étape donnée~:
  \[d \defeq \lambda x.\lambda t. \rec_{\Nat}\;2\;(\lambda x\;n.\rec_{\Nat}\;1\;(\lambda\;\_\;\_.0)\;n)\;(U(x,x,t))\]

  Si $\varphi_e(e)[t]\downarrow$ alors $d(e,t) \rhd^* \rec_{\Nat}\;1\;(\lambda\;\_\;\_.0)\;(\varphi_e(e))$, ce qui se réduit vers $0$ si $\varphi_e(e) > 0$ et vers $1$ si $\varphi_e(e) = 0$. Si $\varphi_e(e)[t]\uparrow$; alors $d(e,t) \rhd^* 2$.

  On définit maintenant
  \[\surd(n,k,a) \defeq \rec_{\Bool}\;\btt\;(d(k,t) =? a)\;(d(k,t) =? 2)\]
  et l'arbre de Kleene~:
  \[K \defeq \lambda \ell. [\rec_{\List}(\lambda n\;k.\btt)\;(\lambda h\;t\;f\;n\;k.\surd(n,k,h) \&\& f\;n\;(k+1)\;t)]\;|\ell|\;0\]

  On voit que $K$ est clos par préfixe. De plus, pour tout $n\in \mathbb N$, on peut effectivement trouver une liste de taille $n$ appartenant à $K$~: $d(-,n)_n \in K$ et $|d(-,n)_n| = n$.

  Cependant, supposons qu'il existe un chemin infini dans l'arbre, $\alpha$, alors on trouve $j$ tel que $\alpha = \varphi_j$. On sait donc que pour $t$ suffisamment grand, $d(j,t) \neq \varphi_j(j) = \alpha_j$, donc $\alpha_j \notin K$, ce qui est une contradiction. On voit donc que $K$ ne possède pas de chemin infini.
\end{proof}

\end{comment}

\end{document}
